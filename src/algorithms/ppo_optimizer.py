"""
PPOä¼˜åŒ–å™¨ - åŸºäºå¼ºåŒ–å­¦ä¹ çš„å¸ƒå±€ä¼˜åŒ–ç®—æ³•
"""

import torch
import time
import numpy as np
from pathlib import Path
from typing import List, Optional, Dict, Any
from stable_baselines3.common.vec_env import VecNormalize
from stable_baselines3.common.env_util import make_vec_env
from sb3_contrib.common.maskable.callbacks import MaskableEvalCallback as EvalCallback
from sb3_contrib import MaskablePPO
from sb3_contrib.common.wrappers import ActionMasker
from sb3_contrib.common.maskable.policies import MaskableActorCriticPolicy

from src.algorithms.base_optimizer import BaseOptimizer, OptimizationResult
from src.algorithms.constraint_manager import ConstraintManager
from src.rl_optimizer.env.cost_calculator import CostCalculator
from src.rl_optimizer.env.layout_env import LayoutEnv
from src.rl_optimizer.env.vec_env_wrapper import EpisodeInfoVecEnvWrapper
from src.rl_optimizer.model.policy_network import LayoutTransformer
from src.rl_optimizer.utils.setup import setup_logger, save_json
from src.rl_optimizer.utils.lr_scheduler import get_lr_scheduler
from src.rl_optimizer.utils.checkpoint_callback import CheckpointCallback
from src.rl_optimizer.data.cache_manager import CacheManager
from src.config import RLConfig

logger = setup_logger(__name__)


def get_action_mask_from_info(infos: List[Dict]) -> np.ndarray:
    """ä»çŸ¢é‡åŒ–ç¯å¢ƒçš„infoå­—å…¸åˆ—è¡¨ä¸­æå–åŠ¨ä½œæ©ç """
    return np.array([info.get("action_mask", []) for info in infos])


class PPOOptimizer(BaseOptimizer):
    """
    PPOä¼˜åŒ–å™¨
    
    åŸºäºå¼ºåŒ–å­¦ä¹ çš„PPOç®—æ³•å®ç°å¸ƒå±€ä¼˜åŒ–ï¼Œä½¿ç”¨MaskablePPOæ¥å¤„ç†åŠ¨ä½œæ©ç çº¦æŸã€‚
    """
    
    def __init__(self, 
                 cost_calculator: CostCalculator,
                 constraint_manager: ConstraintManager,
                 config: RLConfig,
                 cache_manager: CacheManager):
        """
        åˆå§‹åŒ–PPOä¼˜åŒ–å™¨
        
        Args:
            cost_calculator: æˆæœ¬è®¡ç®—å™¨
            constraint_manager: çº¦æŸç®¡ç†å™¨
            config: RLé…ç½®
            cache_manager: ç¼“å­˜ç®¡ç†å™¨
        """
        super().__init__(cost_calculator, constraint_manager, "PPO")
        self.config = config
        self.cache_manager = cache_manager
        
        # ç¯å¢ƒå‚æ•°
        self.env_kwargs = {
            "config": self.config,
            "cache_manager": self.cache_manager,
            "cost_calculator": self.cost_calculator,
            "constraint_manager": self.constraint_manager
        }
        
        # è®­ç»ƒçŠ¶æ€
        self.model = None
        self.vec_env = None
        self.resume_model_path = None
        self.completed_steps = 0
        self.best_model_dir = None  # ä¿å­˜æœ€ä½³æ¨¡å‹ç›®å½•è·¯å¾„
    
    def optimize(self, 
                 initial_layout: Optional[List[str]] = None,
                 max_iterations: int = None,
                 total_timesteps: int = None,
                 original_layout: Optional[List[str]] = None,
                 original_cost: Optional[float] = None,
                 **kwargs) -> OptimizationResult:
        """
        æ‰§è¡ŒPPOä¼˜åŒ–
        
        Args:
            initial_layout: åˆå§‹å¸ƒå±€ï¼ˆPPOä¼šè‡ªåŠ¨æ¢ç´¢ï¼‰
            max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼ˆä½¿ç”¨total_timestepsä»£æ›¿ï¼‰
            total_timesteps: æ€»è®­ç»ƒæ­¥æ•°
            original_layout: åŸå§‹å¸ƒå±€ï¼ˆæœªç»ä¼˜åŒ–çš„åŸºå‡†ï¼‰
            original_cost: åŸå§‹å¸ƒå±€çš„æˆæœ¬
            **kwargs: å…¶ä»–PPOå‚æ•°
            
        Returns:
            OptimizationResult: ä¼˜åŒ–ç»“æœ
        """
        self.start_optimization()
        
        # ä¿å­˜åŸå§‹å¸ƒå±€ä¿¡æ¯
        self.original_layout = original_layout
        self.original_cost = original_cost
        
        # ä½¿ç”¨é…ç½®ä¸­çš„å‚æ•°æˆ–ä¼ å…¥çš„å‚æ•°
        if total_timesteps is None:
            total_timesteps = self.config.TOTAL_TIMESTEPS
        
        logger.info(f"å¼€å§‹PPOä¼˜åŒ–ï¼Œæ€»è®­ç»ƒæ­¥æ•°: {total_timesteps}")
        
        try:
            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ¢å¤è®­ç»ƒ
            self._check_for_resume()
            
            # è®¡ç®—å‰©ä½™è®­ç»ƒæ­¥æ•°
            remaining_steps = max(0, total_timesteps - self.completed_steps)
            if remaining_steps == 0:
                logger.info("è®­ç»ƒå·²å®Œæˆï¼ŒåŠ è½½æœ€ä½³æ¨¡å‹è¿›è¡Œè¯„ä¼°")
                best_layout, best_cost = self._evaluate_best_model()
                self.update_best_solution(best_layout, best_cost)
                return self.finish_optimization()
            
            # åˆ›å»ºç¯å¢ƒå’Œæ¨¡å‹
            self._setup_environment_and_model(remaining_steps)
            
            # æ‰§è¡Œè®­ç»ƒ
            self._train_model(remaining_steps)
            
            # è¯„ä¼°æœ€ä½³æ¨¡å‹
            best_layout, best_cost = self._evaluate_best_model()
            self.update_best_solution(best_layout, best_cost)
            
        except KeyboardInterrupt:
            logger.warning("è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
            if self.model:
                self._save_interrupted_model()
        except Exception as e:
            logger.error(f"PPOä¼˜åŒ–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
            raise
        
        return self.finish_optimization()
    
    def _check_for_resume(self):
        """æ£€æŸ¥æ˜¯å¦éœ€è¦ä»checkpointæ¢å¤è®­ç»ƒ"""
        if not self.config.RESUME_TRAINING:
            return
            
        if self.config.PRETRAINED_MODEL_PATH:
            model_path = Path(self.config.PRETRAINED_MODEL_PATH)
            if not model_path.exists():
                logger.warning(f"æŒ‡å®šçš„é¢„è®­ç»ƒæ¨¡å‹ä¸å­˜åœ¨: {model_path}")
                return
            self.resume_model_path = str(model_path)
        else:
            # è‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°çš„checkpoint
            checkpoint_callback = CheckpointCallback(
                save_freq=1,
                save_path=self.config.LOG_PATH
            )
            model_path = checkpoint_callback.get_latest_checkpoint()
            if model_path:
                self.resume_model_path = str(model_path)
            else:
                logger.info("æœªæ‰¾åˆ°å¯ç”¨çš„checkpointï¼Œå°†å¼€å§‹å…¨æ–°è®­ç»ƒ")
                return
        
        # åŠ è½½checkpointå…ƒæ•°æ®
        metadata = CheckpointCallback.load_checkpoint_metadata(self.resume_model_path)
        if metadata:
            self.completed_steps = metadata.get("training_progress", {}).get("num_timesteps", 0)
            logger.info(f"ä»checkpointæ¢å¤è®­ç»ƒï¼Œå·²å®Œæˆæ­¥æ•°: {self.completed_steps}")
        else:
            logger.warning("æ— æ³•åŠ è½½checkpointå…ƒæ•°æ®ï¼Œä»æ­¥æ•°0å¼€å§‹")
    
    def _setup_environment_and_model(self, remaining_steps: int):
        """è®¾ç½®ç¯å¢ƒå’Œæ¨¡å‹"""
        logger.info(f"æ­£åœ¨åˆ›å»º {self.config.NUM_ENVS} ä¸ªå¹¶è¡Œç¯å¢ƒ...")
        
        # åˆ›å»ºçŸ¢é‡åŒ–ç¯å¢ƒ
        vec_env = make_vec_env(
            lambda: ActionMasker(LayoutEnv(**self.env_kwargs), LayoutEnv._action_mask_fn),
            n_envs=self.config.NUM_ENVS
        )
        
        # ä½¿ç”¨è‡ªå®šä¹‰åŒ…è£…å™¨ç¡®ä¿episodeä¿¡æ¯æ­£ç¡®ä¼ é€’
        self.vec_env = EpisodeInfoVecEnvWrapper(vec_env)
        
        logger.info("çŸ¢é‡åŒ–ç¯å¢ƒåˆ›å»ºæˆåŠŸï¼Œå·²æ·»åŠ episodeä¿¡æ¯åŒ…è£…å™¨")
        
        # åˆ›å»ºæˆ–åŠ è½½æ¨¡å‹
        if self.resume_model_path:
            self._load_pretrained_model()
        else:
            self._create_new_model()
    
    def _load_pretrained_model(self):
        """åŠ è½½é¢„è®­ç»ƒæ¨¡å‹"""
        logger.info(f"æ­£åœ¨åŠ è½½é¢„è®­ç»ƒæ¨¡å‹: {self.resume_model_path}")
        
        try:
            self.model = MaskablePPO.load(
                self.resume_model_path,
                env=self.vec_env,
                device='cuda' if torch.cuda.is_available() else 'cpu'
            )
            
            # é‡æ–°è®¾ç½®å­¦ä¹ ç‡è°ƒåº¦å™¨
            if hasattr(self.model, 'lr_schedule'):
                lr_scheduler = get_lr_scheduler(
                    schedule_type=self.config.LEARNING_RATE_SCHEDULE_TYPE,
                    initial_lr=self.config.LEARNING_RATE_INITIAL,
                    final_lr=self.config.LEARNING_RATE_FINAL
                )
                self.model.lr_schedule = lr_scheduler
                logger.info("å­¦ä¹ ç‡è°ƒåº¦å™¨å·²é‡æ–°è®¾ç½®")
                
            logger.info("é¢„è®­ç»ƒæ¨¡å‹åŠ è½½æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å¤±è´¥: {e}")
            raise
    
    def _create_new_model(self):
        """åˆ›å»ºæ–°çš„PPOæ¨¡å‹"""
        logger.info("åˆ›å»ºå…¨æ–°çš„PPOæ¨¡å‹...")
        
        # åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
        lr_scheduler = get_lr_scheduler(
            schedule_type=self.config.LEARNING_RATE_SCHEDULE_TYPE,
            initial_lr=self.config.LEARNING_RATE_INITIAL,
            final_lr=self.config.LEARNING_RATE_FINAL
        )
        
        logger.info(f"ä½¿ç”¨å­¦ä¹ ç‡è°ƒåº¦å™¨: {self.config.LEARNING_RATE_SCHEDULE_TYPE}")
        logger.info(f"åˆå§‹å­¦ä¹ ç‡: {self.config.LEARNING_RATE_INITIAL}, æœ€ç»ˆå­¦ä¹ ç‡: {self.config.LEARNING_RATE_FINAL}")
        
        # å®šä¹‰ç­–ç•¥ç½‘ç»œå‚æ•°
        policy_kwargs = {
            "features_extractor_class": LayoutTransformer,
            "features_extractor_kwargs": {
                "features_dim": self.config.FEATURES_DIM,  # ä½¿ç”¨ç»Ÿä¸€çš„é…ç½®å±æ€§
                "config": self.config
            },
            "net_arch": dict(pi=[self.config.POLICY_NET_ARCH] * self.config.POLICY_NET_LAYERS,
                            vf=[self.config.VALUE_NET_ARCH] * self.config.VALUE_NET_LAYERS)
        }
        
        # åˆ›å»ºPPOæ¨¡å‹
        self.model = MaskablePPO(
            MaskableActorCriticPolicy,
            self.vec_env,
            learning_rate=lr_scheduler,
            n_steps=self.config.N_STEPS,
            batch_size=self.config.BATCH_SIZE,
            n_epochs=self.config.N_EPOCHS,
            gamma=self.config.GAMMA,
            gae_lambda=self.config.GAE_LAMBDA,
            clip_range=self.config.CLIP_RANGE,
            ent_coef=self.config.ENT_COEF,
            vf_coef=self.config.VF_COEF,
            max_grad_norm=self.config.MAX_GRAD_NORM,
            policy_kwargs=policy_kwargs,
            verbose=1,
            device='cuda' if torch.cuda.is_available() else 'cpu',
            tensorboard_log=str(self.config.LOG_PATH)
        )
        
        logger.info("PPOæ¨¡å‹åˆ›å»ºæˆåŠŸ")
    
    def _train_model(self, remaining_steps: int):
        """è®­ç»ƒæ¨¡å‹"""
        timestamp = time.strftime("%Y%m%d-%H%M%S")
        log_dir = self.config.LOG_PATH / f"ppo_layout_{timestamp}"
        result_dir = self.config.RESULT_PATH / "model" / f"ppo_layout_{timestamp}"
        
        # å¦‚æœæ˜¯æ¢å¤è®­ç»ƒï¼Œä½¿ç”¨åŸæœ‰ç›®å½•
        if self.resume_model_path:
            checkpoint_path = Path(self.resume_model_path)
            if "ppo_layout_" in checkpoint_path.parent.name:
                log_dir = checkpoint_path.parent
                result_dir = self.config.RESULT_PATH / checkpoint_path.parent.name
        
        # åˆ›å»ºç›®å½•
        log_dir.mkdir(parents=True, exist_ok=True)
        result_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹ç›®å½•è·¯å¾„
        self.best_model_dir = result_dir
        
        # è®¾ç½®å›è°ƒ
        callbacks = []
        
        
        # Checkpointå›è°ƒ
        checkpoint_callback = CheckpointCallback(
            save_freq=self.config.CHECKPOINT_FREQUENCY,
            save_path=str(log_dir / "checkpoints"),
            name_prefix="checkpoint"
        )
        callbacks.append(checkpoint_callback)
        
        # è¯„ä¼°å›è°ƒ
        eval_vec_env = make_vec_env(
            lambda: ActionMasker(LayoutEnv(**self.env_kwargs), LayoutEnv._action_mask_fn),
            n_envs=1
        )
        eval_env = EpisodeInfoVecEnvWrapper(eval_vec_env)
        
        eval_callback = EvalCallback(
            eval_env,
            best_model_save_path=str(result_dir / "best_model"),
            log_path=str(log_dir / "eval_logs"),
            eval_freq=self.config.EVAL_FREQUENCY,
            deterministic=True,
            render=False
        )
        callbacks.append(eval_callback)
        
        # å¼€å§‹è®­ç»ƒ
        logger.info(f"å¼€å§‹è®­ç»ƒï¼Œå‰©ä½™æ­¥æ•°: {remaining_steps}")
        logger.info(f"æ—¥å¿—ä¿å­˜è·¯å¾„: {log_dir}")
        
        self.model.learn(
            total_timesteps=remaining_steps,
            callback=callbacks,
            tb_log_name="PPO",
            progress_bar=True,
            reset_num_timesteps=False if self.resume_model_path else True
        )
        
        # è®­ç»ƒå®Œæˆ
        logger.info("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
        logger.info("=" * 80)
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        final_model_path = log_dir / "final_model.zip"
        self.model.save(str(final_model_path))
        logger.info(f"æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜åˆ°: {final_model_path}")
        
        # ä¿å­˜è®­ç»ƒé…ç½®
        config_path = log_dir / "training_config.json"
        config_data = self.config.__dict__.copy()
        save_json(config_data, str(config_path))
        logger.info(f"è®­ç»ƒé…ç½®å·²ä¿å­˜åˆ°: {config_path}")
    
    def _evaluate_best_model(self) -> tuple[List[str], float]:
        """è¯„ä¼°æœ€ä½³æ¨¡å‹å¹¶è¿”å›æœ€ä¼˜å¸ƒå±€å’Œæˆæœ¬"""
        best_model_path = None
        
        # 1. ç¡®å®šæœ€ä½³æ¨¡å‹è·¯å¾„
        if self.best_model_dir:
            best_model_path = self.best_model_dir / "best_model" / "best_model.zip"
            if not best_model_path.exists():
                logger.warning(f"æœ€ä½³æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {best_model_path}")
                best_model_path = None
        
        # 2. å¦‚æœæ²¡æœ‰æœ€ä½³æ¨¡å‹ï¼Œå°è¯•ä½¿ç”¨æœ€ç»ˆæ¨¡å‹
        if not best_model_path:
            final_model_paths = [
                self.config.LOG_PATH / f"ppo_layout_*/final_model.zip",
                self.config.LOG_PATH / "final_model.zip"
            ]
            
            for pattern in final_model_paths:
                import glob
                matches = glob.glob(str(pattern))
                if matches:
                    # ä½¿ç”¨æœ€æ–°çš„æ¨¡å‹
                    best_model_path = Path(max(matches, key=lambda x: Path(x).stat().st_mtime))
                    logger.info(f"ä½¿ç”¨æœ€ç»ˆæ¨¡å‹ä½œä¸ºå¤‡é€‰: {best_model_path}")
                    break
        
        # 3. å¦‚æœä»ç„¶æ²¡æœ‰æ‰¾åˆ°æ¨¡å‹ï¼Œè¿”å›é»˜è®¤å¸ƒå±€
        if not best_model_path or not best_model_path.exists():
            logger.warning("æœªæ‰¾åˆ°å¯ç”¨çš„è®­ç»ƒæ¨¡å‹ï¼Œè¿”å›åˆå§‹å¸ƒå±€")
            best_layout = self.generate_initial_layout()
            best_cost = self.evaluate_layout(best_layout)
            return best_layout, best_cost
        
        try:
            # 4. åŠ è½½æœ€ä½³æ¨¡å‹
            logger.info(f"æ­£åœ¨åŠ è½½æœ€ä½³æ¨¡å‹: {best_model_path}")
            best_model = MaskablePPO.load(
                str(best_model_path),
                device='cuda' if torch.cuda.is_available() else 'cpu'
            )
            
            # 5. åˆ›å»ºè¯„ä¼°ç¯å¢ƒï¼ˆä½¿ç”¨ActionMaskeråŒ…è£…ï¼‰
            from sb3_contrib.common.wrappers import ActionMasker
            eval_env = ActionMasker(LayoutEnv(**self.env_kwargs), LayoutEnv._action_mask_fn)
            
            # 6. æ‰§è¡Œè¯„ä¼°ï¼ˆå¯ä»¥å¤šæ¬¡è¿è¡Œå–æœ€ä½³ç»“æœï¼‰
            best_layout = None
            best_cost = float('inf')
            n_eval_episodes = 5  # è¯„ä¼°5æ¬¡å–æœ€ä½³
            
            for episode in range(n_eval_episodes):
                obs = eval_env.reset()
                # å¤„ç†æ–°ç‰ˆGym APIè¿”å›çš„tuple
                if isinstance(obs, tuple):
                    obs = obs[0]
                terminated = False
                
                while not terminated:
                    # è·å–åŠ¨ä½œæ©ç ï¼ˆä»å†…éƒ¨ç¯å¢ƒï¼‰
                    inner_env_for_mask = eval_env.env if hasattr(eval_env, 'env') else eval_env
                    action_mask = inner_env_for_mask.get_action_mask()
                    
                    # ä½¿ç”¨æ©ç è¿›è¡Œé¢„æµ‹
                    action, _ = best_model.predict(obs, action_masks=action_mask, deterministic=True)
                    result = eval_env.step(int(action))
                    # å¤„ç†ä¸åŒç‰ˆæœ¬APIçš„è¿”å›å€¼
                    if len(result) == 5:
                        obs, _, terminated, _, _ = result
                    else:
                        obs, _, terminated, _ = result[:4]
                
                # è·å–å½“å‰episodeçš„å¸ƒå±€å’Œæˆæœ¬
                # ActionMaskeråŒ…è£…äº†åŸå§‹ç¯å¢ƒï¼Œéœ€è¦è®¿é—®å†…éƒ¨ç¯å¢ƒ
                inner_env = eval_env.env if hasattr(eval_env, 'env') else eval_env
                current_layout = inner_env._get_final_layout_str()
                current_cost = self.cost_calculator.calculate_total_cost(current_layout)
                
                if current_cost < best_cost:
                    best_cost = current_cost
                    best_layout = current_layout
                    logger.info(f"è¯„ä¼°Episode {episode+1}/{n_eval_episodes}: å‘ç°æ›´ä¼˜å¸ƒå±€ï¼Œæˆæœ¬: {best_cost:.2f}")
                else:
                    logger.info(f"è¯„ä¼°Episode {episode+1}/{n_eval_episodes}: æˆæœ¬: {current_cost:.2f}")
            
            logger.info(f"æœ€ä½³æ¨¡å‹è¯„ä¼°å®Œæˆï¼Œæœ€ä¼˜æˆæœ¬: {best_cost:.2f}")
            return best_layout, best_cost
            
        except Exception as e:
            logger.error(f"åŠ è½½æˆ–è¯„ä¼°æ¨¡å‹æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            logger.info("ä½¿ç”¨é»˜è®¤å¸ƒå±€ä½œä¸ºå¤‡é€‰æ–¹æ¡ˆ")
            best_layout = self.generate_initial_layout()
            best_cost = self.evaluate_layout(best_layout)
            return best_layout, best_cost
    
    def _save_interrupted_model(self):
        """ä¿å­˜è¢«ä¸­æ–­çš„æ¨¡å‹"""
        timestamp = time.strftime("%Y%m%d-%H%M%S")
        interrupted_path = self.config.LOG_PATH / f"interrupted_model_{timestamp}.zip"
        self.model.save(str(interrupted_path))
        logger.info(f"ä¸­æ–­çš„æ¨¡å‹å·²ä¿å­˜åˆ°: {interrupted_path}")
    
    def get_additional_metrics(self) -> Dict[str, Any]:
        """è·å–PPOç‰¹å®šçš„é¢å¤–æŒ‡æ ‡"""
        metrics = {
            "total_timesteps": self.config.TOTAL_TIMESTEPS,
            "completed_steps": self.completed_steps,
            "num_envs": self.config.NUM_ENVS,
            "learning_rate_schedule": self.config.LEARNING_RATE_SCHEDULE_TYPE,
            "resume_training": self.config.RESUME_TRAINING
        }
        
        if self.model is not None:
            metrics.update({
                "model_device": str(self.model.device),
                "policy_class": str(type(self.model.policy))
            })
        
        return metrics