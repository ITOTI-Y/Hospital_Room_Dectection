This file is a merged representation of a subset of the codebase, containing specifically included files, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of a subset of the repository's contents that is considered the most important context.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Only files matching these patterns are included: src/, configs/, main.py, result/hospital_travel_times.csv, result/slots.csv
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
configs/
  agent.yaml
  constraints.yaml
  graph_config.yaml
  paths.yaml
  pathways.yaml
  plotter.yaml
src/
  analysis/
    __init__.py
    slots_exporter.py
    travel_time.py
  config/
    __init__.py
    config_loader.py
    config.py
    graph_config.py
    path_manager.py
  network/
    floor_manager.py
    graph_manager.py
    network.py
    node_creators.py
    super_network.py
  pipeline/
    __init__.py
    cost_manager.py
    pathway_generator.py
  plotting/
    __init__.py
    plotter.py
  utils/
    logger.py
    processor.py
  network_generator.py
  optimize_manager.py
main.py
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="configs/agent.yaml">
learning_rate: 0.0003
gamma: 0.99
</file>

<file path="configs/constraints.yaml">
# Hospital Department Layout Constraints Configuration
area_compatibility_tolerance: 0.15
fixed_departments:
  - "急诊科"
  - "挂号收费"

adjacency_preferences:
  - depts: ["产科", "NICU"]
    weight: 1.5
  - depts: ["手术室", "ICU"]
    weight: 2.0

hard_constraint_penalty: -1000.0
adjacency_reward_factor: 10.0
</file>

<file path="configs/plotter.yaml">
# Configuration for PlotlyPlotter

# General settings
image_mirror: true
show_pedestrian_labels: false
node_opacity: 0.8

# Node sizes by category
node_sizes:
  default: 2
  PATH: 2
  CONNECTOR: 3
  SLOT: 7
  FIXED: 7
  STRUCTURE: 1

# Edge styles by type
edge_styles:
  horizontal:
    color: "#1f77b4"
    width: 1.5
    name: "Horizontal"
  vertical:
    color: "rgba(151,152,155,0.7)"
    width: 2.25
    name: "Vertical"
  door:
    color: "#2ca02c"
    width: 1.8
    name: "Door"
  special:
    color: "#d62728"
    width: 2.0
    name: "Special"

# Scene and layout settings
scene:
  aspect_ratio:
    x: 1.0
    y: 1.0
    z: 2.0
  camera:
    eye:
      x: 1.25
      y: 1.25
      z: 1.25

# Plotly export settings
plotly_config:
  toImageButtonOptions:
    format: 'png'
    filename: 'Network'
    scale: 3
</file>

<file path="src/analysis/__init__.py">
# src/analysis/__init__.py
"""Analysis module for calculating travel times and other graph metrics."""

from .travel_time import calculate_room_travel_times

__all__ = ["calculate_room_travel_times"]
</file>

<file path="src/config/config.py">
"""Configuration module for the optimization project."""

import pathlib
from typing import List


class RLConfig:
    """Stores configuration parameters for RL and other optimization algorithms."""

    def __init__(self):
        """Initializes all configuration parameters."""
        # Path configurations
        self.ROOT_PATH: pathlib.Path = pathlib.Path(__file__).parent.parent
        self.RL_OPTIMIZER_PATH: pathlib.Path = self.ROOT_PATH / "src" / "rl_optimizer"
        self.DATA_PATH: pathlib.Path = self.RL_OPTIMIZER_PATH / "data"
        self.CACHE_PATH: pathlib.Path = self.DATA_PATH / "cache"
        self.LOG_PATH: pathlib.Path = self.ROOT_PATH / "logs"
        self.RESULT_PATH: pathlib.Path = self.ROOT_PATH / "results"

        # Input files
        self.TRAVEL_TIMES_CSV: pathlib.Path = (
            self.RESULT_PATH / "network" / "hospital_travel_times.csv"
        )
        self.PROCESS_TEMPLATES_JSON: pathlib.Path = (
            self.DATA_PATH / "process_templates_traditional.json"
        )

        # Auto-generated/cached intermediate files
        self.NODE_VARIANTS_JSON: pathlib.Path = self.CACHE_PATH / "node_variants.json"
        self.TRAFFIC_DISTRIBUTION_JSON: pathlib.Path = (
            self.CACHE_PATH / "traffic_distribution.json"
        )
        self.RESOLVED_PATHWAYS_PKL: pathlib.Path = (
            self.CACHE_PATH / "resolved_pathways.pkl"
        )
        self.COST_MATRIX_CACHE: pathlib.Path = (
            self.CACHE_PATH / "cost_precomputation.npz"
        )

        # Constraint configurations
        self.AREA_SCALING_FACTOR: float = 0.1
        self.EMPTY_SLOT_PENALTY_FACTOR: float = 10000.0
        self.ALLOW_PARTIAL_LAYOUT: bool = True
        self.MANDATORY_ADJACENCY: List[List[str]] = []
        self.FIXED_NODE_TYPES: List[str] = [
            "门",
            "楼梯",
            "电梯",
            "扶梯",
            "走廊",
            "墙",
            "栏杆",
            "室外",
            "绿化",
            "中庭",
            "空房间",
            "急诊科",
            "挂号收费",
        ]

        # --- Heuristic Algorithm Defaults ---
        # Simulated Annealing
        self.SA_DEFAULT_INITIAL_TEMP: float = 1000.0
        self.SA_DEFAULT_FINAL_TEMP: float = 0.1
        self.SA_DEFAULT_COOLING_RATE: float = 0.95
        self.SA_DEFAULT_TEMPERATURE_LENGTH: int = 100
        self.SA_DEFAULT_MAX_ITERATIONS: int = 10000
        self.SA_MAX_REPAIR_ATTEMPTS: int = 10

        # Genetic Algorithm
        self.GA_DEFAULT_POPULATION_SIZE: int = 300
        self.GA_DEFAULT_ELITE_SIZE: int = 20
        self.GA_DEFAULT_MUTATION_RATE: float = 0.15
        self.GA_DEFAULT_CROSSOVER_RATE: float = 0.85
        self.GA_DEFAULT_TOURNAMENT_SIZE: int = 5
        self.GA_DEFAULT_MAX_AGE: int = 100
        self.GA_DEFAULT_CONVERGENCE_THRESHOLD: int = 300
        self.GA_DEFAULT_MAX_ITERATIONS: int = 10000
        self.GA_MAX_REPAIR_ATTEMPTS: int = 5

        # --- PPO & RL Configurations ---
        # Transformer model
        self.EMBEDDING_DIM: int = 128
        self.FEATURES_DIM: int = 128
        self.TRANSFORMER_HEADS: int = 4
        self.TRANSFORMER_LAYERS: int = 4
        self.TRANSFORMER_DROPOUT: float = 0.1

        # Policy network
        self.POLICY_NET_ARCH: int = 128
        self.POLICY_NET_LAYERS: int = 2
        self.VALUE_NET_ARCH: int = 128
        self.VALUE_NET_LAYERS: int = 2

        # Learning rate scheduler
        self.LEARNING_RATE_SCHEDULE_TYPE: str = "linear"
        self.LEARNING_RATE_INITIAL: float = 1e-4
        self.LEARNING_RATE_FINAL: float = 1e-8
        self.LEARNING_RATE: float = 1e-4

        # PPO training hyperparameters
        self.NUM_ENVS: int = 8
        self.N_STEPS: int = 512
        self.TOTAL_TIMESTEPS: int = 5_000_000
        self.GAMMA: float = 0.99
        self.GAE_LAMBDA: float = 0.95
        self.CLIP_RANGE: float = 0.2
        self.ENT_COEF: float = 0.05
        self.VF_COEF: float = 0.5
        self.MAX_GRAD_NORM: float = 0.5
        self.BATCH_SIZE: int = 64
        self.N_EPOCHS: int = 10

        # Checkpoint and evaluation
        self.EVAL_FREQUENCY: int = 10000
        self.RESUME_TRAINING: bool = False
        self.PRETRAINED_MODEL_PATH: str = "data/model"
        self.CHECKPOINT_FREQUENCY: int = 50000
        self.SAVE_TRAINING_STATE: bool = True

        # Ensure critical paths exist
        self.CACHE_PATH.mkdir(parents=True, exist_ok=True)
        self.LOG_PATH.mkdir(parents=True, exist_ok=True)
</file>

<file path="src/config/graph_config.py">
"""
Manages loading and accessing configuration from graph_config.yaml.

This module provides a centralized, read-only interface to the graph
generation configuration, ensuring that the YAML file is loaded only once.
It also builds convenient reverse mappings and helper functions to query
node definitions by various attributes like category or RGB color.
"""

import yaml
from pathlib import Path
from typing import Dict, Any, List, Tuple, Optional

_config_cache: Optional[Dict[str, Any]] = None
_rgb_to_name_map_cache: Optional[Dict[Tuple[int, int, int], str]] = None


def get_config() -> Dict[str, Any]:
    """
    Loads graph configuration from YAML file and caches it.

    This function ensures the YAML configuration is read only once and
    provides a consistent, read-only dictionary of settings for the
    entire application.

    Returns:
        A dictionary containing the entire graph configuration.

    Raises:
        FileNotFoundError: If the configuration file cannot be found.
        yaml.YAMLError: If there is an error parsing the YAML file.
    """
    global _config_cache
    if _config_cache is None:
        config_path = (
            Path(__file__).parent.parent.parent / "configs" / "graph_config.yaml"
        )
        if not config_path.exists():
            raise FileNotFoundError(f"Configuration file not found at: {config_path}")

        with open(config_path, "r", encoding="utf-8") as f:
            try:
                _config_cache = yaml.safe_load(f)
            except yaml.YAMLError as e:
                # Log or handle the error appropriately
                raise e
    return _config_cache


def get_node_definitions() -> Dict[str, Dict[str, Any]]:
    """
    Retrieves the node definitions section from the configuration.

    Returns:
        A dictionary where keys are node names (e.g., 'Wall') and
        values are dictionaries of their properties.
    """
    config = get_config()
    return config.get("node_definitions", {})


def get_rgb_to_name_map() -> Dict[Tuple[int, int, int], str]:
    """
    Creates and caches a mapping from RGB color tuples to node names.

    This is essential for the image processing step, allowing for quick
    identification of node types based on pixel color.

    Returns:
        A dictionary mapping RGB tuples to their corresponding node names.
    """
    global _rgb_to_name_map_cache
    if _rgb_to_name_map_cache is None:
        node_defs = get_node_definitions()
        _rgb_to_name_map_cache = {
            tuple(properties["rgb"]): node_name
            for node_name, properties in node_defs.items()
            if "rgb" in properties
        }
    return _rgb_to_name_map_cache


def get_nodes_by_category(category: str) -> List[str]:
    """
    Finds all node names belonging to a specific category.

    Args:
        category: The category to filter by (e.g., 'SLOT', 'PATH').

    Returns:
        A list of node names that match the given category.
    """
    node_defs = get_node_definitions()
    return [
        node_name
        for node_name, properties in node_defs.items()
        if properties.get("category") == category
    ]


def get_geometry_config() -> Dict[str, float]:
    """
    Retrieves the geometry settings from the configuration.

    Returns:
        A dictionary with geometry-related parameters like scale and speed.
    """
    config = get_config()
    return config.get("geometry", {})


def get_super_network_config() -> Dict[str, Any]:
    """
    Retrieves the super_network settings from the configuration.

    Returns:
        A dictionary with super_network-related parameters.
    """
    config = get_config()
    return config.get("super_network", {})


def get_special_ids() -> Dict[str, int]:
    """
    Retrieves the special ID mappings from the configuration.

    Returns:
        A dictionary mapping special area names to their integer IDs.
    """
    config = get_config()
    return config.get("special_ids", {})


_plotter_config_cache: Optional[Dict[str, Any]] = None


def get_plotter_config() -> Dict[str, Any]:
    """
    Loads plotter configuration from plotter.yaml and caches it.

    Returns:
        A dictionary containing the plotter configuration.
    """
    global _plotter_config_cache
    if _plotter_config_cache is None:
        config_path = Path(__file__).parent.parent.parent / "configs" / "plotter.yaml"
        if not config_path.exists():
            raise FileNotFoundError(
                f"Plotter configuration file not found at: {config_path}"
            )

        with open(config_path, "r", encoding="utf-8") as f:
            try:
                _plotter_config_cache = yaml.safe_load(f)
            except yaml.YAMLError as e:
                raise e
    return _plotter_config_cache
</file>

<file path="src/config/path_manager.py">
"""
Manages project paths based on a centralized YAML configuration.

This module loads path definitions from `configs/paths.yaml`, resolves them
relative to the project root, and provides a simple interface to access
them as `pathlib.Path` objects. It supports variable interpolation to
avoid path repetition.
"""

import yaml
from pathlib import Path
from typing import Dict, Optional

_paths_cache: Optional[Dict[str, Path]] = None
_project_root: Optional[Path] = None


def get_project_root() -> Path:
    """
    Determines and returns the project root directory.

    The project root is assumed to be the parent directory of the 'src' folder.

    Returns:
        A Path object representing the project's root directory.
    """
    global _project_root
    if _project_root is None:
        # Assumes this file is in src/config/path_manager.py
        _project_root = Path(__file__).parent.parent.parent
    return _project_root


def get_path(name: str, create_if_not_exist: bool = False) -> Path:
    """
    Retrieves a resolved, absolute path by its name from the configuration.

    Args:
        name: The key of the path in the `paths.yaml` file (e.g., 'network_dir').
        create_if_not_exist: If True, ensures the directory exists.

    Returns:
        A Path object for the requested path.

    Raises:
        KeyError: If the path name is not found in the configuration.
    """
    global _paths_cache
    if _paths_cache is None:
        _load_paths()

    if _paths_cache and name in _paths_cache:
        path = _paths_cache[name]
        if create_if_not_exist and not path.exists():
            path.mkdir(parents=True, exist_ok=True)
        return path
    else:
        raise KeyError(f"Path '{name}' not found in paths configuration.")


def _load_paths():
    """
    Loads and resolves all paths from the YAML configuration file.

    This internal function reads `configs/paths.yaml`, resolves paths relative
    to the project root, and performs variable substitution.
    """
    global _paths_cache
    _paths_cache = {}
    root = get_project_root()
    config_path = root / "configs" / "paths.yaml"

    if not config_path.exists():
        raise FileNotFoundError(f"Paths configuration file not found at {config_path}")

    with open(config_path, "r", encoding="utf-8") as f:
        raw_paths = yaml.safe_load(f)

    # Simple interpolation for variables like {data_dir}
    resolved_paths = {}
    for key, value in raw_paths.items():
        # Format the string with already resolved paths
        formatted_value = value.format(**resolved_paths)
        resolved_paths[key] = formatted_value

    # Convert to absolute Path objects
    for key, value in resolved_paths.items():
        _paths_cache[key] = root / value
</file>

<file path="src/plotting/__init__.py">
# src/plotting/__init__.py
"""Plotting module for network visualization."""

from .plotter import BasePlotter, PlotlyPlotter  # MatplotlibPlotter can be added later

__all__ = ["BasePlotter", "PlotlyPlotter"]
</file>

<file path="src/utils/logger.py">
# src/utils/logger.py

import sys
from typing import Any, Optional
import json
import pathlib
import pickle
import numpy as np
import multiprocessing
from loguru import logger


class NpEncoder(json.JSONEncoder):
    """自定义JSON编码器，以处理Numpy数据类型和路径对象。"""

    def default(self, obj: Any) -> Any:
        if isinstance(obj, np.integer):
            return int(obj)
        if isinstance(obj, np.floating):
            return float(obj)
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        if isinstance(obj, (pathlib.Path, pathlib.PosixPath, pathlib.WindowsPath)):
            return str(obj)
        return super(NpEncoder, self).default(obj)


def setup_logger(
    name: str, log_file: Optional[pathlib.Path] = None, level: int = 20
) -> Any:
    """配置并返回loguru日志记录器。

    Args:
        name (str): 日志记录器的名称（用于上下文标识）。
        log_file (Optional[pathlib.Path]): 可选的日志文件路径。
        level (int): 日志级别，默认为20（INFO）。

    Returns:
        logger: 配置好的loguru日志记录器实例。
    """
    # 转换标准logging级别到loguru级别
    level_map = {10: "DEBUG", 20: "INFO", 30: "WARNING", 40: "ERROR", 50: "CRITICAL"}
    log_level = level_map.get(level, "INFO")

    # 检查是否为多进程环境中的子进程
    is_main_process = True
    try:
        current_process = multiprocessing.current_process()
        if current_process.name != "MainProcess":
            is_main_process = False
    except Exception as e:
        logger.error(f"无法确定进程类型，假设为主进程: {e}")


    # 移除现有的handlers以避免重复配置
    logger.remove()

    # 配置控制台输出（彩色日志）
    console_format = (
        "<green>{time:YYYY-MM-DD HH:mm:ss.SSS}</green> | "
        "<level>{level: <8}</level> | "
        "<cyan>{extra[module]}</cyan> | "
        "<level>{message}</level>"
    )

    # 为主进程和子进程设置不同的日志级别
    console_level = log_level if is_main_process else "WARNING"

    logger.add(
        sys.stdout,
        format=console_format,
        level=console_level,
        colorize=True,
        backtrace=True,
        diagnose=True,
    )

    # 如果提供了日志文件路径且在主进程中，添加文件处理器
    if log_file and is_main_process:
        log_file.parent.mkdir(parents=True, exist_ok=True)

        file_format = (
            "{time:YYYY-MM-DD HH:mm:ss.SSS} | {level: <8} | {extra[module]} | {message}"
        )

        logger.add(
            str(log_file),
            format=file_format,
            level=log_level,
            rotation="10 MB",
            retention="30 days",
            compression="zip",
            encoding="utf-8",
        )

    # 绑定模块名称到logger上下文
    return logger.bind(module=name)


def save_json(data: dict, path: pathlib.Path):
    """使用自定义编码器将字典保存为JSON文件。"""
    with open(path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2, cls=NpEncoder)


def load_json(path: pathlib.Path) -> dict:
    """从JSON文件加载字典。"""
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)


def save_pickle(data: Any, path: pathlib.Path):
    """将任何Python对象序列化为pickle文件。"""
    with open(path, "wb") as f:
        pickle.dump(data, f)


def load_pickle(path: pathlib.Path) -> Any:
    """从pickle文件加载Python对象。"""
    with open(path, "rb") as f:
        return pickle.load(f)
</file>

<file path="src/analysis/slots_exporter.py">
"""Exports service time information for SLOT nodes to a CSV file."""

import pandas as pd
import networkx as nx
import logging
from pathlib import Path
from typing import List, Dict, Any

logger = logging.getLogger(__name__)


def export_slots_to_csv(
    graph: nx.Graph, output_dir: Path, output_filename: str = "slots.csv"
) -> None:
    """
    Extracts nodes with the 'SLOT' category and writes all their attributes
    to a CSV file.

    Args:
        graph: The input NetworkX graph.
        output_dir: The directory to save the CSV file.
        output_filename: The name of the output CSV file.
    """
    if not graph.nodes:
        logger.warning("Graph is empty. Cannot export slots.")
        return

    slot_nodes: List[Dict[str, Any]] = []
    for node_id, data in graph.nodes(data=True):
        if data.get("category") == "SLOT":
            node_data = data.copy()
            node_data["id"] = node_id
            slot_nodes.append(node_data)

    if not slot_nodes:
        logger.warning("No nodes with category 'SLOT' found in the graph.")
        return

    df = pd.DataFrame(slot_nodes)

    output_path = output_dir / output_filename
    try:
        # The columns will be dynamically determined by the keys in the node data
        df.to_csv(output_path, index=False, encoding="utf-8")
        logger.info(f"Successfully exported {len(df)} slots to {output_path}")
    except IOError as e:
        logger.error(f"Failed to write slots CSV to {output_path}: {e}")
</file>

<file path="src/config/__init__.py">
"""Initializes the config package."""

from . import graph_config as graph_config
from . import path_manager as path_manager
from . import config_loader as config_loader

__all__ = ["graph_config", "path_manager", "config_loader"]
</file>

<file path="src/config/config_loader.py">
from omegaconf import OmegaConf
from pathlib import Path
from typing import Any

class ConfigLoader:

    agent: Any
    constraints: Any
    graph_config: Any
    paths: Any
    pathways: Any
    plotter: Any

    _instance = None
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance._load_configs()
        return cls._instance
    
    def _load_configs(self) -> None:
        config_dir = Path(__file__).parent.parent.parent / "configs"
        
        for config_path in config_dir.glob("*.yaml"):
            config_name = config_path.stem
            config_data = OmegaConf.load(config_path)
            setattr(self, config_name, config_data)

    def __getattr__(self, name):
        raise AttributeError(f"'{self.__class__.__name__}' object has no attribute '{name}'.")
</file>

<file path="src/network/graph_manager.py">
"""Manages the network graph structure, node storage, and ID generation."""

import itertools
import networkx as nx
import numpy as np
from src.utils.logger import setup_logger
from typing import Dict, Optional, Iterator, List, Tuple, Any

from src.config import graph_config

logger = setup_logger(__name__)


class GraphManager:
    """Manages the NetworkX graph and provides node/edge manipulation methods."""

    def __init__(self, id_start: int = 1):
        """Initializes the GraphManager.

        Args:
            id_start: The starting value for the node ID counter.
        """
        self._graph = nx.Graph()
        self._id_counter = itertools.count(start=id_start)
        logger.debug(f"GraphManager initialized with starting ID: {id_start}")

    def generate_node_id(self) -> int:
        """Generates a new unique node ID."""
        return next(self._id_counter)

    def get_current_id_value(self) -> int:
        """Gets the next ID value without consuming it."""
        current_value = next(self._id_counter)
        self._id_counter = itertools.chain([current_value], self._id_counter)
        return current_value

    def add_node(self, node_id: int, **attrs: Any) -> None:
        """Adds a node with its attributes to the graph.

        Args:
            node_id: The unique ID for the node.
            **attrs: A dictionary of attributes for the node.
        """
        self._graph.add_node(node_id, **attrs)
        logger.debug(f"Added node: {node_id} with attributes {attrs}")

    def add_edge(self, node1_id: int, node2_id: int, **attrs: Any) -> None:
        """Adds an edge between two nodes, calculating weight if not provided.

        If 'weight' is not in attrs, it calculates the travel time based on
        Euclidean distance and settings from the configuration.

        Args:
            node1_id: The ID of the first node.
            node2_id: The ID of the second node.
            **attrs: A dictionary of attributes for the edge.
        """
        if self._graph.has_node(node1_id) and self._graph.has_node(node2_id):
            if "weight" not in attrs:
                node1_data = self.get_node_attributes(node1_id)
                node2_data = self.get_node_attributes(node2_id)
                if node1_data and node2_data:
                    dist = np.linalg.norm(
                        np.array(
                            [
                                node1_data["pos_x"],
                                node1_data["pos_y"],
                                node1_data["pos_z"],
                            ]
                        )
                        - np.array(
                            [
                                node2_data["pos_x"],
                                node2_data["pos_y"],
                                node2_data["pos_z"],
                            ]
                        )
                    )
                    geo_config = graph_config.get_geometry_config()
                    scale = geo_config.get("pixel_to_meter_scale", 0.1)
                    speed = geo_config.get("pedestrian_speed", 1.2)
                    attrs["weight"] = (dist * scale) / speed

            self._graph.add_edge(node1_id, node2_id, **attrs)
            logger.debug(f"Added edge: {node1_id} -> {node2_id} with {attrs}")
        else:
            logger.warning(
                f"Attempted to connect non-existent node: {node1_id} or {node2_id}"
            )

    def connect_nodes_by_ids(self, node1_id: int, node2_id: int, **attrs: Any) -> None:
        """Alias for add_edge."""
        self.add_edge(node1_id, node2_id, **attrs)

    def get_node_attributes(self, node_id: int) -> Optional[Dict[str, Any]]:
        """Gets the attribute dictionary of a node."""
        if self._graph.has_node(node_id):
            return self._graph.nodes[node_id]
        return None

    def get_all_nodes_data(self) -> Iterator[Tuple[int, Dict[str, Any]]]:
        """Returns an iterator over all nodes and their data."""
        return self._graph.nodes(data=True)

    def remove_node(self, node_id: int) -> None:
        """Removes a node from the graph."""
        if self._graph.has_node(node_id):
            self._graph.remove_node(node_id)
            logger.debug(f"Removed node: {node_id}")
        else:
            logger.warning(f"Attempted to remove non-existent node: {node_id}")

    def get_nodes_by_attribute(
        self, attr_name: str, attr_value: Any
    ) -> List[Tuple[int, Dict[str, Any]]]:
        """Gets a list of nodes matching a specific attribute value."""
        return [
            (n, d)
            for n, d in self._graph.nodes(data=True)
            if d.get(attr_name) == attr_value
        ]

    def get_graph(self) -> nx.Graph:
        """Returns the internal NetworkX graph object."""
        return self._graph

    def get_graph_copy(self) -> nx.Graph:
        """Returns a copy of the internal NetworkX graph object."""
        return self._graph.copy()

    def get_next_available_node_id_estimate(self) -> int:
        """Estimates the next available node ID."""
        return self.get_current_id_value()

    def node_count(self) -> int:
        """Returns the total number of nodes."""
        return self._graph.number_of_nodes()

    def edge_count(self) -> int:
        """Returns the total number of edges."""
        return self._graph.number_of_edges()

    def clear(self) -> None:
        """Clears all nodes and edges from the graph."""
        self._graph.clear()
        logger.debug("GraphManager cleared.")

    def __len__(self) -> int:
        """Returns the number of nodes."""
        return self.node_count()

    def __str__(self) -> str:
        """Returns a string representation of the GraphManager."""
        return f"GraphManager(nodes={self.node_count()}, edges={self.edge_count()})"
</file>

<file path="src/network/node_creators.py">
"""Defines strategies for creating different types of nodes in the network."""

from __future__ import annotations
import abc
import cv2
import numpy as np
from src.utils.logger import setup_logger
from scipy.spatial import KDTree
from typing import Dict, Tuple, Any, Optional, Iterable, TYPE_CHECKING

if TYPE_CHECKING:
    from .network import Network

from src.config import graph_config

logger = setup_logger(__name__)


class BaseNodeCreator(abc.ABC):
    """Abstract base class for node creators."""

    def __init__(self, network: "Network"):
        self.network = network
        self.image_processor = network.image_processor
        self.graph_manager = network.graph_manager
        self.node_defs = graph_config.get_node_definitions()
        self.geometry_config = graph_config.get_geometry_config()
        self.special_ids = graph_config.get_special_ids()
        self.super_network_config = graph_config.get_super_network_config()

    def _get_node_properties(self, name: str) -> Dict[str, Any]:
        return self.node_defs.get(name, {})

    def _create_mask_for_node(
        self,
        image_data: np.ndarray,
        name: str,
        apply_morphology: bool = True,
        morphology_operation: str = "close_open",
        morphology_kernel_size: Optional[Tuple[int, int]] = None,
    ) -> Optional[np.ndarray]:
        node_props = self._get_node_properties(name)
        color_rgb = node_props.get("rgb")
        if not color_rgb:
            logger.warning(f"Color for node '{name}' not found.")
            return None

        mask = np.all(
            image_data == np.array(color_rgb, dtype=np.uint8).reshape(1, 1, 3), axis=2
        )
        mask = mask.astype(np.uint8) * 255

        if apply_morphology:
            kernel_size = self.geometry_config.get("morphology_kernel_size", (5, 5))
            if not isinstance(kernel_size, Iterable):
                kernel_size = (int(kernel_size), int(kernel_size))
            kernel_size = morphology_kernel_size or kernel_size
            mask = self.image_processor.apply_morphology(
                mask, operation=morphology_operation, kernel_size=kernel_size
            )
        return mask

    def _find_connected_components(self, mask: np.ndarray, connectivity: int = 4):
        return cv2.connectedComponentsWithStats(mask, connectivity=connectivity)

    @abc.abstractmethod
    def create_nodes(
        self,
        processed_image_data: np.ndarray,
        id_map: np.ndarray,
        z_level: float,
        floor_num: int,
    ):
        pass


class RoomNodeCreator(BaseNodeCreator):
    """Creates nodes for room-type areas."""

    def create_nodes(
        self,
        processed_image_data: np.ndarray,
        id_map: np.ndarray,
        z_level: float,
        floor_num: int,
    ):
        target_names = graph_config.get_nodes_by_category(
            "SLOT"
        ) + graph_config.get_nodes_by_category("FIXED")
        area_threshold = self.geometry_config.get("area_threshold", 60)

        for name in target_names:
            mask = self._create_mask_for_node(processed_image_data, name)
            if mask is None:
                continue

            retval, labels, stats, centroids = self._find_connected_components(mask)
            if retval <= 1:
                continue

            node_props = self._get_node_properties(name)
            for i in range(1, retval):
                area = float(stats[i, cv2.CC_STAT_AREA])
                if area < area_threshold:
                    continue

                centroid_x, centroid_y = centroids[i]
                node_id = self.graph_manager.generate_node_id()

                self.graph_manager.add_node(
                    node_id,
                    name=name,
                    cname=node_props.get("cname"),
                    rgb=node_props.get("rgb"),
                    code=node_props.get("code"),
                    service_time=node_props.get("service_time", 0),
                    category=node_props.get("category"),
                    pos_x=centroid_x,
                    pos_y=centroid_y,
                    pos_z=z_level,
                    area=area,
                )
                id_map[labels == i] = node_id


class VerticalNodeCreator(BaseNodeCreator):
    """Creates nodes for vertical transport areas."""

    def create_nodes(
        self,
        processed_image_data: np.ndarray,
        id_map: np.ndarray,
        z_level: float,
        floor_num: int,
    ):
        target_names = self.super_network_config.get("vertical_types", [])
        area_threshold = self.geometry_config.get("area_threshold", 60)

        for name in target_names:
            mask = self._create_mask_for_node(processed_image_data, name)
            if mask is None:
                continue

            retval, labels, stats, centroids = self._find_connected_components(mask)
            if retval <= 1:
                continue

            node_props = self._get_node_properties(name)
            for i in range(1, retval):
                area = float(stats[i, cv2.CC_STAT_AREA])
                if area < area_threshold:
                    continue

                centroid_x, centroid_y = centroids[i]
                node_id = self.graph_manager.generate_node_id()

                self.graph_manager.add_node(
                    node_id,
                    name=name,
                    cname=node_props.get("cname"),
                    rgb=node_props.get("rgb"),
                    code=node_props.get("code"),
                    service_time=node_props.get("service_time", 0),
                    time_per_floor=node_props.get("time_per_floor"),
                    category=node_props.get("category"),
                    pos_x=centroid_x,
                    pos_y=centroid_y,
                    pos_z=z_level,
                    area=area,
                )
                id_map[labels == i] = node_id


class MeshBasedNodeCreator(BaseNodeCreator):
    """Base class for creators that generate a mesh of nodes."""

    def _create_mesh_nodes_for_mask(
        self,
        mask: np.ndarray,
        name: str,
        id_map: np.ndarray,
        id_map_value: int,
        z_level: float,
        multiplier: int,
    ):
        id_map[mask != 0] = id_map_value
        retval, labels, stats, _ = self._find_connected_components(mask, connectivity=8)
        grid_size = self.geometry_config.get("grid_size", 40) * multiplier
        area_threshold = self.geometry_config.get("area_threshold", 60)
        node_props = self._get_node_properties(name)

        for i in range(1, retval):
            if stats[i, cv2.CC_STAT_AREA] < area_threshold:
                continue

            x, y, w, h, _ = stats[i]
            gx = np.arange(x + grid_size / 2, x + w, grid_size)
            gy = np.arange(y + grid_size / 2, y + h, grid_size)
            if len(gx) == 0 or len(gy) == 0:
                continue

            grid_x, grid_y = np.meshgrid(gx, gy)
            valid_indices = (
                labels[
                    grid_y.astype(int).clip(0, mask.shape[0] - 1),
                    grid_x.astype(int).clip(0, mask.shape[1] - 1),
                ]
                == i
            )

            valid_x, valid_y = grid_x[valid_indices], grid_y[valid_indices]
            node_ids, positions = [], []
            for vx, vy in zip(valid_x, valid_y):
                node_id = self.graph_manager.generate_node_id()
                self.graph_manager.add_node(
                    node_id,
                    name=name,
                    cname=node_props.get("cname"),
                    rgb=node_props.get("rgb"),
                    code=node_props.get("code"),
                    service_time=node_props.get("service_time", 0),
                    category=node_props.get("category"),
                    pos_x=vx,
                    pos_y=vy,
                    pos_z=z_level,
                    area=1.0,
                )
                node_ids.append(node_id)
                positions.append([vx, vy])

            if len(node_ids) < 2:
                continue
            kdtree = KDTree(positions)
            max_dist = np.sqrt(2) * grid_size * 1.05
            pairs = kdtree.query_pairs(r=max_dist)
            for i_idx, j_idx in pairs:
                self.graph_manager.connect_nodes_by_ids(
                    node_ids[i_idx], node_ids[j_idx]
                )


class PedestrianNodeCreator(MeshBasedNodeCreator):
    """Creates mesh nodes for pedestrian areas."""

    def create_nodes(
        self,
        processed_image_data: np.ndarray,
        id_map: np.ndarray,
        z_level: float,
        floor_num: int,
    ):
        target_names = graph_config.get_nodes_by_category("PATH")
        for name in target_names:
            mask = self._create_mask_for_node(processed_image_data, name)
            if mask is None:
                continue
            self._create_mesh_nodes_for_mask(
                mask, name, id_map, self.special_ids.get("pedestrian", -3), z_level, 1
            )


class OutsideNodeCreator(MeshBasedNodeCreator):
    """Creates mesh nodes for outside areas."""

    def create_nodes(
        self,
        processed_image_data: np.ndarray,
        id_map: np.ndarray,
        z_level: float,
        floor_num: int,
    ):
        ground_floor_num = self.super_network_config.get(
            "ground_floor_number_for_outside"
        )
        if floor_num != ground_floor_num:
            return

        target_names = self.super_network_config.get("outside_types", [])
        for name in target_names:
            mask = self._create_mask_for_node(processed_image_data, name)
            if mask is None:
                continue
            self._create_mesh_nodes_for_mask(
                mask, name, id_map, self.special_ids.get("outside", -1), z_level, 1
            )


class ConnectionNodeCreator(BaseNodeCreator):
    """Creates nodes for connections and links them to adjacent areas."""

    def create_nodes(
        self,
        processed_image_data: np.ndarray,
        id_map: np.ndarray,
        z_level: float,
        floor_num: int,
    ):
        door_names = [
            t
            for t in graph_config.get_nodes_by_category("CONNECTOR")
            if t not in self.super_network_config.get("vertical_types", [])
        ]
        if not door_names:
            return

        # Get masks for collision detection
        corridor_mask = self.network._get_mask("Corridor", is_category=False)
        outdoor_mask = self.network._get_mask("Outdoor", is_category=False)

        room_categories = ["FIXED", "SLOT"]
        room_mask = np.zeros_like(corridor_mask)
        for cat in room_categories:
            room_mask = cv2.bitwise_or(
                room_mask, self.network._get_mask(cat, is_category=True)
            )

        all_connector_names = graph_config.get_nodes_by_category("CONNECTOR")
        other_connector_names = [
            name for name in all_connector_names if name not in door_names
        ]
        other_connector_mask = self.network._get_mask(
            other_connector_names, is_category=False
        )

        kernel_list = self.geometry_config.get(
            "connection_dilation_kernel_size", [3, 3]
        )
        if not (isinstance(kernel_list, list) and len(kernel_list) == 2):
            kernel_list = [3, 3]
        dilation_kernel = np.ones(tuple(kernel_list), np.uint8)
        area_threshold = self.geometry_config.get("area_threshold", 60)

        for name in door_names:
            node_props = self._get_node_properties(name)
            mask = self._create_mask_for_node(processed_image_data, name)
            if mask is None:
                continue

            retval, labels, stats, centroids = self._find_connected_components(mask)
            if retval <= 1:
                continue

            for i in range(1, retval):
                area = float(stats[i, cv2.CC_STAT_AREA])
                if area < area_threshold / 10 and area < 5:
                    continue

                component_mask = (labels == i).astype(np.uint8) * 255
                dilated_mask = cv2.dilate(
                    component_mask, dilation_kernel, iterations=1
                )

                # Check for collisions
                collides_outdoor = np.any(cv2.bitwise_and(dilated_mask, outdoor_mask))
                collides_corridor = np.any(
                    cv2.bitwise_and(dilated_mask, corridor_mask)
                )
                collides_other_connector = np.any(
                    cv2.bitwise_and(dilated_mask, other_connector_mask)
                )
                collides_room = np.any(cv2.bitwise_and(dilated_mask, room_mask))

                door_type = None
                colliding_node_ids = np.unique(id_map[dilated_mask != 0])
                colliding_node_ids = [
                    nid for nid in colliding_node_ids if nid > 0
                ]  # Filter out special IDs

                ground_floor_num = self.super_network_config.get(
                    "ground_floor_number_for_outside"
                )
                if (
                    collides_outdoor
                    and collides_corridor
                    and floor_num == ground_floor_num
                ):
                    door_type = "EXTERIOR"
                elif collides_other_connector and collides_corridor:
                    door_type = "INTERIOR"
                elif collides_room and (collides_corridor or collides_other_connector):
                    door_type = "ROOM"

                centroid_x, centroid_y = centroids[i]
                node_id = self.graph_manager.generate_node_id()

                self.graph_manager.add_node(
                    node_id,
                    name=name,
                    cname=node_props.get("cname"),
                    rgb=node_props.get("rgb"),
                    code=node_props.get("code"),
                    service_time=node_props.get("service_time", 0),
                    category=node_props.get("category"),
                    pos_x=centroid_x,
                    pos_y=centroid_y,
                    pos_z=z_level,
                    area=area,
                    door_type=door_type,
                    colliding_node_ids=colliding_node_ids,
                )
                id_map[labels == i] = node_id
</file>

<file path="src/pipeline/__init__.py">
from .pathway_generator import PathwayGenerator
from .cost_manager import CostManager

__all__ = ["PathwayGenerator", "CostManager"]
</file>

<file path="src/pipeline/cost_manager.py">
import pandas as pd
import numpy as np
from typing import Dict, Any, Tuple, List, cast
from collections import defaultdict
from itertools import product, combinations_with_replacement

from src.config.config_loader import ConfigLoader
from src.utils.logger import setup_logger


class CostManager:
    def __init__(self, config: ConfigLoader):
        self.logger = setup_logger(__name__)
        self.config = config

        self.node_def: Dict[str, Any] = {}
        self.cname_to_name: Dict[str, str] = {}
        self.name_to_name_id: Dict[str, List[str]] = defaultdict(list)
        self.initial_layout: Dict[str, str] = {}
        self.origin_service_cost: float = 0.0
        self.origin_travel_cost: float = 0.0
        self.travel_times: pd.DataFrame = pd.DataFrame(dtype=float)
        self.pair_times: Dict[Tuple[str, str], float] = defaultdict(float)
        self.pair_weights: Dict[Tuple[str, str], float] = defaultdict(float)
        self.np_times: np.ndarray = np.array([])
        self.np_weights: np.ndarray = np.array([])
        self.layout: Dict[str, str] = {}

        self.id_to_name_id: Dict[int, str] = {}
        self.name_id_to_id: Dict[str, int] = {}
        self.name_id_to_area: Dict[str, float] = {}
        self.id_to_area: Dict[int, float] = {}
        self.area_dict: Dict[Tuple[int,int], bool] = {}

        self._load_travel_times()
        self._load_slots_information()
        self._load_node_definitions()

        self.pathways: Dict[str, Dict[str, Any]] = {}
        self.service_weights: Dict[str, float] = defaultdict(float)

    @property
    def origin_total_cost(self) -> float:
        return self.origin_service_cost + self.origin_travel_cost
    
    @property
    def current_total_cost(self) -> float:
        travel_time = np.dot(self.np_times[:, 2], self.np_weights[:, 2])
        return self.origin_service_cost + travel_time
    
    @property
    def current_travel_cost(self) -> float:
        travel_time = np.dot(self.np_times[:, 2], self.np_weights[:, 2])
        return travel_time
    
    @property
    def current_service_cost(self) -> float:
        return self.origin_service_cost

    def _load_travel_times(self) -> None:
        df = pd.read_csv(self.config.paths.travel_times_csv, index_col=0)
        for name_id in df.index:
            name, id_num = name_id.rsplit('_', 1)
            self.name_to_name_id[name].append(name_id)
            self.id_to_name_id[int(id_num)] = name_id
            self.name_id_to_id[name_id] = int(id_num)
        self.travel_times = df.copy()
    
    def _load_slots_information(self) -> None:
        df = pd.read_csv(self.config.paths.slots_csv)
        for _, row in df[['name', 'id', 'area']].iterrows():
            name_id = row['name'] + '_' + str(row['id'])
            self.name_id_to_area[name_id] = float(row['area'])
            self.id_to_area[int(row['id'])] = float(row['area'])
        self.slots = df.copy()

    def _load_node_definitions(self) -> None:
        self.node_def = self.config.graph_config.node_definitions
        self.cname_to_name: Dict[str, str] = {v['cname']: k for k, v in self.node_def.items()}

    def _get_initial_layout(self) -> None:
        for _, row in self.slots[['name', 'id']].iterrows():
            name = row['name'] + '_' + str(row['id'])
            self.initial_layout[name] = name
        self.layout = self.initial_layout.copy()

    def _sort_np_matrices(self):
        sorted_indices = np.lexsort((self.np_times[:, 1], self.np_times[:, 0]))
        self.np_times = self.np_times[sorted_indices]
        sorted_indices = np.lexsort((self.np_weights[:, 1], self.np_weights[:, 0]))
        self.np_weights = self.np_weights[sorted_indices]

    def _precompute_np_matrices(self):

        time_list = []
        for (dept1, dept2), time in self.pair_times.items():
            id1 = self.name_id_to_id.get(dept1)
            id2 = self.name_id_to_id.get(dept2)
            if id1 is None or id2 is None:
                continue
            time_list.append((id1, id2, time))
        self.np_times = np.array(time_list)

        weight_list = []
        for (dept1, dept2), weight in self.pair_weights.items():
            id1 = self.name_id_to_id.get(dept1)
            id2 = self.name_id_to_id.get(dept2)
            if id1 is None or id2 is None:
                continue
            weight_list.append((id1, id2, weight))
        self.np_weights = np.array(weight_list)

    def initialize(self, pathways: Dict[str, Dict[str, Any]], **kwargs):
        self.pathways = pathways
        self._get_initial_layout()
        self._precompute_pair_weights()
        self._precompute_travel_time_cost(self.initial_layout)
        self._precompute_np_matrices()
        self._precompute_area_dict()
        self._sort_np_matrices()
        self.origin_service_cost = self._precompute_service_time_cost()
        self.origin_travel_cost = np.dot(self.np_times[:, 2], self.np_weights[:, 2])
        self.logger.info(f"Initial Service Cost: {self.origin_service_cost:.2f}")
        self.logger.info(f"Initial Travel Cost: {self.origin_travel_cost:.2f}")
        self.logger.info(f"Initial Total Cost: {self.origin_total_cost:.2f}")
        pass

    def _precompute_area_dict(self):
        tolerance = cast(float, self.config.constraints.area_compatibility_tolerance)
        for id1, id2 in combinations_with_replacement(self.id_to_area.keys(), 2):
            area1 = self.id_to_area[id1]
            area2 = self.id_to_area[id2]
            is_compatible = abs(area1 - area2) <= tolerance * min(area1, area2)
            self.area_dict[(id1, id2)] = is_compatible

    def _precompute_pair_weights(self):
        self.logger.info("Precomputing pair weights for all pathways.")

        for dept1 in self.travel_times.index:
            for dept2 in self.travel_times.columns:
                if dept1 == dept2:
                    continue
                elif (dept2, dept1) in self.pair_weights:
                    continue
                self.pair_weights[(dept1, dept2)] = 0.0

        for pathway in self.pathways.values():
            sequence: List[str] = pathway.get('core_sequence', [])
            weight = pathway.get('weight', 1.0)
            start_nodes: List[str] = pathway.get('start_nodes', [])
            end_nodes: List[str] = pathway.get('end_nodes', [])

            for i, dept in enumerate(sequence):
                dept = self.cname_to_name.get(dept, dept)
                self.service_weights[dept] += weight
                if i > 0:
                    prev_dept = self.cname_to_name.get(sequence[i - 1], sequence[i - 1])
                    pairs = list(product(self.name_to_name_id.get(prev_dept, [prev_dept]), self.name_to_name_id.get(dept, [dept])))
                    for pair in pairs:
                        reversed_pair = (pair[1], pair[0])
                        if reversed_pair in self.pair_weights:
                            self.pair_weights[reversed_pair] += weight / len(pairs)
                            continue
                        self.pair_weights[pair] += weight / len(pairs)

            for start_node in start_nodes:
                start_node = self.cname_to_name.get(start_node, start_node)
                dept = self.cname_to_name.get(sequence[0], sequence[0])
                pairs: List[Tuple[str, str]] = list(product(self.name_to_name_id.get(start_node, [start_node]), self.name_to_name_id.get(dept, [dept])))
                for pair in pairs:
                    reversed_pair = (pair[1], pair[0])
                    if reversed_pair in self.pair_weights:
                        self.pair_weights[reversed_pair] += weight / len(pairs)
                        continue
                    self.pair_weights[pair] += weight / len(pairs)

            for i, end_node in enumerate(end_nodes):
                end_node = self.cname_to_name.get(end_node, end_node)
                if i == 0:
                    dept = self.cname_to_name.get(sequence[-1], sequence[-1])
                    pairs = list(product(self.name_to_name_id.get(dept, [dept]), self.name_to_name_id.get(end_node, [end_node])))
                else:
                    dept = self.cname_to_name.get(end_nodes[i - 1], end_nodes[i - 1])
                    pairs = list(product(self.name_to_name_id.get(dept, [dept]), self.name_to_name_id.get(end_node, [end_node])))
                for pair in pairs:
                    reversed_pair = (pair[1], pair[0])
                    if reversed_pair in self.pair_weights:
                        self.pair_weights[reversed_pair] += weight / len(pairs)
                        continue
                    self.pair_weights[pair] += weight / len(pairs)
        self.logger.info(f"Total {len(self.pair_weights)} pairs computed.")

    def _precompute_service_time_cost(self) -> float:

        service_cost = 0.0
        for dept in self.service_weights:
            service_time = self.node_def.get(dept, {}).get('service_time', 0)
            service_cost += service_time * self.service_weights[dept]
        return service_cost

    def _precompute_travel_time_cost(self, layout: Dict[str, str]) -> None:
        for (dept1, dept2), _ in self.pair_weights.items():
            slot1 = layout.get(dept1, dept1)
            slot2 = layout.get(dept2, dept2)
            time = cast(float, self.travel_times.loc[slot1, slot2])
            self.pair_times[(dept1, dept2)] += time

    

    def swap(self, dept1: str, dept2: str) -> float:
        slot1 = self.layout[dept1]
        slot2 = self.layout[dept2]

        id1 = self.name_id_to_id.get(slot1, -1)
        id2 = self.name_id_to_id.get(slot2, -1)

        if self.area_dict.get((id1, id2), False) is False:
            self.layout[dept1] = slot1
            self.layout[dept2] = slot2
            self.logger.warning(f"Swap between {dept1} and {dept2} violates area compatibility constraint. Swap reverted.")
            return -np.inf

        mask_id1 = self.np_times[:, :2] == id1
        mask_id2 = self.np_times[:, :2] == id2

        self.np_times[:, :2][mask_id1] = id2
        self.np_times[:, :2][mask_id2] = id1

        self._sort_np_matrices()

        self.layout[dept1] = slot2
        self.layout[dept2] = slot1

        return self.current_travel_cost
</file>

<file path="configs/graph_config.yaml">
# configs/node_def.yaml

geometry:
  pixel_to_meter_scale: 0.1       # 像素到米的转换比例 (米/像素)
  pedestrian_speed: 1.2           # 行人平均步行速度 (米/秒)
  grid_size: 40                   # mesh节点生成的基础网格大小
  morphology_kernel_size: [5, 5]  # 形态学操作的默认核大小
  area_threshold: 60              # 节点连通区域最小面积阈值
  mesh_node_connectivity_k: 9     # 网格节点连接的k-nearest neighbors
  connection_dilation_kernel_size: [3, 3] # 连接节点（门）的膨胀核大小

special_ids:
  background: -2
  outside: -1
  pedestrian: -3

super_network:
  generate_outside_nodes: false      # 是否生成室外节点
  ground_floor_number_for_outside: 0 # 指定用于处理室外节点的地面楼层，0代表第一层
  default_floor_height: 4
  default_vertical_connection_tolerance: 0
  estimated_max_nodes_per_floor: 10000
  z_level_diff_threshold: 1.0
  min_vertical_tolerance: 10
  vertical_tolerance_factor: 0.5
  vertical_types: ["Stairs", "Elevator", "Escalator"]
  outside_types: ["Outdoor"]

node_definitions:
  # name (key): 英文名称
  # - cname: 中文名称
  # - rgb: [R, G, B]
  # - code: 标签
  # - service_time: 节点服务时间 (秒) 
  # - category: 节点类别 (STRUCTURE, SLOT, PATH, CONNECTOR, FIXED)

  # --- 结构类 (STRUCTURE) ---
  Wall:
    cname: "墙"
    rgb: [255, 235, 59]
    code: "Z6"
    service_time: 0
    category: "STRUCTURE"
  Greening:
    cname: "绿化"
    rgb: [76, 175, 80]
    code: "Z5"
    service_time: 0
    category: "STRUCTURE"
  Handrail:
    cname: "栏杆"
    rgb: [251, 242, 159]
    code: "Z10"
    service_time: 0
    category: "STRUCTURE"
  Courtyard:
    cname: "中庭"
    rgb: [179, 116, 190]
    code: "Z9"
    service_time: 0
    category: "STRUCTURE"
  VacantRoom:
    cname: "空房间"
    rgb: [255, 255, 255]
    code: "Z11"
    service_time: 0
    category: "STRUCTURE"

  # --- 路径类 (PATH) ---
  # PATH节点的service_time为0, 通行时间由边权重决定。
  Corridor:
    cname: "走廊"
    rgb: [207, 216, 220]
    code: "Z1"
    service_time: 0
    category: "PATH"

  # --- 室外类 (OUTSIDE) ---
  # OUTSIDE节点的service_time为0, 通行时间由边权重决定。
  Outdoor:
    cname: "室外"
    rgb: [156, 39, 176]
    code: "Z8"
    service_time: 0
    category: "OUTSIDE"

  # --- 连接器类 (CONNECTOR) ---
  Door:
    cname: "门"
    rgb: [121, 85, 72]
    code: "Z7"
    service_time: 3.0 # 通过门所需的时间
    category: "CONNECTOR"
    time_per_floor: null
    door_type: null # "EXTERIOR", "INTERIOR", "ROOM", or null
  Stairs:
    cname: "楼梯"
    rgb: [117, 117, 117]
    code: "Z2"
    service_time: 0.0 # 上下一层楼的平均时间
    category: "CONNECTOR"
    time_per_floor: 15.0
  Elevator:
    cname: "电梯"
    rgb: [189, 189, 189]
    code: "Z3"
    service_time: 0.0 # 含等待时间
    category: "CONNECTOR"
    time_per_floor: 10.0
  Escalator:
    cname: "扶梯"
    rgb: [158, 158, 158]
    code: "Z4"
    service_time: 0.0 # 上下一层楼的平均时间
    category: "CONNECTOR"
    time_per_floor: 10.0

  # --- 固定功能类 (FIXED) ---
  RegistrationFee:
    cname: "挂号收费"
    rgb: [0, 150, 136]
    code: "R1-R4"
    service_time: 678
    category: "FIXED"
  Emergency:
    cname: "急诊科"
    rgb: [103, 58, 183]
    code: "D26"
    service_time: 9000
    category: "FIXED"

  # --- 可布局槽位类 (SLOT) ---
  Pharmacy:
    cname: "内诊药房"
    rgb: [244, 67, 54]
    code: "T16"
    service_time: 781
    category: "SLOT"
  GeneralPractice:
    cname: "全科"
    rgb: [33, 150, 243]
    code: "D1"
    service_time: 1965
    category: "SLOT"
  Radiology:
    cname: "放射科"
    rgb: [3, 169, 244]
    code: "T2"
    service_time: 1484
    category: "SLOT"
  Pediatrics:
    cname: "儿科"
    rgb: [0, 188, 212]
    code: "D2"
    service_time: 5632
    category: "SLOT"
  EndoscopyCenter:
    cname: "内镜中心"
    rgb: [139, 195, 74]
    code: "T7"
    service_time: 5333
    category: "SLOT"
  ClinicalLaboratory:
    cname: "检验中心"
    rgb: [205, 220, 57]
    code: "T4"
    service_time: 180
    category: "SLOT"
  Gastroenterology:
    cname: "消化内科"
    rgb: [255, 193, 7]
    code: "D10"
    service_time: 916
    category: "SLOT"
  ThyroidSurgery:
    cname: "甲状腺外科"
    rgb: [255, 152, 0]
    code: "D18"
    service_time: 5637
    category: "SLOT"
  Nephrology:
    cname: "肾内科"
    rgb: [254, 87, 34]
    code: "D6"
    service_time: 1493
    category: "SLOT"
  CardiovascularMedicine:
    cname: "心血管内科"
    rgb: [169, 238, 90]
    code: "D8"
    service_time: 8089
    category: "SLOT"
  BloodCollection:
    cname: "采血处"
    rgb: [88, 67, 60]
    code: "T15"
    service_time: 1104
    category: "SLOT"
  Ophthalmology:
    cname: "眼科"
    rgb: [239, 199, 78]
    code: "D22"
    service_time: 3228
    category: "SLOT"
  ChineseMedicine:
    cname: "中医科"
    rgb: [253, 186, 87]
    code: "D3"
    service_time: 3110
    category: "SLOT"
  Otolaryngology:
    cname: "耳鼻喉科"
    rgb: [250, 133, 96]
    code: "D23"
    service_time: 9550
    category: "SLOT"
  DentalClinic1:
    cname: "口腔一区"
    rgb: [197, 254, 130]
    code: "D24"
    service_time: 10542
    category: "SLOT"
  Ultrasound:
    cname: "超声科"
    rgb: [173, 133, 11]
    code: "T1"
    service_time: 3023
    category: "SLOT"
  Pathology:
    cname: "病理科"
    rgb: [119, 90, 10]
    code: "T5"
    service_time: 0
    category: "SLOT"
  Orthopedics:
    cname: "骨科"
    rgb: [250, 146, 138]
    code: "D7"
    service_time: 1090
    category: "SLOT"
  Urology:
    cname: "泌尿外科"
    rgb: [255, 128, 171]
    code: "D12"
    service_time: 1019
    category: "SLOT"
  HepatobiliaryPancreatic:
    cname: "肝胆胰外科"
    rgb: [33, 250, 230]
    code: "D19"
    service_time: 2953
    category: "SLOT"
  Dermatology:
    cname: "皮肤科"
    rgb: [82, 108, 255]
    code: "D21"
    service_time: 2802
    category: "SLOT"
  Gynaecology:
    cname: "妇科"
    rgb: [226, 58, 255]
    code: "D15"
    service_time: 5000
    category: "SLOT"
  Obstetrics:
    cname: "产科"
    rgb: [100, 139, 55]
    code: "D14"
    service_time: 2208
    category: "SLOT"
  DeliveryRoom:
    cname: "产房"
    rgb: [170, 190, 150]
    code: "T9"
    service_time: 0
    category: "SLOT"
  Theater:
    cname: "手术室"
    rgb: [113, 134, 91]
    code: "T13"
    service_time: 0
    category: "SLOT"
  AmbulatorySurgery:
    cname: "门诊手术室"
    rgb: [175, 207, 142]
    code: "T12"
    service_time: 0
    category: "SLOT"
  DentalClinic2:
    cname: "口腔二区"
    rgb: [232, 137, 248]
    code: "D25"
    service_time: 4964
    category: "SLOT"
  Neurology:
    cname: "神经内科"
    rgb: [63, 100, 23]
    code: "D9"
    service_time: 2396
    category: "SLOT"
  Respiratory:
    cname: "呼吸内科"
    rgb: [240, 222, 165]
    code: "D5"
    service_time: 1457
    category: "SLOT"
  LaserClinic:
    cname: "综合激光科"
    rgb: [187, 24, 80]
    code: "D20"
    service_time: 1848
    category: "SLOT"
  HaemodialysisUnit:
    cname: "透析中心"
    rgb: [150, 133, 179]
    code: "T8"
    service_time: 44267
    category: "SLOT"
  Oncology:
    cname: "肿瘤科"
    rgb: [112, 40, 236]
    code: "D11"
    service_time: 11729
    category: "SLOT"
  PrenatalDiagnosis:
    cname: "产前诊断门诊"
    rgb: [241, 190, 186]
    code: "D13"
    service_time: 1252
    category: "SLOT"
  PhysicalExamination:
    cname: "体检科"
    rgb: [186, 146, 160]
    code: "D4"
    service_time: 7336
    category: "SLOT"
  ReproductiveMedicine:
    cname: "生殖医学科"
    rgb: [71, 195, 180]
    code: "D16"
    service_time: 1086
    category: "SLOT"
  PlasticSurgery:
    cname: "烧伤整形科"
    rgb: [187, 152, 247]
    code: "D17"
    service_time: 854
    category: "SLOT"
  InterventionalTherapy:
    cname: "介入科"
    rgb: [254, 210, 145]
    code: "T6"
    service_time: 14688
    category: "SLOT"
  CSSD:
    cname: "中心供应室"
    rgb: [145, 102, 86]
    code: "T3"
    service_time: 0
    category: "SLOT"
  NICU:
    cname: "NICU"
    rgb: [240, 61, 123]
    code: "T10"
    service_time: 0
    category: "SLOT"
  ICU:
    cname: "ICU"
    rgb: [250, 162, 193]
    code: "T11"
    service_time: 0
    category: "SLOT"
  PIVAS:
    cname: "静配中心"
    rgb: [252, 201, 126]
    code: "T14"
    service_time: 0
    category: "SLOT"
</file>

<file path="configs/paths.yaml">
# Paths are relative to the project root directory.

# Source data directories
data_dir: "data"
label_dir: "${data_dir}/label"

# Output directories
results_dir: "results"
network_dir: "${results_dir}/network"
model_dir: "${results_dir}/model"
debug_dir: "debug"

# Specific file paths
travel_times_csv: "${network_dir}/hospital_travel_times.csv"
slots_csv: "${network_dir}/slots.csv"
vis_html: "${network_dir}/hospital_network_3d.html"
graph_pkl: "${network_dir}/hospital_network.pkl"
</file>

<file path="configs/pathways.yaml">
pathways_number: 60


training_generation:
  # 1. 可互换的科室分组
  department_pools:
    DiagnosticImaging: ["放射科", "超声科"]
    LabWork: ["采血处", "检验中心"]
    PrimaryCare: ["全科", "体检科", "中医科", "眼科", "口腔一区", "口腔二区"]
    InternalMedicine: ["呼吸内科", "肾内科", "心血管内科", "神经内科", "消化内科", "肿瘤科"]
    SurgicalDepts: ["甲状腺外科", "肝胆胰外科", "泌尿外科", "烧伤整形科", "骨科", "介入科"]
    GynAndPeds: ["妇科", "产科", "儿科", "产前诊断门诊", "生殖医学科"]

  # 2. 可复用的常见子流程
  sequence_fragments:
    StandardCheckup:
      - { type: pool, pool_name: LabWork, probability: 0.9 }
      - { type: pool, pool_name: DiagnosticImaging, probability: 0.6 }
    SurgicalPreOp: # 外科术前检查
      - { type: fixed, name: "采血处" }
      - { type: fixed, name: "放射科" }
      - { type: fixed, name: "心血管内科" } # 通常需要心电图

  # 3. 就医流程
  meta_rules:
    - id: ROUTINE_INTERNAL_MEDICINE
      description: "常规内科门诊流程"
      base_weight: 1
      start_nodes: ["门"]
      end_nodes: ["内诊药房", "门"]
      structure:
        - { type: primary_department, from_pool: InternalMedicine }
        - { type: optional, probability: 0.8, step: { type: fragment, fragment_name: "StandardCheckup" } }
        - { type: repeat, target: primary_department }

    - id: SURGICAL_CONSULTATION
      description: "外科咨询流程"
      base_weight: 1
      start_nodes: ["门"]
      end_nodes: ["门"]
      structure:
        - { type: primary_department, from_pool: SurgicalDepts }
        - { type: optional, probability: 0.9, step: { type: fragment, fragment_name: "SurgicalPreOp" } }
        - { type: repeat, target: primary_department } 

    - id: GYN_PEDS_VISIT
      description: "妇产儿科流程"
      base_weight: 1
      start_nodes: ["门"]
      end_nodes: ["内诊药房", "门"]
      structure:
        - { type: primary_department, from_pool: GynAndPeds }
        - { type: optional, probability: 0.7, step: { type: fixed, name: "超声科" } }
        - { type: optional, probability: 0.5, step: { type: fixed, name: "采血处" } }
        - { type: repeat, target: primary_department } # 会出现['妇科', '妇科']，但不影响通行时间

    - id: SPECIAL_EXAM_VISIT
      description: "特殊检查流程"
      base_weight: 1
      start_nodes: ["门"]
      end_nodes: ["内诊药房", "门"]
      structure:
        - { type: primary_department, from_pool: ["消化内科", "肝胆胰外科"] }
        - { type: fixed, name: "内镜中心" }
        - { type: repeat, target: primary_department }

    - id: SIMPLE_FOLLOW_UP
      description: "简单复诊流程"
      base_weight: 1
      start_nodes: ["门"]
      end_nodes: ["内诊药房", "门"]
      structure:
        - { type: primary_department, from_pool: PrimaryCare }

# ===================================================================
# Part 2: Fixed Scenarios for Evaluation (Evaluation Phase)
# ===================================================================
evaluation_scenarios:
  smart:
    description: "智慧医院流程评估场景"
    file_path: "data/process_smart.json"
  traditional:
    description: "传统医院流程评估场景"
    file_path: "data/process_traditional.json"
</file>

<file path="src/network/floor_manager.py">
"""
Manages floor detection from filenames and Z-level calculations for SuperNetwork.
"""

import re
import pathlib
from typing import List, Dict, Tuple, Optional
from src.utils.logger import setup_logger

logger = setup_logger(__name__)


class FloorManager:
    """
    Handles detection of floor numbers from image filenames and calculation
    of their corresponding Z-coordinate levels.
    """

    def __init__(self, base_floor_default: int = 0, default_floor_height: float = 10.0):
        """
        Initializes the FloorManager.

        Args:
            base_floor_default: The default starting floor number if none can be detected.
            default_floor_height: The default height difference between adjacent floors.
        """
        self.base_floor_default = base_floor_default
        self.default_floor_height = default_floor_height
        self._floor_patterns = {
            # Order matters: more specific patterns first
            # B-1, B1, b-1, b1 -> negative
            r"([Bb])-?(\d+)": lambda m: -int(m.group(2)),
            # -1F, -2f -> negative
            r"-(\d+)[Ff]": lambda m: -int(m.group(1)),
            r"([Ll])(\d+)": lambda m: int(m.group(2)) - 1,  # L1, l2 -> positive, 1-based to 0-based
            r"(\d+)[Ff]": lambda m: int(m.group(1)) - 1,  # 1F, 2f -> positive, 1-based to 0-based
            # Add more general patterns if needed, like just a number if no prefix/suffix
            # r'_(\d+)_': lambda m: int(m.group(1)) # Example: floor_1_plan.png
        }

    def detect_floor_from_filename(self, file_path: pathlib.Path) -> Optional[int]:
        """
        Detects the floor number from a filename.

        Args:
            file_path: Path object of the image file.

        Returns:
            The detected floor number (integer) or None if not detected.
        """
        filename = file_path.stem  # Get filename without extension
        for pattern, converter in self._floor_patterns.items():
            match = re.search(pattern, filename)
            if match:
                try:
                    return converter(match)
                except ValueError:
                    continue  # Conversion failed, try next pattern
        return None

    def auto_assign_floors(
        self, image_paths: List[pathlib.Path]
    ) -> Tuple[Dict[pathlib.Path, int], Dict[int, pathlib.Path]]:
        """
        Assigns floor numbers to a list of image paths.

        Attempts to detect from filenames first. If unsuccessful for some or all,
        assigns sequentially based on sort order or a defined starting floor.

        Args:
            image_paths: A list of Path objects for the images.

        Returns:
            A tuple containing:
                - path_to_floor_map (Dict[pathlib.Path, int]): Maps image path to floor number.
                - floor_to_path_map (Dict[int, pathlib.Path]): Maps floor number to image path.
                                                               (Assumes one image per floor for this map)
        """
        path_to_floor_map: Dict[pathlib.Path, int] = {}
        detected_floors: Dict[pathlib.Path, int] = {}
        undetected_paths: List[pathlib.Path] = []

        for p_path in image_paths:
            floor = self.detect_floor_from_filename(p_path)
            if floor is not None:
                if floor in path_to_floor_map.values():
                    logger.warning(
                        f"Warning: Duplicate floor number {floor} detected. Check filenames."
                    )
                detected_floors[p_path] = floor
            else:
                undetected_paths.append(p_path)

        path_to_floor_map.update(detected_floors)

        if undetected_paths:
            undetected_paths.sort()

            if detected_floors:
                all_detected_nos = list(detected_floors.values())
                if all(f < 0 for f in all_detected_nos):
                    start_floor = (
                        min(all_detected_nos) - 1
                        if min(all_detected_nos) - 1 not in all_detected_nos
                        else max(all_detected_nos) + 1
                    )
                else:
                    start_floor = max(all_detected_nos) + 1

                while start_floor in path_to_floor_map.values():
                    start_floor += 1
            else:
                start_floor = self.base_floor_default

            for i, p_path in enumerate(undetected_paths):
                current_assigned_floor = start_floor + i
                while (
                    current_assigned_floor in path_to_floor_map.values()
                ):  # Avoid collision
                    current_assigned_floor += 1
                path_to_floor_map[p_path] = current_assigned_floor

        floor_to_path_map: Dict[int, pathlib.Path] = {
            v: k for k, v in path_to_floor_map.items()
        }

        if len(floor_to_path_map) != len(path_to_floor_map):
            logger.warning(
                "Non-unique floor numbers assigned or detected, "
                "floor_to_path_map may not represent all images."
            )

        return path_to_floor_map, floor_to_path_map

    def calculate_z_levels(
        self, floor_to_path_map: Dict[int, pathlib.Path]
    ) -> Dict[int, float]:
        """
        Calculates the Z-coordinate for each floor.

        Args:
            floor_to_path_map: A map from floor number to image path (used to get sorted floors).

        Returns:
            A dictionary mapping floor number to its Z-coordinate.
        """
        if not floor_to_path_map:
            return {}

        sorted_floor_numbers = sorted(floor_to_path_map.keys())

        revision_num = 0

        if 0 not in sorted_floor_numbers:
            revision_num = 1

        z_levels: Dict[int, float] = {
            floor_num: float(
                (floor_num - revision_num) * self.default_floor_height
                if floor_num > 0
                else floor_num * self.default_floor_height
            )
            for floor_num in sorted_floor_numbers
        }
        return z_levels
</file>

<file path="src/network/network.py">
"""
Orchestrates the construction of a single-floor network graph.
"""

from __future__ import annotations
from pathlib import Path
import numpy as np
import networkx as nx
from src.utils.logger import setup_logger
from typing import Tuple, List, Optional, Dict, TYPE_CHECKING, Sequence
from scipy.spatial import KDTree
import cv2

if TYPE_CHECKING:
    from .node_creators import BaseNodeCreator

from src.config import graph_config
from .graph_manager import GraphManager
from src.utils.processor import ImageProcessor
from .node_creators import (
    BaseNodeCreator,
    RoomNodeCreator,
    VerticalNodeCreator,
    PedestrianNodeCreator,
    OutsideNodeCreator,
    ConnectionNodeCreator,
)

logger = setup_logger(__name__)


class Network:
    """
    Manages the creation of a network graph for a single floor from an image.

    This class is now driven by the new configuration system and is decoupled
    from the old NetworkConfig class.
    """

    def __init__(self, id_generator_start_value: int):
        """
        Initializes the Network orchestrator.

        Args:
            id_generator_start_value: The starting ID for nodes in this network.
        """
        self.image_processor = ImageProcessor()
        self.graph_manager = GraphManager(id_generator_start_value)

        # Node creators are now initialized without passing config objects.
        # They will import and use the new config modules directly.
        self._node_creators: Sequence[BaseNodeCreator] = [
            RoomNodeCreator(self),
            VerticalNodeCreator(self),
            PedestrianNodeCreator(self),
            ConnectionNodeCreator(self),
        ]

        self._outside_node_creator = OutsideNodeCreator(self)

        self._current_image_data: Optional[np.ndarray] = None
        self._id_map: Optional[np.ndarray] = None
        self._image_height: Optional[int] = None
        self._image_width: Optional[int] = None
        self._mask_cache: Dict[str, np.ndarray] = {}

    def _get_mask(
        self,
        identifier: str | List[str],
        is_category: bool,
        apply_morphology: bool = True,
    ) -> np.ndarray:
        """Generates and caches a combined mask for a category or a list of node names."""
        if isinstance(identifier, list):
            key = "_".join(sorted(identifier))
        else:
            key = identifier
        cache_key = f"{key}_{is_category}_{apply_morphology}"

        if cache_key in self._mask_cache:
            return self._mask_cache[cache_key]

        if self._image_height is None or self._image_width is None:
            raise RuntimeError("Image dimensions not set. Call _initialize_run first.")

        if is_category:
            if not isinstance(identifier, str):
                raise ValueError("Category identifier must be a string.")
            target_names = graph_config.get_nodes_by_category(identifier)
        else:
            target_names = (
                identifier if isinstance(identifier, list) else [identifier]
            )

        if not target_names:
            return np.zeros((self._image_height, self._image_width), dtype=np.uint8)

        combined_mask = np.zeros(
            (self._image_height, self._image_width), dtype=np.uint8
        )
        node_defs = graph_config.get_node_definitions()

        for name in target_names:
            node_props = node_defs.get(name, {})
            color_rgb = node_props.get("rgb")
            if not color_rgb or self._current_image_data is None:
                continue

            mask = np.all(
                self._current_image_data
                == np.array(color_rgb, dtype=np.uint8).reshape(1, 1, 3),
                axis=2,
            )
            mask = mask.astype(np.uint8) * 255
            combined_mask = cv2.bitwise_or(combined_mask, mask)

        if apply_morphology:
            geometry_config = graph_config.get_geometry_config()
            kernel_size = geometry_config.get("morphology_kernel_size", (5, 5))
            logger.info(f"Kernel size before conversion: {kernel_size} (type: {type(kernel_size)})")
            if isinstance(kernel_size, (float, int, str)):
                kernel_size = (int(kernel_size), int(kernel_size))
            combined_mask = self.image_processor.apply_morphology(
                combined_mask, operation="close_open", kernel_size=kernel_size
            )

        self._mask_cache[cache_key] = combined_mask
        return combined_mask

    def _initialize_run(self, image_path: Path, process_outside_nodes: bool) -> None:
        """Loads image, prepares internal data structures for a run."""
        # Load and preprocess image (quantize colors)
        raw_image_data = self.image_processor.load_and_prepare_image(image_path)
        self._current_image_data = self.image_processor.quantize_colors(raw_image_data)

        self._image_height, self._image_width = (
            self.image_processor.get_image_dimensions()
        )

        special_ids = graph_config.get_special_ids()
        self._id_map = np.full(
            (self._image_height, self._image_width),
            special_ids.get("background", -2),
            dtype=np.int32,
        )

        if process_outside_nodes:
            s_config = graph_config.get_super_network_config()
            outside_types = s_config.get("outside_types", [])
            if outside_types:
                for outside_type_name in outside_types:
                    outside_mask = self._outside_node_creator._create_mask_for_node(
                        self._current_image_data,
                        outside_type_name,
                        apply_morphology=True,
                    )
                    if outside_mask is not None:
                        self._id_map[outside_mask != 0] = special_ids.get(
                            "outside", -1
                        )

    def _create_all_node_types(
        self, z_level: float, process_outside_nodes: bool, floor_num: int
    ) -> None:
        """Iterates through node creators to populate the graph."""
        if self._current_image_data is None or self._id_map is None:
            raise RuntimeError(
                "Network run not initialized properly. Call _initialize_run first."
            )

        # Execute creators in the predefined order
        for creator in self._node_creators:
            logger.info(f"Running creator: {creator.__class__.__name__}")
            creator.create_nodes(
                self._current_image_data, self._id_map, z_level, floor_num
            )

        if process_outside_nodes:
            logger.info(
                f"Running creator: {self._outside_node_creator.__class__.__name__}"
            )
            self._outside_node_creator.create_nodes(
                self._current_image_data, self._id_map, z_level, floor_num
            )

    def _refine_door_connections(self, z_level: float) -> None:
        """
        Refines the connections for all door nodes based on their 'door_type'.
        - EXTERIOR doors connect to the nearest 'Corridor'.
        - INTERIOR doors connect to the nearest non-door 'CONNECTOR' and 'Corridor'.
        - ROOM doors connect to the nearest 'FIXED' or 'SLOT' and 'Corridor'.
        - Doors with a null 'door_type' are removed.
        """
        if self.graph_manager.node_count() == 0:
            return

        all_nodes = list(self.graph_manager.get_all_nodes_data())
        door_nodes = [
            (nid, data)
            for nid, data in all_nodes
            if data.get("category") == "CONNECTOR"
            and data.get("name") == "Door"
            and data.get("pos_z") == z_level
        ]

        if not door_nodes:
            return

        # Helper to create a KD-tree for a list of nodes
        def build_kdtree(nodes):
            if not nodes:
                return None, []
            positions = np.array(
                [(n_data["pos_x"], n_data["pos_y"]) for _, n_data in nodes]
            )
            return KDTree(positions), nodes

        # Prepare KD-trees for target node types
        corridor_nodes = [
            (nid, data)
            for nid, data in all_nodes
            if data.get("name") == "Corridor" and data.get("pos_z") == z_level
        ]
        corridor_tree, corridor_nodes_list = build_kdtree(corridor_nodes)

        room_nodes = [
            (nid, data)
            for nid, data in all_nodes
            if data.get("category") in ["FIXED", "SLOT"]
            and data.get("pos_z") == z_level
        ]
        room_tree, room_nodes_list = build_kdtree(room_nodes)

        other_connector_nodes = [
            (nid, data)
            for nid, data in all_nodes
            if data.get("category") == "CONNECTOR"
            and data.get("name") != "Door"
            and data.get("pos_z") == z_level
        ]
        other_connector_tree, other_connector_nodes_list = build_kdtree(
            other_connector_nodes
        )

        nodes_to_remove = []

        for door_id, door_data in door_nodes:
            door_type = door_data.get("door_type")
            door_pos = (door_data["pos_x"], door_data["pos_y"])

            if door_type == "EXTERIOR":
                if corridor_tree:
                    _, idx = corridor_tree.query(door_pos)
                    nearest_node_id, _ = corridor_nodes_list[idx]
                    self.graph_manager.connect_nodes_by_ids(door_id, nearest_node_id)

            elif door_type == "INTERIOR":
                if corridor_tree:
                    _, idx = corridor_tree.query(door_pos)
                    nearest_node_id, _ = corridor_nodes_list[idx]
                    self.graph_manager.connect_nodes_by_ids(door_id, nearest_node_id)
                if other_connector_tree:
                    _, idx = other_connector_tree.query(door_pos)
                    nearest_node_id, _ = other_connector_nodes_list[idx]
                    self.graph_manager.connect_nodes_by_ids(door_id, nearest_node_id)

            elif door_type == "ROOM":
                # Connect to the colliding room node(s)
                colliding_ids = door_data.get("colliding_node_ids", [])
                for nid in colliding_ids:
                    node_data = self.graph_manager.get_node_attributes(nid)
                    if node_data and node_data.get("category") in ["FIXED", "SLOT"]:
                        self.graph_manager.connect_nodes_by_ids(door_id, nid)

                # Connect to the nearest passageway
                combined_passageway_nodes = corridor_nodes_list + other_connector_nodes_list
                passageway_tree, passageway_nodes_list = build_kdtree(
                    combined_passageway_nodes
                )
                if passageway_tree:
                    _, idx = passageway_tree.query(door_pos)
                    nearest_passageway_id, _ = passageway_nodes_list[idx]
                    self.graph_manager.connect_nodes_by_ids(
                        door_id, nearest_passageway_id
                    )
            else:
                nodes_to_remove.append(door_id)

        # Remove nodes marked for deletion
        for node_id in nodes_to_remove:
            self.graph_manager.remove_node(node_id)
        if nodes_to_remove:
            logger.info(f"Removed {len(nodes_to_remove)} doors with null door_type.")

    def run(
        self,
        image_path: Path,
        z_level: float = 0.0,
        process_outside_nodes: bool = False,
        floor_num: int = 0,
    ) -> Tuple[nx.Graph, int, int, int]:
        """
        Executes the full network generation pipeline.
        Args:
            process_outside_nodes: If True, detailed mesh nodes for outside areas are created.
                                   If False, outside areas are only marked in id_map (for door typing)
                                   but no actual outside mesh nodes are generated by OutsideNodeCreator.
        """
        logger.info(
            f"Processing floor: {image_path} at z={z_level}, process_outside_nodes={process_outside_nodes}"
        )

        self._initialize_run(image_path, process_outside_nodes)

        self._create_all_node_types(
            z_level, process_outside_nodes, floor_num
        ) 

        self._refine_door_connections(z_level)

        logger.info(
            f"Finished floor. Nodes: {self.graph_manager.node_count()}"
        )

        if self._image_width is None or self._image_height is None:
            raise RuntimeError("Image dimensions not set.")

        return (
            self.graph_manager.get_graph_copy(),
            self._image_width,
            self._image_height,
            self.graph_manager.get_next_available_node_id_estimate(),
        )
</file>

<file path="src/network/super_network.py">
"""Manages the construction of a multi-floor network graph.

This module orchestrates the creation of a comprehensive, multi-floor
network by managing individual `Network` instances for each floor. It
supports parallel processing to accelerate the generation of large-scale
graphs and handles the vertical connections between floors.
"""

import os
from pathlib import Path
import networkx as nx
import numpy as np
from joblib import Parallel, delayed
from src.utils.logger import setup_logger
from typing import List, Dict, Tuple, Optional, Any
from scipy.spatial import KDTree

from src.config import graph_config
from .network import Network
from .floor_manager import FloorManager

logger = setup_logger(__name__)


def _process_floor_worker(
    task_args: Tuple[Path, float, int, bool, int],
) -> Tuple[Optional[nx.Graph], Optional[int], Optional[int], int, Path, float, int]:
    """Processes a single floor's network generation in a worker process.

    Args:
        task_args: A tuple containing the arguments for the worker:
            - image_path (Path): Path to the floor's image file.
            - z_level (float): The Z-coordinate for all nodes on this floor.
            - id_start_value (int): The starting ID for node generation.
            - process_outside_nodes (bool): Flag to process outside areas.
            - floor_num (int): The floor number.

    Returns:
        A tuple with the results:
            - graph (Optional[nx.Graph]): The generated graph for the floor.
            - width (Optional[int]): The width of the floor image.
            - height (Optional[int]): The height of the floor image.
            - next_id (int): The next available node ID after this worker.
            - image_path (Path): The original image path for identification.
            - z_level (float): The original Z-level for identification.
            - floor_num (int): The original floor number for identification.
    """
    image_path, z_level, id_start_value, process_outside_nodes, floor_num = task_args
    try:
        network_builder = Network(id_generator_start_value=id_start_value)
        graph, width, height, next_id = network_builder.run(
            image_path=image_path,
            z_level=z_level,
            process_outside_nodes=process_outside_nodes,
            floor_num=floor_num,
        )
        return graph, width, height, next_id, image_path, z_level, floor_num
    except Exception as e:
        logger.error(f"Error processing floor {image_path.name} in worker: {e}")
        s_config = graph_config.get_super_network_config()
        est_next_id = id_start_value + s_config.get(
            "estimated_max_nodes_per_floor", 10000
        )
        return None, None, None, est_next_id, image_path, z_level, floor_num


class SuperNetwork:
    """Orchestrates the creation of a multi-floor network graph."""

    def __init__(self, num_processes: Optional[int] = None, base_floor: int = 0):
        """Initializes the SuperNetwork.

        Args:
            num_processes: Number of processes for parallel floor processing.
                           Defaults to the number of CPU cores.
            base_floor: The default base floor number if it cannot be
                        detected from the image filename.
        """
        self.s_config = graph_config.get_super_network_config()
        self.super_graph: nx.Graph = nx.Graph()
        self.designated_ground_floor_number: Optional[int] = None
        self.designated_ground_floor_z: Optional[float] = None
        self.num_processes: int = (
            num_processes if num_processes is not None else (os.cpu_count() or 1)
        )

        floor_height = self.s_config.get("default_floor_height", 10.0)
        self.floor_manager = FloorManager(
            base_floor_default=base_floor, default_floor_height=floor_height
        )

        self.vertical_connection_tolerance: int = self.s_config.get(
            "default_vertical_connection_tolerance", 0
        )
        self.floor_z_map: Dict[int, float] = {}
        self.path_to_floor_map: Dict[Path, int] = {}
        self.width: Optional[int] = None
        self.height: Optional[int] = None

    def _should_process_outside_nodes(
        self, floor_num: int, designated_ground_floor_num: Optional[int]
    ) -> bool:
        """Determines if outside nodes should be processed for a specific floor."""
        generate_outside = self.s_config.get("generate_outside_nodes", False)
        if not generate_outside:
            return False

        if (
            designated_ground_floor_num is None
            or floor_num != designated_ground_floor_num
        ):
            return False
        return bool(self.s_config.get("outside_types"))

    def _prepare_floor_data(
        self,
        image_file_paths: List[Path],
        z_levels_override: Optional[List[float]] = None,
    ) -> List[Tuple[Path, float, bool, int]]:
        """Determines floor numbers and Z-levels for each image path."""
        self.path_to_floor_map, floor_to_path_map = (
            self.floor_manager.auto_assign_floors(image_file_paths)
        )

        if z_levels_override and len(z_levels_override) == len(image_file_paths):
            sorted_paths = sorted(
                self.path_to_floor_map.keys(), key=lambda p: self.path_to_floor_map[p]
            )
            path_to_z = {path: z for path, z in zip(sorted_paths, z_levels_override)}
            self.floor_z_map = {
                self.path_to_floor_map[p]: z for p, z in path_to_z.items()
            }
        else:
            self.floor_z_map = self.floor_manager.calculate_z_levels(floor_to_path_map)

        if not self.floor_z_map:
            raise ValueError("Could not determine Z-levels for floors.")

        all_floor_nums = list(self.floor_z_map.keys())
        if not all_floor_nums:
            return []

        designated_ground_floor_num = self.s_config.get(
            "ground_floor_number_for_outside"
        )
        if designated_ground_floor_num is None:
            positive_or_zero_floors = sorted([fn for fn in all_floor_nums if fn >= 0])
            if 0 in all_floor_nums:
                designated_ground_floor_num = 0
            elif 1 in all_floor_nums and not any(0 <= fn < 1 for fn in all_floor_nums):
                designated_ground_floor_num = 1
            elif positive_or_zero_floors:
                designated_ground_floor_num = positive_or_zero_floors[0]

        self.designated_ground_floor_number = designated_ground_floor_num
        if designated_ground_floor_num is not None:
            self.designated_ground_floor_z = self.floor_z_map.get(
                designated_ground_floor_num
            )

        tasks = []
        for p, floor_num in self.path_to_floor_map.items():
            if (z_level := self.floor_z_map.get(floor_num)) is not None:
                process_outside = self._should_process_outside_nodes(
                    floor_num, designated_ground_floor_num
                )
                tasks.append((p, z_level, process_outside, floor_num))

        return sorted(tasks, key=lambda item: item[1])

    def run(
        self,
        image_file_paths: List[Path],
        z_levels_override: Optional[List[float]] = None,
        force_vertical_tolerance: Optional[int] = None,
    ) -> nx.Graph:
        """Builds the multi-floor network."""
        self.super_graph.clear()
        floor_run_data = self._prepare_floor_data(
            [p for p in image_file_paths], z_levels_override
        )
        if not floor_run_data:
            logger.warning("No floor data to process.")
            return self.super_graph

        max_nodes = self.s_config.get("estimated_max_nodes_per_floor", 10000)
        tasks = [
            (p, z, i * max_nodes + 1, outside, floor_num)
            for i, (p, z, outside, floor_num) in enumerate(floor_run_data, start=1)
        ]

        self.num_processes = min(self.num_processes, len(tasks))
        logger.info(
            f"Processing {len(tasks)} floors using {self.num_processes} processes..."
        )
        results: List[Any] = []
        if self.num_processes > 1 and len(tasks) > 1:
            results = list(
                Parallel(n_jobs=self.num_processes)(
                    delayed(_process_floor_worker)(task) for task in tasks
                )
            )
        else:
            results = [_process_floor_worker(task) for task in tasks]

        first_floor = True
        for graph, width, height, _, path, _, _ in results:
            if graph is None:
                logger.warning(f"Failed to process floor image {path.name}. Skipping.")
                continue
            if first_floor:
                self.width, self.height, first_floor = width, height, False
            elif (self.width, self.height) != (width, height):
                raise ValueError(f"Image dimensions mismatch for {path.name}.")

            self.super_graph.add_nodes_from(graph.nodes(data=True))
            self.super_graph.add_edges_from(graph.edges(data=True))

        if force_vertical_tolerance is not None:
            self.vertical_connection_tolerance = force_vertical_tolerance
        elif self.s_config.get("default_vertical_connection_tolerance") == 0:
            self.vertical_connection_tolerance = (
                self._auto_calculate_vertical_tolerance()
            )

        self._connect_floors()
        logger.info(
            f"SuperNetwork construction complete. Total nodes: {self.super_graph.number_of_nodes()}"
        )
        return self.super_graph

    def _auto_calculate_vertical_tolerance(self) -> int:
        """Automatically calculates a tolerance for connecting vertical nodes."""
        vertical_types = self.s_config.get("vertical_types", [])
        vertical_nodes_data = [
            data
            for _, data in self.super_graph.nodes(data=True)
            if data.get("type") in vertical_types
        ]
        if len(vertical_nodes_data) < 2:
            return self.s_config.get("default_vertical_connection_tolerance", 0)

        positions_xy = np.array(
            [(data["pos_x"], data["pos_y"]) for data in vertical_nodes_data]
        )
        if len(positions_xy) < 2:
            return self.s_config.get("default_vertical_connection_tolerance", 0)

        try:
            tree = KDTree(positions_xy)
            distances, _ = tree.query(positions_xy, k=2)
            nearest_distances = distances[:, 1][distances[:, 1] > 1e-6]
            if nearest_distances.size == 0:
                return self.s_config.get("default_vertical_connection_tolerance", 0)

            avg_min_dist = np.mean(nearest_distances)
            factor = self.s_config.get("vertical_tolerance_factor", 0.5)
            min_tol = self.s_config.get("min_vertical_tolerance", 10)
            calculated_tolerance = int(avg_min_dist * factor)
            logger.info(
                f"Auto-calculated vertical tolerance: {calculated_tolerance} (based on avg_min_dist: {avg_min_dist:.2f})"
            )
            return max(min_tol, calculated_tolerance)
        except Exception as e:
            logger.error(f"Error in auto-calculating tolerance: {e}. Using default.")
            return self.s_config.get("default_vertical_connection_tolerance", 0)

    def _connect_floors(self) -> None:
        """Connects vertical transport nodes between different floors."""
        vertical_types = self.s_config.get("vertical_types", [])
        all_vertical_nodes = [
            (node_id, data)
            for node_id, data in self.super_graph.nodes(data=True)
            if data.get("name") in vertical_types
        ]
        if not all_vertical_nodes:
            logger.info("No vertical nodes found to connect between floors.")
            return

        nodes_by_name: Dict[str, List[Dict[str, Any]]] = {}
        for node_id, data in all_vertical_nodes:
            data["id"] = node_id
            nodes_by_name.setdefault(data.get("name", "unknown"), []).append(data)

        logger.info(
            f"Attempting to connect floors. Tolerance: {self.vertical_connection_tolerance} pixels."
        )
        connected_pairs_count = 0
        z_diff_threshold = self.s_config.get("z_level_diff_threshold", 1.0)

        for node_name, nodes in nodes_by_name.items():
            if len(nodes) < 2:
                continue

            nodes.sort(key=lambda n: (n["pos_z"], n["pos_y"], n["pos_x"]))
            positions = np.array([(n["pos_x"], n["pos_y"]) for n in nodes])
            if len(positions) < 2:
                continue

            kdtree = KDTree(positions)
            pairs = kdtree.query_pairs(r=self.vertical_connection_tolerance)

            for i, j in pairs:
                node_i, node_j = nodes[i], nodes[j]
                if abs(node_i["pos_z"] - node_j["pos_z"]) > z_diff_threshold:
                    if not self.super_graph.has_edge(node_i["id"], node_j["id"]):
                        time_per_floor = node_i.get(
                            "time_per_floor", 10.0
                        )  # Default to 10s if not specified
                        self.super_graph.add_edge(
                            node_i["id"],
                            node_j["id"],
                            type="vertical_connection",
                            weight=time_per_floor,
                        )
                        connected_pairs_count += 1

        logger.info(
            f"Inter-floor connections made for {connected_pairs_count} pairs of vertical nodes."
        )
</file>

<file path="src/pipeline/pathway_generator.py">
import random
from typing import Dict, Any, List, Tuple
from src.utils.logger import setup_logger
from src.config.config_loader import ConfigLoader

class PathwayGenerator:

    def __init__(self, config: ConfigLoader):
        self.process_id = 1
        self.paths = config.paths
        self.pools = config.pathways.training_generation.department_pools
        self.fragments = config.pathways.training_generation.sequence_fragments
        self.meta_rules = config.pathways.training_generation.meta_rules
        self.pathways_number = config.pathways.pathways_number
        self.logger = setup_logger(self.__class__.__name__)
        self.logger.info("PathwayGenerator initialized")


    def generate_all(self) -> Dict[str, Dict[str, Any]]:
        pathways: Dict[str, Dict[str, Any]] = {}
        count = 0
        while count < self.pathways_number:
            full_sequence_str, final_pathway = self.generate()
            if full_sequence_str in pathways:
                continue
            pathways[full_sequence_str] = final_pathway
            count += 1
        return pathways


    def generate(self) -> Tuple[str, Dict[str, Any]]:
        rule_weights = [rule.base_weight for rule in self.meta_rules]
        chosen_rule = random.choices(self.meta_rules, weights=rule_weights, k=1)[0]

        generated_sequence: List[str] = []
        context: Dict[str, str] = {}

        for step in chosen_rule.structure:
            self._parse_step(step, context, generated_sequence)

        final_pathway = {
            "process_id": f"PROC_GEN_{chosen_rule['id']}_{self.process_id}",
            "description": f"Programmatic Generation: {chosen_rule['description']}",
            "core_sequence": generated_sequence,
            "start_nodes": chosen_rule.start_nodes,
            "end_nodes": chosen_rule.end_nodes,
            "base_weight": chosen_rule.base_weight,
        }

        full_sequence_str = (
            f"{' -> '.join(final_pathway['start_nodes'])} -> "
            f"{' -> '.join(generated_sequence)} -> "
            f"{' -> '.join(final_pathway['end_nodes'])}"
        )
        self.logger.info(
            f"Generated pathway: {final_pathway['process_id']}, Full sequence: {full_sequence_str}"
        )

        self.process_id += 1

        return full_sequence_str, final_pathway

    def _parse_step(self, step: Any, context: Dict[str, Any], generated_sequence: List[str]):
        step_type = step.type

        if step_type == "primary_department":
            from_pool_value = step.from_pool
            department_list = []
            if isinstance(from_pool_value, str) and from_pool_value in self.pools:
                department_list = self.pools[from_pool_value]
            elif isinstance(from_pool_value, list):
                department_list = from_pool_value

            if department_list:
                dept = random.choice(department_list)
                context["primary_department"] = dept
                generated_sequence.append(dept)

        elif step_type == "fixed":
            dept_name = step.name
            if dept_name:
                generated_sequence.append(dept_name)

        elif step_type == "pool":
            pool_name = step.pool_name
            if pool_name in self.pools:
                generated_sequence.append(random.choice(self.pools[pool_name]))

        elif step_type == "optional":
            if random.random() < step.probability:
                self._parse_step(step.step, context, generated_sequence)

        elif step_type == "fragment":
            fragment_name = step.fragment_name
            if fragment_name in self.fragments:
                fragment_steps = self.fragments[fragment_name]
                for frag_step in fragment_steps:
                    self._parse_step(frag_step, context, generated_sequence)

        elif step_type == "repeat":
            target = step.target
            if target in context:
                generated_sequence.append(context[target])
</file>

<file path="src/utils/processor.py">
"""Handles image loading, preprocessing, and basic morphological operations."""

from pathlib import Path
import cv2
from src.utils.logger import setup_logger
import numpy as np
from PIL import Image
from scipy.spatial import KDTree
from typing import Tuple, Optional

from src.config import graph_config, path_manager

logger = setup_logger(__name__)


class ImageProcessor:
    """
    Provides functionalities for image loading, color quantization,
    and morphological operations.
    """

    def __init__(self):
        """
        Initializes the ImageProcessor.
        """
        self._current_image_data: Optional[np.ndarray] = None
        self._image_height: Optional[int] = None
        self._image_width: Optional[int] = None
        # Caching config lookups
        self.node_defs = graph_config.get_node_definitions()
        self.rgb_map = {
            tuple(props["rgb"]): name
            for name, props in self.node_defs.items()
            if "rgb" in props
        }
        self.geometry_config = graph_config.get_geometry_config()

    def load_and_prepare_image(self, image_path: Path) -> np.ndarray:
        """
        Loads an image, rotates it, and stores its dimensions.

        The processed image is stored internally and also returned.

        Args:
            image_path: Path to the image file.

        Returns:
            A NumPy array representing the processed image (RGB).

        Raises:
            FileNotFoundError: If the image_path does not exist.
            IOError: If the image cannot be opened or read.
        """
        try:
            img = Image.open(image_path).convert("RGB")
        except FileNotFoundError:
            raise FileNotFoundError(f"Image file not found: {image_path}")
        except IOError:
            raise IOError(f"Could not open or read image file: {image_path}")

        self._current_image_data = np.asarray(img, dtype=np.uint8)
        if self._current_image_data is None:
            raise ValueError(f"Failed to convert image to numpy array: {image_path}")

        self._image_height, self._image_width = self._current_image_data.shape[:2]
        return self._current_image_data.copy()

    def get_image_dimensions(self) -> Tuple[int, int]:
        """
        Returns the dimensions of the last loaded image.

        Returns:
            A tuple (height, width).

        Raises:
            ValueError: If no image has been loaded yet.
        """
        if self._image_height is None or self._image_width is None:
            raise ValueError("Image not loaded. Call load_and_prepare_image() first.")
        return self._image_height, self._image_width

    def quantize_colors(self, image_data: np.ndarray) -> np.ndarray:
        """
        Replaces each pixel's color in the image with the nearest color
        from the provided color_map using a KDTree for efficiency.

        Args:
            image_data: A NumPy array representing the image (H, W, 3) in RGB.

        Returns:
            A NumPy array of the same shape with colors replaced.
        """
        if not self.rgb_map:
            logger.warning("Color map is empty. Returning original image.")
            return image_data.copy()

        pixels = image_data.reshape(-1, 3)
        map_colors_rgb = list(self.rgb_map.keys())

        kdtree = KDTree(map_colors_rgb)
        _, closest_indices = kdtree.query(pixels)

        new_pixels = np.array(map_colors_rgb, dtype=np.uint8)[closest_indices]
        new_image = new_pixels.reshape(image_data.shape).astype(np.uint8)
        return new_image

    def apply_morphology(
        self,
        mask: np.ndarray,
        operation: str = "close_open",
        kernel_size: Optional[Tuple[int, int]] = None,
    ) -> np.ndarray:
        """
        Applies morphological operations to a binary mask.

        Args:
            mask: The input binary mask (NumPy array, dtype=uint8).
            operation: The type of operation.
                       'close_open': MORPH_CLOSE then MORPH_OPEN (default)
                       'open': MORPH_OPEN
                       'close': MORPH_CLOSE
                       'dilate': MORPH_DILATE
                       'erode': MORPH_ERODE
            kernel_size: Tuple (k_height, k_width) for the morphological kernel.
                         Defaults to `config.MORPHOLOGY_KERNEL_SIZE`.

        Returns:
            The processed binary mask.
        """
        if kernel_size is None:
            k_size_val = self.geometry_config.get("morphology_kernel_size", [5, 5])
            if isinstance(k_size_val, (int, float)):
                k_size = (int(k_size_val), int(k_size_val))
            else:
                k_size = tuple(k_size_val)
        else:
            k_size = kernel_size

        kernel = np.ones(k_size, np.uint8)
        processed_mask = mask.copy()

        if operation == "close_open":
            processed_mask = cv2.morphologyEx(processed_mask, cv2.MORPH_CLOSE, kernel)
            processed_mask = cv2.morphologyEx(processed_mask, cv2.MORPH_OPEN, kernel)
        elif operation == "open":
            processed_mask = cv2.morphologyEx(processed_mask, cv2.MORPH_OPEN, kernel)
        elif operation == "close":
            processed_mask = cv2.morphologyEx(processed_mask, cv2.MORPH_CLOSE, kernel)
        elif operation == "dilate":
            processed_mask = cv2.dilate(processed_mask, kernel, iterations=1)
        elif operation == "erode":
            processed_mask = cv2.erode(processed_mask, kernel, iterations=1)
        else:
            raise ValueError(f"Unsupported morphological operation: {operation}")

        return processed_mask


class DebugImage:
    """Helper class for saving and displaying debug images."""

    count = 0

    def __init__(
        self,
        image_data: np.ndarray,
        save: bool = False,
        show_napari: bool = False,
        suffix: str = "",
    ):
        """
        Initializes DebugImage.

        Args:
            image_data: NumPy array of the image to debug.
            save: If True, saves the image.
            show_napari: If True, shows the image using napari (requires napari installed).
            suffix: Suffix for the saved filename.
        """
        self.image_to_debug = image_data.copy()
        self.debug_path = path_manager.get_path("debug_dir", create_if_not_exist=True)

        if save:
            self._save_image(suffix)
        if show_napari:
            self._show_with_napari()

    def _save_image(self, suffix: str = ""):
        """Saves the debug image."""
        filename = f"debug_{DebugImage.count}_{suffix}.png"
        save_path = self.debug_path / filename
        try:
            # 如果是RGB格式，转换为BGR以供Pillow保存，或处理灰度图
            if self.image_to_debug.ndim == 3 and self.image_to_debug.shape[2] == 3:
                # Assume RGB from PIL, convert to BGR for OpenCV-style saving or save as is with PIL
                img_to_save_pil = Image.fromarray(self.image_to_debug)
            elif self.image_to_debug.ndim == 2:  # 灰度图像
                img_to_save_pil = Image.fromarray(self.image_to_debug, mode="L")
            else:
                logger.warning(
                    f"Warning: Unsupported image format for saving: {self.image_to_debug.shape}"
                )
                return

            img_to_save_pil.save(save_path)
            DebugImage.count += 1
            logger.info(f"Debug image saved to {save_path}")
        except Exception as e:
            logger.error(f"Error saving debug image {save_path}: {e}")

    def _show_with_napari(self):
        """Shows the image using napari."""
        try:
            import napari

            viewer = napari.Viewer()
            viewer.add_image(self.image_to_debug)
            napari.run()
        except ImportError:
            logger.warning("Napari is not installed. Skipping napari display.")
        except Exception as e:
            logger.error(f"Error showing image with napari: {e}")
</file>

<file path="src/optimize_manager.py">
import random

from src.pipeline import PathwayGenerator, CostManager
from src.config.config_loader import ConfigLoader
from src.utils.logger import setup_logger

class OptimizeManager:
    def __init__(self, config: ConfigLoader, **kwargs):
        self.logger = setup_logger(__name__)
        self.config = config
        self.kwargs = kwargs
        self.pathway_generator = PathwayGenerator(self.config)
        self.cost_manager = CostManager(self.config)

    def run(self):
        self.pathways = self.pathway_generator.generate_all()
        self.cost_manager.initialize(pathways=self.pathways)
        self.original_travel_cost = self.cost_manager.current_travel_cost
        self.layout = self.cost_manager.layout

        # for i in range(1000):
        #     dept1, dept2 = random.sample(list(self.layout.keys()), 2)
        #     if self.cost_manager.swap(dept1, dept2) > 0:
        #         self.logger.info(f"Swap {dept1} and {dept2}, new cost: {self.cost_manager.current_travel_cost}")
        #         pass
</file>

<file path="src/plotting/plotter.py">
"""
Defines plotter classes for visualizing network graphs using Matplotlib and Plotly.
"""

import abc
import pathlib
from src.utils.logger import setup_logger
import networkx as nx
import numpy as np
import plotly.graph_objects as go
from typing import Dict, Any, List, Optional

from src.config import graph_config

logger = setup_logger(__name__)


class BasePlotter(abc.ABC):
    """
    Abstract base class for graph plotters.
    """

    def __init__(self):
        """
        Initializes the BasePlotter.
        """
        self.plotter_config = graph_config.get_plotter_config()
        self.node_defs = graph_config.get_node_definitions()
        self.super_network_config = graph_config.get_super_network_config()

    def _validate_node_coordinates(
        self, node_data: Dict[str, Any], node_id: Any
    ) -> bool:
        """
        Validates node coordinates from the graph attributes.
        """
        try:
            coords = [
                node_data.get("pos_x"),
                node_data.get("pos_y"),
                node_data.get("pos_z"),
            ]
            return all(isinstance(c, (int, float)) and np.isfinite(c) for c in coords)
        except (TypeError, AttributeError):
            return False

    def _get_edge_style_config(self) -> Dict[str, Dict[str, Any]]:
        """
        Gets edge style configuration from the plotter config.
        """
        return self.plotter_config.get("edge_styles", {})

    def _classify_edge_type(self, start_node: Dict, end_node: Dict) -> str:
        """
        Classifies the edge type based on node properties.
        """
        z_diff = abs(start_node.get("pos_z", 0) - end_node.get("pos_z", 0))
        z_threshold = self.super_network_config.get("z_level_diff_threshold", 1.0)
        if z_diff > z_threshold:
            return "vertical"

        start_category = start_node.get("category")
        end_category = end_node.get("category")

        if start_category == "CONNECTOR" or end_category == "CONNECTOR":
            return "door"

        return "horizontal"

    def _get_node_color(self, node_name: str) -> str:
        """
        Determines the plotting color for a given node name from config.
        """
        node_props = self.node_defs.get(node_name, {})
        rgb = node_props.get("rgb")
        if rgb and len(rgb) == 3:
            return f"rgb({rgb[0]},{rgb[1]},{rgb[2]})"
        return "#1f77b4"  # Default Plotly blue

    @abc.abstractmethod
    def plot(
        self,
        graph: nx.Graph,
        output_path: Optional[pathlib.Path] = None,
        title: str = "Network Graph",
        # For Plotly layout, original image width
        graph_width: Optional[int] = None,
        # For Plotly layout, original image height
        graph_height: Optional[int] = None,
        # For SuperNetwork floor labels
        floor_z_map: Optional[Dict[int, float]] = None,
    ):
        """
        Abstract method to plot the graph.

        Args:
            graph: The NetworkX graph to plot.
            output_path: Optional path to save the plot. If None, displays the plot.
            title: The title for the plot.
            graph_width: Original width of the (floor plan) image space. Used by Plotly.
            graph_height: Original height of the (floor plan) image space. Used by Plotly.
            floor_z_map: Mapping from floor number to Z-coordinate, for floor slider labels.
        """
        pass


class PlotlyPlotter(BasePlotter):
    """
    Generates interactive 3D network graph visualizations using Plotly.
    """

    def _create_floor_selection_controls(
        self,
        all_z_levels: List[float],
        min_z: float,
        max_z: float,
        floor_z_map_for_labels: Optional[Dict[int, float]] = None,
        base_floor_for_labels: int = 0,
    ) -> Dict[str, Any]:
        """
        Creates slider controls for selecting and viewing individual floors or all floors.
        Args:
            all_z_levels: Sorted list of unique Z-coordinates present in the graph.
            min_z: Minimum Z-coordinate.
            max_z: Maximum Z-coordinate.
            floor_z_map_for_labels: Mapping from actual floor number to Z-coordinate.
            base_floor_for_labels: The base floor number for labeling (e.g. 0 for ground, 1 for first).
        """
        if not all_z_levels:
            return {"sliders": []}

        # Create floor labels. Try to map Z-levels back to "human-readable" floor numbers.
        z_to_floor_label_map: Dict[float, str] = {}
        if floor_z_map_for_labels:
            # Invert floor_z_map_for_labels to map z -> floor_num for easier lookup
            # Handle potential multiple floors at the same Z (unlikely with good input)
            z_to_floor_num: Dict[float, List[int]] = {}
            for fn, z_val in floor_z_map_for_labels.items():
                z_to_floor_num.setdefault(z_val, []).append(fn)

            for z_level in all_z_levels:
                floor_nums_at_z = z_to_floor_num.get(z_level)
                if floor_nums_at_z:
                    # If multiple floor numbers map to the same z_level, list them or take first
                    f_num_str = "/".join(map(str, sorted(floor_nums_at_z)))
                    # e.g., F1, F-1/B1
                    z_to_floor_label_map[z_level] = f"F{f_num_str}"
                # Fallback if z_level not in map (should not happen if map is complete)
                else:
                    z_to_floor_label_map[z_level] = f"Z={z_level:.1f}"
        else:  # Fallback if no floor_z_map is provided
            for i, z_level in enumerate(all_z_levels):
                # Attempt simple labeling if base_floor is known
                floor_num_guess = base_floor_for_labels + i  # This is a rough guess
                z_to_floor_label_map[z_level] = f"F{floor_num_guess} (Z={z_level:.1f})"

        slider_steps = []
        for z_level in all_z_levels:
            label = z_to_floor_label_map.get(z_level, f"Z={z_level:.1f}")
            slider_steps.append(
                dict(
                    label=label,
                    method="relayout",
                    args=[
                        {
                            "scene.zaxis.range": [
                                z_level
                                - self.super_network_config.get("floor_height", 3.0) / 2
                                + 0.1,
                                z_level
                                + self.super_network_config.get("floor_height", 3.0) / 2
                                - 0.1,
                            ]
                        }
                    ],  # View single floor
                )
            )

        # Add a step to show all floors
        slider_steps.append(
            dict(
                label="All Floors",
                method="relayout",
                args=[
                    {
                        "scene.zaxis.range": [
                            min_z
                            - self.super_network_config.get("floor_height", 3.0) * 0.5,
                            max_z
                            + self.super_network_config.get("floor_height", 3.0) * 0.5,
                        ]
                    }
                ],  # View all
            )
        )

        sliders = [
            dict(
                active=len(all_z_levels),  # Default to "All Floors"
                currentvalue={"prefix": "Current Display: "},
                pad={"t": 50},
                steps=slider_steps,
                name="Floor Selection",
            )
        ]
        return {"sliders": sliders}

    def plot(
        self,
        graph: nx.Graph,
        output_path: Optional[str] = None,
        title: str = "3D Network Graph",
        graph_width: Optional[int] = None,
        floor_z_map: Optional[Dict[int, float]] = None,
    ):
        if not graph.nodes:
            logger.warning("PlotlyPlotter: Graph has no nodes to plot.")
            return

        nodes_by_name: Dict[str, Dict[str, list]] = {}
        all_z_coords = [data.get("pos_z", 0) for _, data in graph.nodes(data=True)]

        # Group nodes by their 'name' for creating traces
        for node_id, data in graph.nodes(data=True):
            name = data.get("name", "Unknown")
            if name not in nodes_by_name:
                nodes_by_name[name] = {
                    "x": [],
                    "y": [],
                    "z": [],
                    "hover_text": [],
                    "sizes": [],
                    "ids": [],
                }

            if not self._validate_node_coordinates(data, node_id):
                logger.warning(f"Skipping node {node_id} due to invalid coordinates.")
                continue

            x, y, z = data["pos_x"], data["pos_y"], data["pos_z"]
            plot_x = (
                (graph_width - x)
                if self.plotter_config.get("image_mirror") and graph_width
                else x
            )

            nodes_by_name[name]["x"].append(plot_x)
            nodes_by_name[name]["y"].append(y)
            nodes_by_name[name]["z"].append(z)
            nodes_by_name[name]["ids"].append(node_id)

            hover_label = f"ID: {node_id}<br>Name: {name}<br>CName: {data.get('cname', 'N/A')}<br>Code: {data.get('code', 'N/A')}<br>Pos: ({x:.1f}, {y:.1f}, {z:.1f})"
            if name == "Door":
                door_type = data.get('door_type', 'N/A')
                hover_label += f"<br>Door Type: {door_type}"
            nodes_by_name[name]["hover_text"].append(hover_label)

            node_sizes = self.plotter_config.get("node_sizes", {})
            size = node_sizes.get(
                data.get("category", "default"), node_sizes.get("default", 3)
            )
            nodes_by_name[name]["sizes"].append(size)

        # Create node traces
        node_traces = []
        for name, data in nodes_by_name.items():
            node_traces.append(
                go.Scatter3d(
                    x=data["x"],
                    y=data["y"],
                    z=data["z"],
                    mode="markers",
                    marker=dict(
                        size=data["sizes"],
                        color=self._get_node_color(name),
                        opacity=self.plotter_config.get("node_opacity", 0.8),
                    ),
                    text=data["hover_text"],
                    hoverinfo="text",
                    name=name,
                    customdata=data["ids"],
                )
            )

        # Prepare edge data
        edge_styles = self._get_edge_style_config()
        edge_data = {key: {"x": [], "y": [], "z": []} for key in edge_styles}

        for u, v in graph.edges():
            node_u, node_v = graph.nodes[u], graph.nodes[v]
            if not self._validate_node_coordinates(
                node_u, u
            ) or not self._validate_node_coordinates(node_v, v):
                continue

            x0, y0, z0 = node_u["pos_x"], node_u["pos_y"], node_u["pos_z"]
            x1, y1, z1 = node_v["pos_x"], node_v["pos_y"], node_v["pos_z"]

            if self.plotter_config.get("image_mirror") and graph_width:
                x0, x1 = graph_width - x0, graph_width - x1

            edge_type = self._classify_edge_type(node_u, node_v)
            edge_data[edge_type]["x"].extend([x0, x1, None])
            edge_data[edge_type]["y"].extend([y0, y1, None])
            edge_data[edge_type]["z"].extend([z0, z1, None])

        # Create edge traces
        edge_traces = []
        for edge_type, data in edge_data.items():
            if data["x"]:
                style = edge_styles[edge_type]
                edge_traces.append(
                    go.Scatter3d(
                        x=data["x"],
                        y=data["y"],
                        z=data["z"],
                        mode="lines",
                        line={"color": style["color"], "width": style["width"]},
                        hoverinfo="none",
                        name=style["name"],
                        showlegend=True,
                    )
                )

        # Create layout
        scene_config = self.plotter_config.get("scene", {})
        aspect_ratio = scene_config.get("aspect_ratio", {"x": 1, "y": 1, "z": 1})
        camera_eye = scene_config.get("camera", {}).get(
            "eye", {"x": 1.25, "y": 1.25, "z": 1.25}
        )

        layout = go.Layout(
            title=title,
            showlegend=True,
            hovermode="closest",
            margin=dict(b=20, l=5, r=5, t=40),
            scene=dict(
                xaxis=dict(
                    title="X",
                ),
                yaxis=dict(title="Y"),
                zaxis=dict(title="Z (Floor)"),
                aspectmode="manual",
                aspectratio=dict(
                    x=aspect_ratio["x"], y=aspect_ratio["y"], z=aspect_ratio["z"]
                ),
                camera=dict(
                    eye=dict(x=camera_eye["x"], y=camera_eye["y"], z=camera_eye["z"])
                ),
            ),
            legend=dict(
                orientation="v",
                x=0.02,
                y=1.0,
                xanchor="left",
                yanchor="top",
                bgcolor="rgba(255, 255, 255, 0.7)",
            ),
        )

        unique_z = sorted(list(set(z for z in all_z_coords if z is not None)))
        if len(unique_z) > 1:
            min_z, max_z = min(unique_z), max(unique_z)
            layout.update(
                self._create_floor_selection_controls(
                    all_z_levels=unique_z,
                    min_z=min_z,
                    max_z=max_z,
                    floor_z_map_for_labels=floor_z_map,
                )
            )

        fig = go.Figure(data=node_traces + edge_traces, layout=layout)

        if output_path:
            p = pathlib.Path(output_path)
            p.parent.mkdir(parents=True, exist_ok=True)
            fig.write_html(str(p), config=self.plotter_config.get("plotly_config"))
        else:
            fig.show()
</file>

<file path="src/analysis/travel_time.py">
"""
Calculates travel times between specified room-like nodes in the graph.
"""

import pathlib
import networkx as nx
import logging
import pandas as pd
from typing import Dict, Hashable, Any

logger = logging.getLogger(__name__)


def calculate_room_travel_times(
    graph: nx.Graph,
    output_dir: pathlib.Path,
    output_filename: str = "hospital_travel_times.csv",
) -> Dict[Hashable, Any]:
    """
    Calculates shortest travel times between all pairs of service locations.

    This function identifies all nodes categorized as 'SLOT' or 'FIXED',
    calculates the travel time between them using Dijkstra's algorithm based
    on the 'weight' attribute of edges, and saves the results to a CSV file
    using pandas.

    Args:
        graph: The input NetworkX graph with nodes containing attributes.
        output_dir: The directory to save the resulting CSV file.
        output_filename: The name of the output CSV file.

    Returns:
        A dictionary where keys are source location names and values are
        dictionaries mapping target location names to travel times.
    """
    if not graph.nodes:
        logger.warning("Graph is empty. Cannot calculate travel times.")
        return {}

    location_nodes = {
        node_id: data
        for node_id, data in graph.nodes(data=True)
        if data.get("category") in ["SLOT", "FIXED"] or data.get("door_type") == "EXTERIOR"
    }

    if not location_nodes:
        logger.warning("No 'SLOT' or 'FIXED' nodes found to calculate travel times.")
        return {}

    location_names = sorted(
        [f"{data['name']}_{node_id}" for node_id, data in location_nodes.items()]
    )
    name_to_id = {name: int(name.split("_")[-1]) for name in location_names}

    logger.info(
        f"Calculating travel times for {len(location_names)} unique locations..."
    )

    all_pairs_lengths = dict(nx.all_pairs_dijkstra_path_length(graph, weight="weight"))

    df_data = {}
    for start_name in location_names:
        start_id = name_to_id[start_name]
        row = {}
        if start_id in all_pairs_lengths:
            for target_name in location_names:
                target_id = name_to_id[target_name]
                time = all_pairs_lengths[start_id].get(target_id)
                row[target_name] = round(time, 2) if time is not None else float("inf")
        else:
            logger.warning(f"Node {start_name} (ID: {start_id}) is disconnected.")
            for target_name in location_names:
                row[target_name] = float("inf")
        df_data[start_name] = row

    df = pd.DataFrame.from_dict(df_data, orient="index")
    df.index.name = "Source/Target"

    output_dir.mkdir(parents=True, exist_ok=True)
    csv_file_path = output_dir / output_filename
    try:
        df.to_csv(csv_file_path, float_format="%.2f")
        logger.info(f"Travel times matrix saved to {csv_file_path}")
    except IOError as e:
        logger.error(f"Failed to write travel times CSV to {csv_file_path}: {e}")

    return df.to_dict()
</file>

<file path="src/network_generator.py">
from pathlib import Path
from typing import Dict, Optional, Any
import networkx as nx

from src.utils.logger import setup_logger
from src.config.config_loader import ConfigLoader
from src.network.super_network import SuperNetwork
from src.plotting.plotter import PlotlyPlotter
from src.analysis.travel_time import calculate_room_travel_times
from src.analysis.slots_exporter import export_slots_to_csv

logger = setup_logger(__name__)


class NetworkGenerator:
    """
    NetworkGenerator handles the complete process of generating a multi-floor hospital network,
    including network generation, visualization, travel time calculation, and SLOT export.
    """

    def __init__(self, config:ConfigLoader ):
        """
        Initializes the NetworkGenerator.
        """
        self.config = config
        self.paths = config.paths
        self.super_network: Optional[SuperNetwork] = None
        self.super_graph: Optional[nx.Graph] = None

        logger.info("NetworkGenerator initialized.")
        logger.info(f"Results will be saved to: {self.paths.network_dir}")
        logger.info(f"Debug images will be saved to: {self.paths.debug_dir}")

    def generate_network(
        self,
        image_dir: Optional[Path] = None,
        base_floor: int = 0,
        num_processes: Optional[int] = None,
    ) -> bool:
        """
        Generates the multi-floor hospital network from floor annotation images.

        Args:
            image_dir: Floor annotation images directory.
            base_floor: Base floor.
            num_processes: Number of parallel processing.

        Returns:
            bool: Whether the generation was successful.
        """
        logger.info("Starting multi floor network generation...")

        if image_dir is None:
            image_dir = Path(self.paths.label_dir)

        if not image_dir.is_dir():
            logger.error(f"Floor annotation images directory does not exist: {image_dir}")
            return False

        image_file_paths = [
            p for p in sorted(image_dir.glob("*.png")) if p.is_file()
        ]

        if not image_file_paths:
            logger.warning(f"Image files is not found in {image_dir}")
            return False

        logger.info(f"Found {len(image_file_paths)} image files: {image_file_paths}")

        try:
            self.super_network = SuperNetwork(
                base_floor=base_floor, num_processes=num_processes
            )

            self.super_graph = self.super_network.run(image_file_paths=image_file_paths)

            if self.super_graph:
                logger.info("Complete multi floor network generation successful")
                logger.info(f"  Nodes: {self.super_graph.number_of_nodes()}")
                logger.info(f"  Edges: {self.super_graph.number_of_edges()}")
            logger.info(
                f"  Image size: Width={self.super_network.width}, Height={self.super_network.height}"
            )

            return True

        except Exception as e:
            logger.error(f"Error occurred while generating network: {e}", exc_info=True)
            return False

    def visualize_network(
        self, output_filename: str = "hospital_network_3d.html"
    ) -> bool:
        """
        Visualizes the generated network.

        Args:
            output_filename: Output file name.

        Returns:
            bool: whether the visualization was successful.
        """
        if self.super_graph is None or self.super_network is None:
            logger.error("No generated network to visualize")
            return False

        try:
            plotter = PlotlyPlotter()

            network_path = Path(self.paths.network_dir)
            network_path.mkdir(parents=True, exist_ok=True)
            plot_output_path = network_path / output_filename
            plotter.plot(
                graph=self.super_graph,
                output_path=str(plot_output_path),
                title="Multi-Floor Hospital Network",
                graph_width=self.super_network.width,
                floor_z_map=self.super_network.floor_z_map,
            )

            logger.info(f"Network visualization saved to: {plot_output_path}")
            return True

        except Exception as e:
            logger.error(f"Error occurred while visualizing network: {e}", exc_info=True)
            return False

    def calculate_travel_times(
        self, output_filename: str = "hospital_travel_times.csv"
    ) -> bool:
        """
        Calculates and saves travel times between rooms.

        Args:
            output_filename: Output file name.

        Returns:
            bool: whether the calculation was successful.
        """
        if self.super_graph is None:
            logger.error("No generated network to calculate travel times")
            return False

        try:
            logger.info("Calculating travel times between rooms...")

            network_path = Path(self.paths.network_dir)
            network_path.mkdir(parents=True, exist_ok=True)
            calculate_room_travel_times(
                graph=self.super_graph,
                output_dir=network_path,
                output_filename=output_filename,
            )

            travel_times_path = network_path / output_filename
            logger.info(f"Travel times matrix saved to: {travel_times_path}")
            return True

        except Exception as e:
            logger.error(f"Error occurred while calculating travel times: {e}", exc_info=True)
            return False

    def export_slots(self, output_filename: str = "slots.csv") -> bool:
        """
        Exports SLOT nodes to a CSV file.

        Args:
            output_filename: The name of the output CSV file.

        Returns:
            bool: True if export was successful, False otherwise.
        """
        if self.super_graph is None:
            logger.error("No generated network to export slots")
            return False

        try:
            logger.info("Exporting SLOT nodes...")
            network_path = Path(self.paths.network_dir)
            network_path.mkdir(parents=True, exist_ok=True)
            export_slots_to_csv(
                graph=self.super_graph,
                output_dir=network_path,
                output_filename=output_filename,
            )
            slots_path = network_path / output_filename
            logger.info(f"SLOT Nodes exported to: {slots_path}")
            return True
        except Exception as e:
            logger.error(f"Error occurred while exporting SLOT nodes: {e}", exc_info=True)
            return False

    def get_network_info(self) -> Dict[str, Any]:
        """
        Gets basic information about the generated network.

        Returns:
            Dict: Network information dictionary.
        """
        if self.super_graph is None or self.super_network is None:
            return {}

        return {
            "nodes_count": self.super_graph.number_of_nodes(),
            "edges_count": self.super_graph.number_of_edges(),
            "width": self.super_network.width,
            "height": self.super_network.height,
            "floor_z_map": self.super_network.floor_z_map,
            "ground_floor_z": self.super_network.designated_ground_floor_z,
        }

    def run_complete_generation(
        self,
        image_dir: Optional[Path] = None,
        visualization_filename: str = "hospital_network_3d.html",
        travel_times_filename: str = "hospital_travel_times.csv",
        slots_filename: str = "slots.csv",
    ) -> bool:
        """
        Runs the complete network generation process.

        Args:
            image_dir: floor annotation images directory
            visualization_filename: network visualization output filename
            travel_times_filename: travel times output filename
            slots_filename: SLOT nodes output filename

        Returns:
            bool: whether the complete process was successful
        """
        logger.info("Starting complete network generation process...")

        if not self.generate_network(image_dir):
            logger.error("Network generation failed, aborting process")
            return False

        if not self.visualize_network(visualization_filename):
            logger.error("Error occurred while visualizing network")

        if not self.calculate_travel_times(travel_times_filename):
            logger.error("Error occurred while calculating travel times")
            # Do not return False, as other steps might succeed

        if not self.export_slots(slots_filename):
            logger.error("Error occurred while exporting SLOT nodes")

        if self.super_graph:
            import pickle
            graph_path = Path(self.paths.network_dir)
            logger.info(f"Saving generated network graph to {graph_path / 'hospital_network.pkl'}")
            graph_path.mkdir(parents=True, exist_ok=True)
            with open(graph_path / "hospital_network.pkl", "wb") as f:
                pickle.dump(self.super_graph, f)

        logger.info("Complete network generation process completed successfully")
        network_info = self.get_network_info()
        logger.info(f"Network statistics: {network_info}")

        return True
</file>

<file path="main.py">
from pathlib import Path
import typer
from typing import Optional
from typing_extensions import Annotated

from src.network_generator import NetworkGenerator
from src.optimize_manager import OptimizeManager
from src.utils.logger import setup_logger
from src.config import config_loader

config = config_loader.ConfigLoader()
logger = setup_logger(__name__)
app = typer.Typer()


@app.command()
def network(
    image_dir: Annotated[
        Optional[Path],
        typer.Option(
            "--image-dir",
            "-i",
            help="Floor annotation images directory",
            exists=True,
            file_okay=False,
            dir_okay=True,
            readable=True,
            resolve_path=True,
        ),
    ] = None,
    vis_output: Annotated[
        str, typer.Option("--vis-output", "-v", help="Network visualization output filename")
    ] = "hospital_network_3d.html",
    travel_times_output: Annotated[
        str, typer.Option("--travel-times-output", "-t", help="Travel times matrix output filename")
    ] = "hospital_travel_times.csv",
    slots_output: Annotated[
        str, typer.Option("--slots-output", "-s", help="SLOT nodes output filename")
    ] = "slots.csv",
):

    generator = NetworkGenerator(config)

    try:
        success = generator.run_complete_generation(
            image_dir=image_dir,
            visualization_filename=vis_output,
            travel_times_filename=travel_times_output,
            slots_filename=slots_output,
        )

        if success:
            logger.info("System execution completed successfully")
        else:
            logger.error("System execution failed")
            raise typer.Exit(code=1)

    except KeyboardInterrupt:
        logger.warning("Interrupted by user")
        raise typer.Exit(code=1)
    except Exception as e:
        logger.error(f"System execution error: {e}", exc_info=True)
        raise typer.Exit(code=1)

@app.command()
def train():
    optimize_manager = OptimizeManager(config)
    optimize_manager.run()

if __name__ == "__main__":
    app()
</file>

</files>
