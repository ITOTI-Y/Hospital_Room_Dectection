This file is a merged representation of a subset of the codebase, containing specifically included files, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of a subset of the repository's contents that is considered the most important context.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Only files matching these patterns are included: src/, main.py, result/super_network_travel_times.csv
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
src/
  algorithms/
    __init__.py
    base_optimizer.py
    constraint_manager.py
    genetic_algorithm.py
    ppo_optimizer.py
    simulated_annealing.py
  analysis/
    __init__.py
    process_flow.py
    travel_time.py
    word_detect.py
  comparison/
    __init__.py
    results_comparator.py
  core/
    __init__.py
    algorithm_manager.py
    network_generator.py
  graph/
    graph_manager.py
    node.py
  image_processing/
    processor.py
  network/
    floor_manager.py
    network.py
    node_creators.py
    super_network.py
  optimization/
    optimizer.py
  plotting/
    __init__.py
    plotter.py
  rl_optimizer/
    agent/
      ppo_agent.py
    data/
      cache/
        node_variants.json
        traffic_distribution.json
      cache_manager.py
      process_templates.json
    env/
      cost_calculator.py
      layout_env.py
      vec_env_wrapper.py
    model/
      policy_network.py
    utils/
      checkpoint_callback.py
      lr_scheduler.py
      setup.py
  config.py
main.py
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="src/algorithms/__init__.py">
"""
算法模块 - 包含所有优化算法的实现
"""
</file>

<file path="src/algorithms/base_optimizer.py">
"""
基础优化器抽象类 - 定义所有优化算法的统一接口
"""

from abc import ABC, abstractmethod
from typing import List, Dict, Any, Tuple, Optional
import time
import logging
from dataclasses import dataclass

from src.rl_optimizer.env.cost_calculator import CostCalculator

logger = logging.getLogger(__name__)


@dataclass
class OptimizationResult:
    """优化结果数据类"""
    algorithm_name: str
    best_layout: List[str]
    best_cost: float
    execution_time: float
    iterations: int
    convergence_history: List[float]
    additional_metrics: Dict[str, Any]


class BaseOptimizer(ABC):
    """
    优化算法基类
    
    所有优化算法（PPO、模拟退火、遗传算法）都应继承此类，
    确保使用统一的接口和相同的目标函数（CostCalculator）。
    """
    
    def __init__(self, 
                 cost_calculator: CostCalculator,
                 constraint_manager: 'ConstraintManager',
                 name: str = "BaseOptimizer"):
        """
        初始化基础优化器
        
        Args:
            cost_calculator: 统一的成本计算器
            constraint_manager: 约束管理器
            name: 算法名称
        """
        self.cost_calculator = cost_calculator
        self.constraint_manager = constraint_manager
        self.name = name
        self.logger = logging.getLogger(f"{__name__}.{name}")
        
        # 优化过程跟踪
        self.current_iteration = 0
        self.best_cost = float('inf')
        self.best_layout = None
        self.convergence_history = []
        self.start_time = None
        
    @abstractmethod
    def optimize(self, 
                 initial_layout: Optional[List[str]] = None,
                 max_iterations: int = 1000,
                 **kwargs) -> OptimizationResult:
        """
        执行优化算法
        
        Args:
            initial_layout: 初始布局，如果为None则生成随机布局
            max_iterations: 最大迭代次数
            **kwargs: 算法特定参数
            
        Returns:
            OptimizationResult: 优化结果
        """
        pass
    
    def evaluate_layout(self, layout: List[str]) -> float:
        """
        评估布局的成本
        
        Args:
            layout: 待评估的布局
            
        Returns:
            float: 布局成本
        """
        if not self.constraint_manager.is_valid_layout(layout):
            return float('inf')
        
        return self.cost_calculator.calculate_total_cost(layout)
    
    def generate_initial_layout(self) -> List[str]:
        """
        生成符合约束的初始布局
        
        Returns:
            List[str]: 初始布局
        """
        return self.constraint_manager.generate_valid_layout()
    
    def update_best_solution(self, layout: List[str], cost: float):
        """
        更新最优解
        
        Args:
            layout: 候选布局
            cost: 布局成本
        """
        if cost < self.best_cost:
            self.best_cost = cost
            self.best_layout = layout.copy()
            self.logger.info(f"发现更优解: 成本 = {cost:.2f}, 迭代 = {self.current_iteration}")
    
    def start_optimization(self):
        """开始优化过程"""
        self.start_time = time.time()
        self.current_iteration = 0
        self.best_cost = float('inf')
        self.best_layout = None
        self.convergence_history = []
        self.logger.info(f"开始 {self.name} 优化")
    
    def finish_optimization(self) -> OptimizationResult:
        """
        结束优化过程并返回结果
        
        Returns:
            OptimizationResult: 优化结果
        """
        execution_time = time.time() - self.start_time if self.start_time else 0
        
        result = OptimizationResult(
            algorithm_name=self.name,
            best_layout=self.best_layout,
            best_cost=self.best_cost,
            execution_time=execution_time,
            iterations=self.current_iteration,
            convergence_history=self.convergence_history.copy(),
            additional_metrics=self.get_additional_metrics()
        )
        
        self.logger.info(f"{self.name} 优化完成:")
        self.logger.info(f"  最优成本: {self.best_cost:.2f}")
        self.logger.info(f"  执行时间: {execution_time:.2f}s")
        self.logger.info(f"  迭代次数: {self.current_iteration}")
        
        return result
    
    def get_additional_metrics(self) -> Dict[str, Any]:
        """
        获取算法特定的额外指标
        
        Returns:
            Dict[str, Any]: 额外指标字典
        """
        return {}
    
    def log_iteration(self, cost: float, additional_info: str = ""):
        """
        记录迭代信息
        
        Args:
            cost: 当前成本
            additional_info: 额外信息
        """
        self.convergence_history.append(cost)
        
        if self.current_iteration % 100 == 0:  # 每100次迭代记录一次
            self.logger.debug(f"迭代 {self.current_iteration}: 成本 = {cost:.2f} {additional_info}")
</file>

<file path="src/algorithms/constraint_manager.py">
"""
约束管理器 - 统一处理布局优化中的各种约束条件
"""

import logging
import random
from typing import List, Dict, Set, Tuple, Optional, Any
from dataclasses import dataclass
import pandas as pd

logger = logging.getLogger(__name__)


@dataclass
class DepartmentInfo:
    """科室信息"""
    name: str
    area_requirement: float
    is_fixed: bool = False
    fixed_position: Optional[int] = None
    adjacency_preferences: List[str] = None
    
    def __post_init__(self):
        if self.adjacency_preferences is None:
            self.adjacency_preferences = []


@dataclass 
class SlotInfo:
    """槽位信息"""
    index: int
    name: str
    area: float
    is_available: bool = True


class ConstraintManager:
    """
    约束管理器
    
    统一管理布局优化中的各种约束条件，包括：
    - 面积约束：科室面积需求与槽位面积的匹配
    - 固定位置约束：某些科室必须位于特定位置
    - 相邻性约束：某些科室之间的邻接偏好
    - 容量约束：每个槽位只能放置一个科室
    """
    
    def __init__(self,
                 placeable_slots: List[str],
                 placeable_departments: List[str],
                 travel_times: pd.DataFrame,
                 area_tolerance_ratio: float = 0.3):
        """
        初始化约束管理器
        
        Args:
            placeable_slots: 可用槽位列表（原始节点名）
            placeable_departments: 可放置科室列表
            travel_times: 行程时间矩阵（用于获取槽位信息）
            area_tolerance_ratio: 面积容忍比例
        """
        self.placeable_slots = placeable_slots
        self.placeable_departments = placeable_departments
        self.travel_times = travel_times
        self.area_tolerance_ratio = area_tolerance_ratio
        
        # 初始化槽位和科室信息
        self.slots_info = self._initialize_slots_info()
        self.departments_info = self._initialize_departments_info()
        
        # 构建约束关系
        self.area_compatibility_matrix = self._build_area_compatibility_matrix()
        self.fixed_assignments = self._build_fixed_assignments()
        
        logger.info(f"约束管理器初始化完成:")
        logger.info(f"  可用槽位数: {len(self.slots_info)}")
        logger.info(f"  可放置科室数: {len(self.departments_info)}")
        logger.info(f"  面积兼容对数: {self.area_compatibility_matrix.sum()}")
        logger.info(f"  固定分配数: {len(self.fixed_assignments)}")
    
    def _initialize_slots_info(self) -> List[SlotInfo]:
        """初始化槽位信息"""
        slots_info = []
        for i, slot_name in enumerate(self.placeable_slots):
            # 从行程时间矩阵中获取槽位面积信息
            # 这里假设槽位面积存储在travel_times的索引属性中
            # 如果没有面积信息，使用默认值
            area = self._get_slot_area(slot_name)
            slots_info.append(SlotInfo(
                index=i,
                name=slot_name,
                area=area,
                is_available=True
            ))
        return slots_info
    
    def _initialize_departments_info(self) -> List[DepartmentInfo]:
        """初始化科室信息"""
        departments_info = []
        for dept_name in self.placeable_departments:
            # 获取科室面积需求
            area_req = self._get_department_area_requirement(dept_name)
            
            # 检查是否为固定科室
            is_fixed, fixed_pos = self._check_fixed_department(dept_name)
            
            departments_info.append(DepartmentInfo(
                name=dept_name,
                area_requirement=area_req,
                is_fixed=is_fixed,
                fixed_position=fixed_pos,
                adjacency_preferences=self._get_adjacency_preferences(dept_name)
            ))
        return departments_info
    
    def _get_slot_area(self, slot_name: str) -> float:
        """
        获取槽位面积
        
        Args:
            slot_name: 槽位名称
            
        Returns:
            float: 槽位面积
        """
        # 这里需要根据实际的数据结构来获取面积
        # 临时使用随机值，实际应该从网络节点属性或配置中获取
        return random.uniform(50, 500)  # 临时实现
    
    def _get_department_area_requirement(self, dept_name: str) -> float:
        """
        获取科室面积需求
        
        Args:
            dept_name: 科室名称
            
        Returns:
            float: 面积需求
        """
        # 这里应该从配置或数据中获取科室面积需求
        # 临时使用随机值
        return random.uniform(80, 400)  # 临时实现
    
    def _check_fixed_department(self, dept_name: str) -> Tuple[bool, Optional[int]]:
        """
        检查科室是否需要固定位置
        
        Args:
            dept_name: 科室名称
            
        Returns:
            Tuple[bool, Optional[int]]: (是否固定, 固定位置索引)
        """
        # 这里定义需要固定位置的科室
        # 例如：入口、急诊科等可能需要固定在特定位置
        fixed_departments = {
            '急诊科': 0,  # 固定在第一个槽位
            '入口': None,  # 固定但位置待定
        }
        
        if dept_name in fixed_departments:
            return True, fixed_departments[dept_name]
        return False, None
    
    def _get_adjacency_preferences(self, dept_name: str) -> List[str]:
        """
        获取科室相邻偏好
        
        Args:
            dept_name: 科室名称
            
        Returns:
            List[str]: 偏好相邻的科室列表
        """
        # 定义科室间的相邻偏好关系
        adjacency_preferences = {
            '妇科': ['超声科', '采血处'],
            '心血管内科': ['采血处', '超声科', '放射科'],
            '呼吸内科': ['采血处', '放射科'],
            '采血处': ['检验中心'],
            '超声科': ['放射科'],
        }
        
        return adjacency_preferences.get(dept_name, [])
    
    def _build_area_compatibility_matrix(self) -> any:
        """
        构建面积兼容性矩阵
        
        Returns:
            numpy.ndarray: 兼容性矩阵 [slots x departments]
        """
        import numpy as np
        
        num_slots = len(self.slots_info)
        num_depts = len(self.departments_info)
        compatibility = np.zeros((num_slots, num_depts), dtype=bool)
        
        for i, slot in enumerate(self.slots_info):
            for j, dept in enumerate(self.departments_info):
                # 检查面积兼容性
                area_diff = abs(slot.area - dept.area_requirement)
                max_allowed_diff = dept.area_requirement * self.area_tolerance_ratio
                compatibility[i, j] = area_diff <= max_allowed_diff
        
        return compatibility
    
    def _build_fixed_assignments(self) -> Dict[str, int]:
        """
        构建固定分配映射
        
        Returns:
            Dict[str, int]: 科室名 -> 槽位索引的映射
        """
        fixed_assignments = {}
        for dept in self.departments_info:
            if dept.is_fixed and dept.fixed_position is not None:
                fixed_assignments[dept.name] = dept.fixed_position
        return fixed_assignments
    
    def is_valid_layout(self, layout: List[str]) -> bool:
        """
        检查布局是否满足所有约束
        
        Args:
            layout: 布局列表，索引为槽位，值为科室名
            
        Returns:
            bool: 是否有效
        """
        if len(layout) != len(self.slots_info):
            return False
        
        # 检查固定位置约束
        if not self._check_fixed_constraints(layout):
            return False
        
        # 检查面积约束
        if not self._check_area_constraints(layout):
            return False
        
        # 检查唯一性约束（每个科室只能出现一次）
        if not self._check_uniqueness_constraints(layout):
            return False
        
        return True
    
    def _check_fixed_constraints(self, layout: List[str]) -> bool:
        """检查固定位置约束"""
        for dept_name, fixed_slot_idx in self.fixed_assignments.items():
            if layout[fixed_slot_idx] != dept_name:
                return False
        return True
    
    def _check_area_constraints(self, layout: List[str]) -> bool:
        """检查面积约束"""
        for slot_idx, dept_name in enumerate(layout):
            if dept_name is None:
                continue
            
            dept_idx = self._get_department_index(dept_name)
            if dept_idx is None:
                return False
            
            if not self.area_compatibility_matrix[slot_idx, dept_idx]:
                return False
        
        return True
    
    def _check_uniqueness_constraints(self, layout: List[str]) -> bool:
        """检查唯一性约束"""
        non_none_depts = [dept for dept in layout if dept is not None]
        return len(non_none_depts) == len(set(non_none_depts))
    
    def _get_department_index(self, dept_name: str) -> Optional[int]:
        """获取科室在departments_info中的索引"""
        for i, dept in enumerate(self.departments_info):
            if dept.name == dept_name:
                return i
        return None
    
    def generate_valid_layout(self) -> List[str]:
        """
        生成一个满足约束的有效布局
        
        Returns:
            List[str]: 有效布局
        """
        layout = [None] * len(self.slots_info)
        
        # 首先放置固定科室
        available_depts = set(self.placeable_departments)
        for dept_name, slot_idx in self.fixed_assignments.items():
            layout[slot_idx] = dept_name
            available_depts.remove(dept_name)
        
        # 随机放置剩余科室
        available_slots = [i for i, dept in enumerate(layout) if dept is None]
        available_depts_list = list(available_depts)
        
        for slot_idx in available_slots:
            if not available_depts_list:
                break
                
            # 找到与该槽位面积兼容的科室
            compatible_depts = []
            for dept_name in available_depts_list:
                dept_idx = self._get_department_index(dept_name)
                if dept_idx is not None and self.area_compatibility_matrix[slot_idx, dept_idx]:
                    compatible_depts.append(dept_name)
            
            if compatible_depts:
                chosen_dept = random.choice(compatible_depts)
                layout[slot_idx] = chosen_dept
                available_depts_list.remove(chosen_dept)
        
        return layout
    
    def get_compatible_departments(self, slot_idx: int) -> List[str]:
        """
        获取与指定槽位兼容的科室列表
        
        Args:
            slot_idx: 槽位索引
            
        Returns:
            List[str]: 兼容的科室名称列表
        """
        compatible_depts = []
        for dept_idx, dept in enumerate(self.departments_info):
            if self.area_compatibility_matrix[slot_idx, dept_idx]:
                compatible_depts.append(dept.name)
        return compatible_depts
    
    def get_swap_candidates(self, layout: List[str], slot1: int, slot2: int) -> bool:
        """
        检查两个槽位是否可以交换科室
        
        Args:
            layout: 当前布局
            slot1: 槽位1索引
            slot2: 槽位2索引
            
        Returns:
            bool: 是否可以交换
        """
        dept1, dept2 = layout[slot1], layout[slot2]
        
        # 创建交换后的布局进行验证
        new_layout = layout.copy()
        new_layout[slot1], new_layout[slot2] = dept2, dept1
        
        return self.is_valid_layout(new_layout)
    
    def calculate_constraint_violation_penalty(self, layout: List[str]) -> float:
        """
        计算布局的约束违反惩罚
        
        Args:
            layout: 布局
            
        Returns:
            float: 惩罚值（0表示无违反）
        """
        penalty = 0.0
        
        # 固定位置违反惩罚
        for dept_name, fixed_slot_idx in self.fixed_assignments.items():
            if layout[fixed_slot_idx] != dept_name:
                penalty += 1000.0  # 高惩罚
        
        # 面积约束违反惩罚
        for slot_idx, dept_name in enumerate(layout):
            if dept_name is None:
                continue
            
            dept_idx = self._get_department_index(dept_name)
            if dept_idx is not None and not self.area_compatibility_matrix[slot_idx, dept_idx]:
                penalty += 500.0  # 中等惩罚
        
        return penalty
</file>

<file path="src/algorithms/genetic_algorithm.py">
"""
遗传算法优化器 - 基于遗传算法的布局优化
"""

import random
import copy
import logging
from typing import List, Optional, Dict, Any, Tuple
from dataclasses import dataclass

from src.algorithms.base_optimizer import BaseOptimizer, OptimizationResult
from src.algorithms.constraint_manager import ConstraintManager
from src.rl_optimizer.env.cost_calculator import CostCalculator

logger = logging.getLogger(__name__)


@dataclass
class Individual:
    """遗传算法中的个体"""
    layout: List[str]
    fitness: float = float('inf')
    age: int = 0
    
    def __post_init__(self):
        if self.fitness == float('inf'):
            # 如果没有设置适应度，则需要外部计算
            pass


class GeneticAlgorithmOptimizer(BaseOptimizer):
    """
    遗传算法优化器
    
    使用遗传算法进行布局优化。遗传算法模拟自然选择和进化过程，
    通过选择、交叉、变异等操作来寻找最优解。
    """
    
    def __init__(self, 
                 cost_calculator: CostCalculator,
                 constraint_manager: ConstraintManager,
                 population_size: int = 100,
                 elite_size: int = 20,
                 mutation_rate: float = 0.1,
                 crossover_rate: float = 0.8,
                 tournament_size: int = 5,
                 max_age: int = 50):
        """
        初始化遗传算法优化器
        
        Args:
            cost_calculator: 成本计算器
            constraint_manager: 约束管理器
            population_size: 种群大小
            elite_size: 精英个体数量
            mutation_rate: 变异率
            crossover_rate: 交叉率
            tournament_size: 锦标赛选择的参与者数量
            max_age: 个体最大年龄
        """
        super().__init__(cost_calculator, constraint_manager, "GeneticAlgorithm")
        
        self.population_size = population_size
        self.elite_size = elite_size
        self.mutation_rate = mutation_rate
        self.crossover_rate = crossover_rate
        self.tournament_size = tournament_size
        self.max_age = max_age
        
        # 算法状态
        self.population = []
        self.generation = 0
        self.stagnation_count = 0
        self.best_fitness_history = []
        
        logger.info(f"遗传算法优化器初始化完成:")
        logger.info(f"  种群大小: {population_size}")
        logger.info(f"  精英数量: {elite_size}")
        logger.info(f"  变异率: {mutation_rate}")
        logger.info(f"  交叉率: {crossover_rate}")
        logger.info(f"  锦标赛大小: {tournament_size}")
    
    def optimize(self, 
                 initial_layout: Optional[List[str]] = None,
                 max_iterations: int = 1000,
                 convergence_threshold: int = 50,
                 **kwargs) -> OptimizationResult:
        """
        执行遗传算法优化
        
        Args:
            initial_layout: 初始布局（用于种群初始化的种子）
            max_iterations: 最大代数
            convergence_threshold: 收敛阈值（连续多少代无改进则停止）
            **kwargs: 其他参数
            
        Returns:
            OptimizationResult: 优化结果
        """
        self.start_optimization()
        
        logger.info(f"遗传算法优化开始，最大代数: {max_iterations}")
        
        # 初始化种群
        self._initialize_population(initial_layout)
        
        # 评估初始种群
        self._evaluate_population()
        
        # 更新最优解
        best_individual = min(self.population, key=lambda x: x.fitness)
        self.update_best_solution(best_individual.layout, best_individual.fitness)
        self.best_fitness_history.append(best_individual.fitness)
        
        logger.info(f"初始种群最优适应度: {best_individual.fitness:.2f}")
        
        # 重置算法状态
        self.generation = 0
        self.stagnation_count = 0
        
        try:
            while (self.generation < max_iterations and 
                   self.stagnation_count < convergence_threshold):
                
                # 执行一代进化
                self._evolve_generation()
                
                # 评估种群
                self._evaluate_population()
                
                # 更新最优解
                current_best = min(self.population, key=lambda x: x.fitness)
                previous_best = self.best_cost
                
                if current_best.fitness < self.best_cost:
                    self.update_best_solution(current_best.layout, current_best.fitness)
                    self.stagnation_count = 0
                    logger.info(f"第{self.generation}代发现更优解: {current_best.fitness:.2f}")
                else:
                    self.stagnation_count += 1
                
                self.best_fitness_history.append(current_best.fitness)
                self.generation += 1
                self.current_iteration = self.generation
                
                # 记录进化信息
                avg_fitness = sum(ind.fitness for ind in self.population) / len(self.population)
                self.log_iteration(current_best.fitness, 
                                 f"avg={avg_fitness:.2f}, stagnation={self.stagnation_count}")
                
                if self.generation % 50 == 0:
                    logger.info(f"第{self.generation}代: "
                              f"最优={current_best.fitness:.2f}, "
                              f"平均={avg_fitness:.2f}, "
                              f"停滞={self.stagnation_count}")
                
                # 种群多样性维护
                if self.generation % 100 == 0:
                    self._maintain_diversity()
                    
        except KeyboardInterrupt:
            logger.warning("遗传算法优化被用户中断")
        
        logger.info(f"遗传算法优化完成:")
        logger.info(f"  总代数: {self.generation}")
        logger.info(f"  停滞代数: {self.stagnation_count}")
        logger.info(f"  种群多样性: {self._calculate_diversity():.3f}")
        
        return self.finish_optimization()
    
    def _initialize_population(self, seed_layout: Optional[List[str]] = None):
        """
        初始化种群
        
        Args:
            seed_layout: 种子布局，用于生成部分个体
        """
        self.population = []
        
        # 如果有种子布局，用它生成一些个体
        if seed_layout is not None:
            self.population.append(Individual(layout=seed_layout.copy()))
            
            # 基于种子布局生成变异个体
            for _ in range(min(10, self.population_size // 4)):
                variant = seed_layout.copy()
                self._mutate_individual_layout(variant)
                if self.constraint_manager.is_valid_layout(variant):
                    self.population.append(Individual(layout=variant))
        
        # 生成剩余的随机个体
        while len(self.population) < self.population_size:
            random_layout = self.generate_initial_layout()
            self.population.append(Individual(layout=random_layout))
        
        logger.info(f"初始化种群完成，大小: {len(self.population)}")
    
    def _evaluate_population(self):
        """评估种群中所有个体的适应度"""
        for individual in self.population:
            if individual.fitness == float('inf'):  # 只计算未评估的个体
                individual.fitness = self.evaluate_layout(individual.layout)
    
    def _evolve_generation(self):
        """执行一代进化"""
        new_population = []
        
        # 1. 精英保留
        elite_individuals = self._select_elite()
        new_population.extend(elite_individuals)
        
        # 2. 生成新个体
        while len(new_population) < self.population_size:
            if random.random() < self.crossover_rate:
                # 交叉生成后代
                parent1 = self._tournament_selection()
                parent2 = self._tournament_selection()
                child1, child2 = self._crossover(parent1, parent2)
                
                # 变异
                if random.random() < self.mutation_rate:
                    self._mutate_individual(child1)
                if random.random() < self.mutation_rate:
                    self._mutate_individual(child2)
                
                new_population.extend([child1, child2])
            else:
                # 复制并变异
                parent = self._tournament_selection()
                child = Individual(layout=parent.layout.copy())
                if random.random() < self.mutation_rate:
                    self._mutate_individual(child)
                new_population.append(child)
        
        # 3. 更新年龄并移除过老个体
        new_population = self._age_and_filter_population(new_population)
        
        # 4. 裁剪到目标大小
        if len(new_population) > self.population_size:
            new_population = new_population[:self.population_size]
        
        self.population = new_population
    
    def _select_elite(self) -> List[Individual]:
        """选择精英个体"""
        # 按适应度排序并选择最优的
        sorted_pop = sorted(self.population, key=lambda x: x.fitness)
        elite = []
        
        for individual in sorted_pop[:self.elite_size]:
            elite_copy = Individual(
                layout=individual.layout.copy(),
                fitness=individual.fitness,
                age=individual.age + 1
            )
            elite.append(elite_copy)
        
        return elite
    
    def _tournament_selection(self) -> Individual:
        """锦标赛选择"""
        tournament = random.sample(self.population, 
                                 min(self.tournament_size, len(self.population)))
        return min(tournament, key=lambda x: x.fitness)
    
    def _crossover(self, parent1: Individual, parent2: Individual) -> Tuple[Individual, Individual]:
        """
        交叉操作 - 使用基于位置的交叉 (Position-based Crossover)
        
        Args:
            parent1: 父代1
            parent2: 父代2
            
        Returns:
            Tuple[Individual, Individual]: 两个子代
        """
        layout1, layout2 = parent1.layout.copy(), parent2.layout.copy()
        
        # 选择交叉点
        if len(layout1) > 2:
            # 使用顺序交叉 (Order Crossover, OX)
            child1_layout, child2_layout = self._order_crossover(layout1, layout2)
        else:
            child1_layout, child2_layout = layout1, layout2
        
        # 修复约束违反
        child1_layout = self._repair_layout_constraints(child1_layout)
        child2_layout = self._repair_layout_constraints(child2_layout)
        
        child1 = Individual(layout=child1_layout)
        child2 = Individual(layout=child2_layout)
        
        return child1, child2
    
    def _order_crossover(self, parent1: List[str], parent2: List[str]) -> Tuple[List[str], List[str]]:
        """顺序交叉操作"""
        length = len(parent1)
        
        # 随机选择交叉区间
        start = random.randint(0, length - 1)
        end = random.randint(start, length - 1)
        
        # 初始化子代
        child1 = [None] * length
        child2 = [None] * length
        
        # 复制交叉区间
        child1[start:end+1] = parent1[start:end+1]
        child2[start:end+1] = parent2[start:end+1]
        
        # 填充剩余位置
        self._fill_remaining_positions(child1, parent2, start, end)
        self._fill_remaining_positions(child2, parent1, start, end)
        
        return child1, child2
    
    def _fill_remaining_positions(self, child: List[str], parent: List[str], start: int, end: int):
        """填充顺序交叉中的剩余位置"""
        child_set = set(child[start:end+1]) - {None}
        fill_items = [item for item in parent if item not in child_set and item is not None]
        
        fill_index = 0
        for i in range(len(child)):
            if child[i] is None and fill_index < len(fill_items):
                child[i] = fill_items[fill_index]
                fill_index += 1
    
    def _repair_layout_constraints(self, layout: List[str]) -> List[str]:
        """修复布局约束违反"""
        if self.constraint_manager.is_valid_layout(layout):
            return layout
        
        # 尝试修复
        repaired = layout.copy()
        
        # 处理重复的科室
        seen = set()
        duplicates = []
        for i, dept in enumerate(repaired):
            if dept is not None:
                if dept in seen:
                    duplicates.append(i)
                else:
                    seen.add(dept)
        
        # 用未使用的科室替换重复的科室
        available_depts = set(self.constraint_manager.placeable_departments) - seen
        available_depts_list = list(available_depts)
        
        for i, dup_idx in enumerate(duplicates):
            if i < len(available_depts_list):
                repaired[dup_idx] = available_depts_list[i]
        
        # 如果仍不满足约束，返回一个新的有效布局
        if not self.constraint_manager.is_valid_layout(repaired):
            return self.generate_initial_layout()
        
        return repaired
    
    def _mutate_individual(self, individual: Individual):
        """个体变异"""
        self._mutate_individual_layout(individual.layout)
        individual.fitness = float('inf')  # 重置适应度，需要重新计算
    
    def _mutate_individual_layout(self, layout: List[str]):
        """布局变异操作"""
        mutation_type = random.choice(['swap', 'insert', 'scramble'])
        
        if mutation_type == 'swap' and len(layout) >= 2:
            # 交换变异
            i, j = random.sample(range(len(layout)), 2)
            if self.constraint_manager.get_swap_candidates(layout, i, j):
                layout[i], layout[j] = layout[j], layout[i]
        
        elif mutation_type == 'insert' and len(layout) >= 2:
            # 插入变异
            i = random.randint(0, len(layout) - 1)
            j = random.randint(0, len(layout) - 1)
            if i != j:
                item = layout.pop(i)
                layout.insert(j, item)
        
        elif mutation_type == 'scramble' and len(layout) >= 3:
            # 乱序变异
            start = random.randint(0, len(layout) - 3)
            end = random.randint(start + 2, len(layout))
            segment = layout[start:end]
            random.shuffle(segment)
            layout[start:end] = segment
    
    def _age_and_filter_population(self, population: List[Individual]) -> List[Individual]:
        """更新年龄并过滤过老的个体"""
        filtered = []
        for individual in population:
            individual.age += 1
            if individual.age <= self.max_age:
                filtered.append(individual)
            else:
                # 替换过老的个体
                new_individual = Individual(layout=self.generate_initial_layout())
                filtered.append(new_individual)
        
        return filtered
    
    def _maintain_diversity(self):
        """维护种群多样性"""
        # 计算多样性
        diversity = self._calculate_diversity()
        
        if diversity < 0.1:  # 多样性过低
            logger.info(f"种群多样性过低({diversity:.3f})，注入新个体")
            
            # 替换一部分个体为随机个体
            num_to_replace = self.population_size // 4
            worst_individuals = sorted(self.population, key=lambda x: x.fitness, reverse=True)
            
            for i in range(min(num_to_replace, len(worst_individuals))):
                new_layout = self.generate_initial_layout()
                worst_individuals[i].layout = new_layout
                worst_individuals[i].fitness = float('inf')
                worst_individuals[i].age = 0
    
    def _calculate_diversity(self) -> float:
        """计算种群多样性"""
        if len(self.population) < 2:
            return 1.0
        
        total_distance = 0
        comparisons = 0
        
        for i in range(len(self.population)):
            for j in range(i + 1, len(self.population)):
                distance = self._layout_distance(self.population[i].layout, self.population[j].layout)
                total_distance += distance
                comparisons += 1
        
        if comparisons == 0:
            return 1.0
        
        avg_distance = total_distance / comparisons
        max_distance = len(self.population[0].layout)  # 最大可能距离
        
        return avg_distance / max_distance if max_distance > 0 else 0
    
    def _layout_distance(self, layout1: List[str], layout2: List[str]) -> float:
        """计算两个布局之间的距离"""
        if len(layout1) != len(layout2):
            return float('inf')
        
        # 使用汉明距离
        distance = sum(1 for a, b in zip(layout1, layout2) if a != b)
        return distance / len(layout1)
    
    def get_additional_metrics(self) -> Dict[str, Any]:
        """获取遗传算法特定的额外指标"""
        diversity = self._calculate_diversity() if self.population else 0
        
        return {
            "population_size": self.population_size,
            "elite_size": self.elite_size,
            "mutation_rate": self.mutation_rate,
            "crossover_rate": self.crossover_rate,
            "tournament_size": self.tournament_size,
            "max_age": self.max_age,
            "final_generation": self.generation,
            "stagnation_count": self.stagnation_count,
            "population_diversity": diversity,
            "best_fitness_history": self.best_fitness_history.copy(),
            "convergence_rate": (self.best_fitness_history[0] - self.best_cost) / self.best_fitness_history[0] 
                               if self.best_fitness_history and self.best_fitness_history[0] > 0 else 0
        }
</file>

<file path="src/algorithms/simulated_annealing.py">
"""
模拟退火优化器 - 基于模拟退火算法的布局优化
"""

import math
import random
import copy
import logging
from typing import List, Optional, Dict, Any

from src.algorithms.base_optimizer import BaseOptimizer, OptimizationResult
from src.algorithms.constraint_manager import ConstraintManager
from src.rl_optimizer.env.cost_calculator import CostCalculator

logger = logging.getLogger(__name__)


class SimulatedAnnealingOptimizer(BaseOptimizer):
    """
    模拟退火优化器
    
    使用模拟退火算法进行布局优化。模拟退火是一种启发式全局优化算法，
    通过模拟固体退火过程来寻找全局最优解。
    """
    
    def __init__(self, 
                 cost_calculator: CostCalculator,
                 constraint_manager: ConstraintManager,
                 initial_temperature: float = 1000.0,
                 final_temperature: float = 0.1,
                 cooling_rate: float = 0.95,
                 temperature_length: int = 100):
        """
        初始化模拟退火优化器
        
        Args:
            cost_calculator: 成本计算器
            constraint_manager: 约束管理器
            initial_temperature: 初始温度
            final_temperature: 终止温度
            cooling_rate: 冷却速率
            temperature_length: 每个温度下的迭代次数
        """
        super().__init__(cost_calculator, constraint_manager, "SimulatedAnnealing")
        
        self.initial_temperature = initial_temperature
        self.final_temperature = final_temperature
        self.cooling_rate = cooling_rate
        self.temperature_length = temperature_length
        
        # 算法状态
        self.current_temperature = initial_temperature
        self.acceptance_count = 0
        self.rejection_count = 0
        self.improvement_count = 0
        
        logger.info(f"模拟退火优化器初始化完成:")
        logger.info(f"  初始温度: {initial_temperature}")
        logger.info(f"  终止温度: {final_temperature}") 
        logger.info(f"  冷却速率: {cooling_rate}")
        logger.info(f"  温度长度: {temperature_length}")
    
    def optimize(self, 
                 initial_layout: Optional[List[str]] = None,
                 max_iterations: int = 10000,
                 **kwargs) -> OptimizationResult:
        """
        执行模拟退火优化
        
        Args:
            initial_layout: 初始布局
            max_iterations: 最大迭代次数
            **kwargs: 其他参数
            
        Returns:
            OptimizationResult: 优化结果
        """
        self.start_optimization()
        
        # 初始化当前解
        if initial_layout is None:
            current_layout = self.generate_initial_layout()
        else:
            current_layout = initial_layout.copy()
            
        current_cost = self.evaluate_layout(current_layout)
        self.update_best_solution(current_layout, current_cost)
        
        logger.info(f"模拟退火优化开始，初始成本: {current_cost:.2f}")
        
        # 重置算法状态
        self.current_temperature = self.initial_temperature
        self.acceptance_count = 0
        self.rejection_count = 0
        self.improvement_count = 0
        
        try:
            while (self.current_temperature > self.final_temperature and 
                   self.current_iteration < max_iterations):
                
                # 在当前温度下进行多次迭代
                for _ in range(self.temperature_length):
                    if self.current_iteration >= max_iterations:
                        break
                    
                    # 生成邻域解
                    neighbor_layout = self._generate_neighbor(current_layout)
                    neighbor_cost = self.evaluate_layout(neighbor_layout)
                    
                    # 计算成本差
                    delta_cost = neighbor_cost - current_cost
                    
                    # 决定是否接受新解
                    if self._accept_solution(delta_cost, self.current_temperature):
                        current_layout = neighbor_layout
                        current_cost = neighbor_cost
                        self.acceptance_count += 1
                        
                        # 检查是否为新的最优解
                        if neighbor_cost < self.best_cost:
                            self.update_best_solution(neighbor_layout, neighbor_cost)
                            self.improvement_count += 1
                    else:
                        self.rejection_count += 1
                    
                    self.current_iteration += 1
                    self.log_iteration(current_cost, f"T={self.current_temperature:.2f}")
                
                # 降温
                self.current_temperature *= self.cooling_rate
                
                if self.current_iteration % 1000 == 0:
                    logger.info(f"迭代 {self.current_iteration}: "
                              f"当前成本={current_cost:.2f}, "
                              f"最优成本={self.best_cost:.2f}, "
                              f"温度={self.current_temperature:.2f}")
                              
        except KeyboardInterrupt:
            logger.warning("模拟退火优化被用户中断")
        
        logger.info(f"模拟退火优化完成:")
        logger.info(f"  最终温度: {self.current_temperature:.2f}")
        logger.info(f"  接受次数: {self.acceptance_count}")
        logger.info(f"  拒绝次数: {self.rejection_count}")
        logger.info(f"  改进次数: {self.improvement_count}")
        logger.info(f"  接受率: {self.acceptance_count/(self.acceptance_count + self.rejection_count)*100:.1f}%")
        
        return self.finish_optimization()
    
    def _generate_neighbor(self, current_layout: List[str]) -> List[str]:
        """
        生成邻域解
        
        Args:
            current_layout: 当前布局
            
        Returns:
            List[str]: 邻域布局
        """
        neighbor = current_layout.copy()
        
        # 选择邻域生成策略
        strategies = ['swap', 'relocate', 'multiple_swap']
        strategy = random.choice(strategies)
        
        if strategy == 'swap':
            # 交换两个科室的位置
            self._swap_departments(neighbor)
        elif strategy == 'relocate':
            # 重新定位一个科室
            self._relocate_department(neighbor)
        elif strategy == 'multiple_swap':
            # 多次小幅调整
            num_swaps = random.randint(2, min(5, len(neighbor)//2))
            for _ in range(num_swaps):
                if random.random() < 0.7:
                    self._swap_departments(neighbor)
                else:
                    self._relocate_department(neighbor)
        
        # 确保生成的邻域解满足约束
        if not self.constraint_manager.is_valid_layout(neighbor):
            # 如果不满足约束，尝试修复或返回原布局
            neighbor = self._repair_layout(neighbor) or current_layout.copy()
        
        return neighbor
    
    def _swap_departments(self, layout: List[str]):
        """随机交换两个科室的位置"""
        if len(layout) < 2:
            return
            
        # 随机选择两个不同的位置
        pos1, pos2 = random.sample(range(len(layout)), 2)
        
        # 检查是否可以交换（考虑固定位置约束）
        if self.constraint_manager.get_swap_candidates(layout, pos1, pos2):
            layout[pos1], layout[pos2] = layout[pos2], layout[pos1]
    
    def _relocate_department(self, layout: List[str]):
        """重新定位一个科室到新位置"""
        if len(layout) < 2:
            return
            
        # 随机选择一个科室和新位置
        old_pos = random.randint(0, len(layout) - 1)
        new_pos = random.randint(0, len(layout) - 1)
        
        if old_pos != new_pos:
            dept = layout[old_pos]
            # 移除科室
            layout[old_pos] = None
            # 将后续科室前移
            for i in range(old_pos, len(layout) - 1):
                layout[i] = layout[i + 1]
            # 在新位置插入科室
            layout.insert(new_pos, dept)
            # 保持长度一致
            if len(layout) > len(self.constraint_manager.slots_info):
                layout.pop()
    
    def _repair_layout(self, layout: List[str]) -> Optional[List[str]]:
        """
        尝试修复不满足约束的布局
        
        Args:
            layout: 待修复的布局
            
        Returns:
            Optional[List[str]]: 修复后的布局，如果无法修复则返回None
        """
        repaired = layout.copy()
        
        # 简单修复策略：随机重新分配违反约束的科室
        for attempt in range(10):  # 最多尝试10次
            if self.constraint_manager.is_valid_layout(repaired):
                return repaired
                
            # 找到违反约束的位置并尝试修复
            for i, dept in enumerate(repaired):
                if dept is not None:
                    compatible_depts = self.constraint_manager.get_compatible_departments(i)
                    if dept not in compatible_depts and compatible_depts:
                        # 尝试找到一个兼容的科室进行交换
                        for j, other_dept in enumerate(repaired):
                            if other_dept in compatible_depts:
                                repaired[i], repaired[j] = repaired[j], repaired[i]
                                break
        
        return None  # 无法修复
    
    def _accept_solution(self, delta_cost: float, temperature: float) -> bool:
        """
        根据模拟退火接受准则决定是否接受新解
        
        Args:
            delta_cost: 成本差值
            temperature: 当前温度
            
        Returns:
            bool: 是否接受新解
        """
        if delta_cost <= 0:
            # 更优解总是接受
            return True
        
        if temperature <= 0:
            # 温度为0时只接受更优解
            return False
        
        # 按概率接受较差解
        probability = math.exp(-delta_cost / temperature)
        return random.random() < probability
    
    def get_additional_metrics(self) -> Dict[str, Any]:
        """获取模拟退火特定的额外指标"""
        total_attempts = self.acceptance_count + self.rejection_count
        acceptance_rate = self.acceptance_count / total_attempts if total_attempts > 0 else 0
        
        return {
            "initial_temperature": self.initial_temperature,
            "final_temperature": self.final_temperature,
            "current_temperature": self.current_temperature,
            "cooling_rate": self.cooling_rate,
            "temperature_length": self.temperature_length,
            "acceptance_count": self.acceptance_count,
            "rejection_count": self.rejection_count,
            "improvement_count": self.improvement_count,
            "acceptance_rate": acceptance_rate,
            "total_temperature_cycles": int((math.log(self.final_temperature/self.initial_temperature) / 
                                           math.log(self.cooling_rate)) if self.cooling_rate < 1 else 0)
        }
</file>

<file path="src/analysis/__init__.py">
# src/analysis/__init__.py
"""Analysis module for calculating travel times and other graph metrics."""
from .travel_time import calculate_room_travel_times

__all__ = ["calculate_room_travel_times"]
</file>

<file path="src/comparison/__init__.py">
"""
对比分析模块 - 用于算法结果对比和可视化
"""
</file>

<file path="src/comparison/results_comparator.py">
"""
结果对比分析器 - 提供详细的算法对比分析和可视化功能
"""

import logging
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from pathlib import Path
from typing import Dict, List, Optional, Any
import json
import seaborn as sns
from datetime import datetime

from src.algorithms.base_optimizer import OptimizationResult

logger = logging.getLogger(__name__)


class ResultsComparator:
    """
    结果对比分析器
    
    提供算法结果的详细对比分析，包括统计分析、可视化图表、
    性能指标计算和报告生成功能。
    """
    
    def __init__(self, results: Dict[str, OptimizationResult]):
        """
        初始化结果对比分析器
        
        Args:
            results: 算法名到结果的映射
        """
        self.results = results
        self.algorithm_names = list(results.keys())
        self.comparison_df = None
        
        logger.info(f"结果对比分析器初始化完成，包含 {len(results)} 个算法结果")
    
    def generate_comparison_table(self) -> pd.DataFrame:
        """
        生成详细的对比表格
        
        Returns:
            pd.DataFrame: 对比表格
        """
        comparison_data = []
        
        for algorithm_name, result in self.results.items():
            # 基础指标
            row = {
                'Algorithm': result.algorithm_name,
                'Best_Cost': result.best_cost,
                'Runtime_Sec': result.execution_time,
                'Iterations': result.iterations,
                'Iter_Per_Sec': result.iterations / result.execution_time if result.execution_time > 0 else 0,
                'Improvement_Rate': self._calculate_improvement_rate(result),
                'Convergence_Stability': self._calculate_convergence_stability(result),
                'Search_Efficiency': self._calculate_search_efficiency(result)
            }
            
            # 算法特定指标
            metrics = result.additional_metrics
            if algorithm_name == 'simulated_annealing':
                row.update({
                    'SA_Acceptance_Rate': metrics.get('acceptance_rate', 0) * 100,
                    'SA_Improvement_Count': metrics.get('improvement_count', 0),
                    'SA_Initial_Temp': metrics.get('initial_temperature', 0),
                    'SA_Final_Temp': metrics.get('current_temperature', 0)
                })
            elif algorithm_name == 'genetic_algorithm':
                row.update({
                    'GA_Final_Generation': metrics.get('final_generation', 0),
                    'GA_Population_Diversity': metrics.get('population_diversity', 0),
                    'GA_Convergence_Rate': metrics.get('convergence_rate', 0) * 100,
                    'GA_Stagnation_Count': metrics.get('stagnation_count', 0)
                })
            elif algorithm_name == 'ppo':
                row.update({
                    'PPO_Training_Steps': metrics.get('total_timesteps', 0),
                    'PPO_Num_Envs': metrics.get('num_envs', 0),
                    'PPO_LR_Schedule': metrics.get('learning_rate_schedule', 'N/A')
                })
            
            comparison_data.append(row)
        
        self.comparison_df = pd.DataFrame(comparison_data)
        
        # 计算排名
        self.comparison_df['Cost_Rank'] = self.comparison_df['Best_Cost'].rank()
        self.comparison_df['Time_Rank'] = self.comparison_df['Runtime_Sec'].rank()
        self.comparison_df['Efficiency_Rank'] = self.comparison_df['Search_Efficiency'].rank(ascending=False)
        
        # 计算综合得分
        self.comparison_df['Composite_Score'] = self._calculate_composite_score()
        
        # 按最优成本排序
        self.comparison_df = self.comparison_df.sort_values('Best_Cost').reset_index(drop=True)
        
        return self.comparison_df
    
    def _calculate_improvement_rate(self, result: OptimizationResult) -> float:
        """计算改进率"""
        if not result.convergence_history or len(result.convergence_history) < 2:
            return 0.0
        
        initial_cost = result.convergence_history[0]
        final_cost = result.best_cost
        
        if initial_cost > 0:
            return ((initial_cost - final_cost) / initial_cost) * 100
        return 0.0
    
    def _calculate_convergence_stability(self, result: OptimizationResult) -> float:
        """计算收敛稳定性"""
        if not result.convergence_history or len(result.convergence_history) < 10:
            return 0.0
        
        # 取最后20%的迭代历史计算稳定性
        history = result.convergence_history
        tail_length = max(10, len(history) // 5)
        tail_history = history[-tail_length:]
        
        if len(tail_history) < 2:
            return 0.0
        
        # 使用变异系数衡量稳定性
        mean_cost = np.mean(tail_history)
        std_cost = np.std(tail_history)
        
        if mean_cost > 0:
            cv = std_cost / mean_cost
            stability = 1.0 / (1.0 + cv)  # 变异系数越小，稳定性越高
            return stability
        
        return 0.0
    
    def _calculate_search_efficiency(self, result: OptimizationResult) -> float:
        """计算搜索效率"""
        if result.execution_time <= 0 or not result.convergence_history:
            return 0.0
        
        improvement = self._calculate_improvement_rate(result)
        time_penalty = 1.0 / (1.0 + np.log10(result.execution_time + 1))
        
        efficiency = (improvement / 100.0) * time_penalty
        return efficiency
    
    def _calculate_composite_score(self) -> pd.Series:
        """计算综合得分"""
        if self.comparison_df is None:
            return pd.Series()
        
        # 归一化各项指标（越小越好的指标需要取倒数）
        cost_norm = 1.0 / (self.comparison_df['Best_Cost'] / self.comparison_df['Best_Cost'].min())
        time_norm = 1.0 / (self.comparison_df['Runtime_Sec'] / self.comparison_df['Runtime_Sec'].min())
        improvement_norm = self.comparison_df['Improvement_Rate'] / self.comparison_df['Improvement_Rate'].max()
        stability_norm = self.comparison_df['Convergence_Stability'] / self.comparison_df['Convergence_Stability'].max()
        efficiency_norm = self.comparison_df['Search_Efficiency'] / self.comparison_df['Search_Efficiency'].max()
        
        # 加权计算综合得分
        weights = {
            'cost': 0.4,      # 成本权重最高
            'time': 0.2,      # 时间权重
            'improvement': 0.2, # 改进率权重
            'stability': 0.1,  # 稳定性权重
            'efficiency': 0.1  # 效率权重
        }
        
        composite_score = (
            weights['cost'] * cost_norm +
            weights['time'] * time_norm + 
            weights['improvement'] * improvement_norm +
            weights['stability'] * stability_norm +
            weights['efficiency'] * efficiency_norm
        )
        
        return composite_score
    
    def create_comparison_plots(self, output_dir: str = "./results/plots"):
        """
        创建对比图表
        
        Args:
            output_dir: 输出目录
        """
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
        
        # 1. 最优成本对比图
        self._plot_cost_comparison(output_path, timestamp)
        
        # 2. 执行时间对比图
        self._plot_time_comparison(output_path, timestamp)
        
        # 3. 收敛曲线图
        self._plot_convergence_curves(output_path, timestamp)
        
        # 4. 综合性能雷达图
        self._plot_performance_radar(output_path, timestamp)
        
        # 5. 算法特性热力图
        self._plot_algorithm_heatmap(output_path, timestamp)
        
        logger.info(f"所有对比图表已保存到: {output_path}")
    
    def _plot_cost_comparison(self, output_path: Path, timestamp: str):
        """绘制成本对比图"""
        plt.figure(figsize=(10, 6))
        
        costs = [result.best_cost for result in self.results.values()]
        colors = plt.cm.Set3(np.linspace(0, 1, len(self.algorithm_names)))
        
        bars = plt.bar(self.algorithm_names, costs, color=colors)
        plt.title('Algorithm Cost Comparison', fontsize=16, fontweight='bold')
        plt.xlabel('Algorithm', fontsize=12)
        plt.ylabel('Optimal Cost', fontsize=12)
        plt.xticks(rotation=45)
        
        # 添加数值标签
        for bar, cost in zip(bars, costs):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(costs)*0.01,
                    f'{cost:.2f}', ha='center', va='bottom', fontweight='bold')
        
        plt.grid(axis='y', alpha=0.3)
        plt.tight_layout()
        plt.savefig(output_path / f"cost_comparison_{timestamp}.png", dpi=300, bbox_inches='tight')
        plt.close()
    
    def _plot_time_comparison(self, output_path: Path, timestamp: str):
        """绘制执行时间对比图"""
        plt.figure(figsize=(10, 6))
        
        times = [result.execution_time for result in self.results.values()]
        colors = plt.cm.Set2(np.linspace(0, 1, len(self.algorithm_names)))
        
        bars = plt.bar(self.algorithm_names, times, color=colors)
        plt.title('Algorithm Runtime Comparison', fontsize=16, fontweight='bold')
        plt.xlabel('Algorithm', fontsize=12)
        plt.ylabel('Runtime (seconds)', fontsize=12)
        plt.xticks(rotation=45)
        plt.yscale('log')  # 使用对数刻度，因为PPO可能时间很长
        
        # 添加数值标签
        for bar, time in zip(bars, times):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() * 1.1,
                    f'{time:.1f}s', ha='center', va='bottom', fontweight='bold')
        
        plt.grid(axis='y', alpha=0.3)
        plt.tight_layout()
        plt.savefig(output_path / f"time_comparison_{timestamp}.png", dpi=300, bbox_inches='tight')
        plt.close()
    
    def _plot_convergence_curves(self, output_path: Path, timestamp: str):
        """绘制收敛曲线图"""
        plt.figure(figsize=(12, 8))
        
        colors = plt.cm.tab10(np.linspace(0, 1, len(self.algorithm_names)))
        
        for i, (algorithm_name, result) in enumerate(self.results.items()):
            if result.convergence_history:
                history = result.convergence_history
                # 对于长度不同的历史，进行采样以便比较
                if len(history) > 1000:
                    indices = np.linspace(0, len(history)-1, 1000, dtype=int)
                    sampled_history = [history[idx] for idx in indices]
                    x_vals = np.linspace(0, len(history), 1000)
                else:
                    sampled_history = history
                    x_vals = range(len(history))
                
                plt.plot(x_vals, sampled_history, label=result.algorithm_name, 
                        color=colors[i], linewidth=2)
        
        plt.title('Algorithm Convergence Curves', fontsize=16, fontweight='bold')
        plt.xlabel('Iterations', fontsize=12)
        plt.ylabel('Cost', fontsize=12)
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.savefig(output_path / f"convergence_curves_{timestamp}.png", dpi=300, bbox_inches='tight')
        plt.close()
    
    def _plot_performance_radar(self, output_path: Path, timestamp: str):
        """绘制性能雷达图"""
        if self.comparison_df is None:
            self.generate_comparison_table()
        
        # 检查DataFrame是否为空
        if self.comparison_df.empty:
            logger.warning("对比数据为空，跳过雷达图生成")
            return
        
        # 选择关键指标
        metrics = ['Best_Cost', 'Runtime_Sec', 'Improvement_Rate', 'Convergence_Stability', 'Search_Efficiency']
        
        # 数据归一化（转换为0-1范围，越大越好）
        normalized_data = {}
        # 使用DataFrame中实际的算法名称而不是self.algorithm_names
        actual_algorithm_names = self.comparison_df['Algorithm'].tolist()
        
        for algorithm_name in actual_algorithm_names:
            try:
                row = self.comparison_df[self.comparison_df['Algorithm'] == algorithm_name].iloc[0]
            except IndexError:
                logger.warning(f"未找到算法 {algorithm_name} 的数据，跳过")
                continue
            
            values = []
            try:
                # 成本：越小越好，取倒数后归一化
                cost_val = 1.0 / row['Best_Cost'] if pd.notna(row['Best_Cost']) and row['Best_Cost'] > 0 else 0
                values.append(cost_val)
                
                # 时间：越小越好，取倒数后归一化
                time_val = 1.0 / row['Runtime_Sec'] if pd.notna(row['Runtime_Sec']) and row['Runtime_Sec'] > 0 else 0
                values.append(time_val)
                
                # 其他指标：越大越好，直接使用
                improvement_rate = row['Improvement_Rate'] / 100.0 if pd.notna(row['Improvement_Rate']) else 0
                convergence = row['Convergence_Stability'] if pd.notna(row['Convergence_Stability']) else 0
                efficiency = row['Search_Efficiency'] if pd.notna(row['Search_Efficiency']) else 0
                
                values.extend([improvement_rate, convergence, efficiency])
                
                normalized_data[algorithm_name] = values
                
            except Exception as e:
                logger.warning(f"处理算法 {algorithm_name} 的雷达图数据时出错: {e}")
                continue
        
        # 检查是否有有效数据
        if not normalized_data:
            logger.warning("没有有效的雷达图数据，跳过雷达图生成")
            return
        
        # 创建雷达图
        fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))
        
        angles = np.linspace(0, 2*np.pi, len(metrics), endpoint=False).tolist()
        angles += angles[:1]  # 闭合图形
        
        colors = plt.cm.Set1(np.linspace(0, 1, len(actual_algorithm_names)))
        
        for i, (algorithm_name, values) in enumerate(normalized_data.items()):
            values += values[:1]  # 闭合图形
            ax.plot(angles, values, 'o-', linewidth=2, label=algorithm_name, color=colors[i])
            ax.fill(angles, values, alpha=0.25, color=colors[i])
        
        ax.set_xticks(angles[:-1])
        ax.set_xticklabels(['Cost', 'Time', 'Improvement', 'Stability', 'Efficiency'])
        ax.set_ylim(0, 1)
        ax.set_title('Algorithm Performance Radar Chart', fontsize=16, fontweight='bold', pad=20)
        ax.legend(loc='upper right', bbox_to_anchor=(1.2, 1.0))
        ax.grid(True)
        
        plt.tight_layout()
        plt.savefig(output_path / f"performance_radar_{timestamp}.png", dpi=300, bbox_inches='tight')
        plt.close()
    
    def _plot_algorithm_heatmap(self, output_path: Path, timestamp: str):
        """绘制算法特性热力图"""
        if self.comparison_df is None:
            self.generate_comparison_table()
        
        # 选择数值列进行热力图展示
        numeric_cols = self.comparison_df.select_dtypes(include=[np.number]).columns
        display_cols = [col for col in numeric_cols if not col.endswith('排名')]
        
        # 准备数据
        heatmap_data = self.comparison_df[['Algorithm'] + display_cols].set_index('Algorithm')
        
        # 数据标准化
        from sklearn.preprocessing import StandardScaler
        scaler = StandardScaler()
        heatmap_data_scaled = pd.DataFrame(
            scaler.fit_transform(heatmap_data),
            index=heatmap_data.index,
            columns=heatmap_data.columns
        )
        
        # 创建热力图
        plt.figure(figsize=(12, 6))
        sns.heatmap(heatmap_data_scaled.T, annot=True, cmap='RdYlBu_r', center=0,
                   fmt='.2f', cbar_kws={'label': '标准化值'})
        plt.title('Algorithm Characteristics Heatmap (Normalized)', fontsize=16, fontweight='bold')
        plt.xlabel('Algorithm', fontsize=12)
        plt.ylabel('Performance Metrics', fontsize=12)
        plt.tight_layout()
        plt.savefig(output_path / f"algorithm_heatmap_{timestamp}.png", dpi=300, bbox_inches='tight')
        plt.close()
    
    def generate_detailed_report(self, output_dir: str = "./results/reports") -> str:
        """
        生成详细的对比分析报告
        
        Args:
            output_dir: 输出目录
            
        Returns:
            str: 报告文件路径
        """
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
        report_path = output_path / f"algorithm_comparison_report_{timestamp}.md"
        
        if self.comparison_df is None:
            self.generate_comparison_table()
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("# 医院布局优化算法对比分析报告\n\n")
            f.write(f"生成时间: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            # 执行摘要
            f.write("## 执行摘要\n\n")
            best_cost_algo = self.comparison_df.iloc[0]['Algorithm']
            best_cost = self.comparison_df.iloc[0]['Best_Cost']
            f.write(f"- **最优成本算法**: {best_cost_algo} (成本: {best_cost:.2f})\n")
            
            fastest_algo = self.comparison_df.loc[self.comparison_df['Runtime_Sec'].idxmin(), 'Algorithm']
            fastest_time = self.comparison_df['Runtime_Sec'].min()
            f.write(f"- **最快算法**: {fastest_algo} (时间: {fastest_time:.2f}秒)\n")
            
            best_efficiency_algo = self.comparison_df.loc[self.comparison_df['Search_Efficiency'].idxmax(), 'Algorithm']
            best_efficiency = self.comparison_df['Search_Efficiency'].max()
            f.write(f"- **最高效率算法**: {best_efficiency_algo} (效率: {best_efficiency:.4f})\n\n")
            
            # 详细对比表格
            f.write("## 详细对比结果\n\n")
            f.write(self.comparison_df.to_markdown(index=False))
            f.write("\n\n")
            
            # 算法分析
            f.write("## 算法详细分析\n\n")
            for algorithm_name, result in self.results.items():
                f.write(f"### {result.algorithm_name}\n\n")
                f.write(f"- **最优成本**: {result.best_cost:.2f}\n")
                f.write(f"- **执行时间**: {result.execution_time:.2f}秒\n")
                f.write(f"- **迭代次数**: {result.iterations}\n")
                
                # 算法特定分析
                metrics = result.additional_metrics
                if algorithm_name == 'simulated_annealing':
                    f.write(f"- **接受率**: {metrics.get('acceptance_rate', 0)*100:.1f}%\n")
                    f.write(f"- **改进次数**: {metrics.get('improvement_count', 0)}\n")
                elif algorithm_name == 'genetic_algorithm':
                    f.write(f"- **最终代数**: {metrics.get('final_generation', 0)}\n")
                    f.write(f"- **种群多样性**: {metrics.get('population_diversity', 0):.3f}\n")
                elif algorithm_name == 'ppo':
                    f.write(f"- **训练步数**: {metrics.get('total_timesteps', 0)}\n")
                    f.write(f"- **环境数量**: {metrics.get('num_envs', 0)}\n")
                
                f.write("\n")
            
            # 结论和建议
            f.write("## 结论和建议\n\n")
            f.write("基于上述分析结果，我们得出以下结论：\n\n")
            
            # 根据结果生成智能建议
            if len(self.results) >= 2:
                cost_range = self.comparison_df['Best_Cost'].max() - self.comparison_df['Best_Cost'].min()
                time_range = self.comparison_df['Runtime_Sec'].max() - self.comparison_df['Runtime_Sec'].min()
                
                if cost_range / self.comparison_df['Best_Cost'].min() > 0.1:
                    f.write("1. **成本差异显著**: 不同算法在优化质量上存在明显差异，建议优先使用成本最低的算法。\n")
                
                if time_range > 60:
                    f.write("2. **时间效率考虑**: 算法执行时间差异较大，在实际应用中需要平衡优化质量和时间成本。\n")
                
                f.write("3. **算法选择建议**:\n")
                f.write(f"   - 追求最优解质量: 推荐使用 **{best_cost_algo}**\n")
                f.write(f"   - 追求快速求解: 推荐使用 **{fastest_algo}**\n")
                f.write(f"   - 平衡质量和效率: 推荐使用 **{best_efficiency_algo}**\n")
        
        logger.info(f"详细对比报告已生成: {report_path}")
        return str(report_path)
    
    def export_layouts_comparison(self, output_dir: str = "./results/layouts"):
        """
        导出所有算法的最优布局进行可视化对比
        
        Args:
            output_dir: 输出目录
        """
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
        
        layouts_data = {}
        for algorithm_name, result in self.results.items():
            layouts_data[algorithm_name] = {
                'layout': result.best_layout,
                'cost': result.best_cost,
                'algorithm': result.algorithm_name
            }
        
        # 保存为JSON文件
        layouts_path = output_path / f"best_layouts_{timestamp}.json"
        with open(layouts_path, 'w', encoding='utf-8') as f:
            json.dump(layouts_data, f, indent=2, ensure_ascii=False)
        
        logger.info(f"最优布局对比数据已保存到: {layouts_path}")
        
        return str(layouts_path)
</file>

<file path="src/core/__init__.py">
"""
核心模块 - 包含网络生成器和算法管理器
"""
</file>

<file path="src/core/algorithm_manager.py">
"""
算法管理器 - 统一管理和运行所有优化算法
"""

import logging
import time
import pandas as pd
from pathlib import Path
from typing import Dict, List, Optional, Any, Type
from concurrent.futures import ThreadPoolExecutor, as_completed

from src.algorithms.base_optimizer import BaseOptimizer, OptimizationResult
from src.algorithms.constraint_manager import ConstraintManager
from src.algorithms.ppo_optimizer import PPOOptimizer
from src.algorithms.simulated_annealing import SimulatedAnnealingOptimizer
from src.algorithms.genetic_algorithm import GeneticAlgorithmOptimizer
from src.rl_optimizer.env.cost_calculator import CostCalculator
from src.rl_optimizer.data.cache_manager import CacheManager
from src.config import RLConfig, NetworkConfig

logger = logging.getLogger(__name__)


class AlgorithmManager:
    """
    算法管理器
    
    统一管理所有优化算法的运行，提供单一接口来执行不同的优化算法，
    支持算法对比、并行执行和结果管理。
    """
    
    def __init__(self, 
                 cost_calculator: CostCalculator,
                 constraint_manager: ConstraintManager,
                 config: RLConfig,
                 cache_manager: CacheManager):
        """
        初始化算法管理器
        
        Args:
            cost_calculator: 成本计算器
            constraint_manager: 约束管理器  
            config: RL配置
            cache_manager: 缓存管理器
        """
        self.cost_calculator = cost_calculator
        self.constraint_manager = constraint_manager
        self.config = config
        self.cache_manager = cache_manager
        
        # 算法注册表
        self.algorithm_registry = {
            'ppo': PPOOptimizer,
            'simulated_annealing': SimulatedAnnealingOptimizer,
            'genetic_algorithm': GeneticAlgorithmOptimizer
        }
        
        # 算法参数配置
        self.algorithm_configs = {
            'ppo': {
                'total_timesteps': config.TOTAL_TIMESTEPS,
            },
            'simulated_annealing': {
                'initial_temperature': 1000.0,
                'final_temperature': 0.1,
                'cooling_rate': 0.95,
                'temperature_length': 100,
                'max_iterations': 10000
            },
            'genetic_algorithm': {
                'population_size': 100,
                'elite_size': 20,
                'mutation_rate': 0.1,
                'crossover_rate': 0.8,
                'tournament_size': 5,
                'max_age': 50,
                'max_iterations': 1000,
                'convergence_threshold': 50
            }
        }
        
        # 结果存储
        self.results = {}
        
        logger.info("算法管理器初始化完成")
        logger.info(f"已注册算法: {list(self.algorithm_registry.keys())}")
    
    def run_single_algorithm(self, 
                           algorithm_name: str,
                           initial_layout: Optional[List[str]] = None,
                           custom_params: Optional[Dict[str, Any]] = None) -> OptimizationResult:
        """
        运行单个算法
        
        Args:
            algorithm_name: 算法名称
            initial_layout: 初始布局
            custom_params: 自定义参数
            
        Returns:
            OptimizationResult: 优化结果
        """
        if algorithm_name not in self.algorithm_registry:
            raise ValueError(f"未知算法: {algorithm_name}. 可用算法: {list(self.algorithm_registry.keys())}")
        
        logger.info(f"开始运行算法: {algorithm_name}")
        
        # 合并参数
        params = self.algorithm_configs[algorithm_name].copy()
        if custom_params:
            params.update(custom_params)
        
        # 创建算法实例（传递自定义参数用于构造函数）
        optimizer = self._create_optimizer(algorithm_name, custom_params)
        
        # 准备运行时参数（移除构造函数参数）
        runtime_params = params.copy()
        if algorithm_name == 'simulated_annealing':
            # 移除SA的构造函数参数，只保留运行时参数
            for key in ['initial_temperature', 'final_temperature', 'cooling_rate', 'temperature_length']:
                runtime_params.pop(key, None)
        elif algorithm_name == 'genetic_algorithm':
            # 移除GA的构造函数参数，只保留运行时参数
            for key in ['population_size', 'elite_size', 'mutation_rate', 'crossover_rate', 'tournament_size', 'max_age']:
                runtime_params.pop(key, None)
        
        # 运行优化
        start_time = time.time()
        try:
            result = optimizer.optimize(initial_layout=initial_layout, **runtime_params)
            result.execution_time = time.time() - start_time
            
            # 存储结果
            self.results[algorithm_name] = result
            
            logger.info(f"算法 {algorithm_name} 完成:")
            logger.info(f"  最优成本: {result.best_cost:.2f}")
            logger.info(f"  执行时间: {result.execution_time:.2f}s")
            logger.info(f"  迭代次数: {result.iterations}")
            
            return result
            
        except Exception as e:
            logger.error(f"算法 {algorithm_name} 执行失败: {e}", exc_info=True)
            raise
    
    def run_multiple_algorithms(self, 
                              algorithm_names: List[str],
                              initial_layout: Optional[List[str]] = None,
                              custom_params: Optional[Dict[str, Dict[str, Any]]] = None,
                              parallel: bool = False) -> Dict[str, OptimizationResult]:
        """
        运行多个算法
        
        Args:
            algorithm_names: 算法名称列表
            initial_layout: 初始布局
            custom_params: 自定义参数字典，键为算法名
            parallel: 是否并行执行
            
        Returns:
            Dict[str, OptimizationResult]: 算法名到结果的映射
        """
        logger.info(f"开始运行多个算法: {algorithm_names}")
        logger.info(f"并行执行: {parallel}")
        
        results = {}
        
        if parallel:
            # 并行执行（注意：PPO可能需要GPU资源，谨慎并行）
            results = self._run_algorithms_parallel(algorithm_names, initial_layout, custom_params)
        else:
            # 串行执行
            for algorithm_name in algorithm_names:
                params = custom_params.get(algorithm_name, {}) if custom_params else {}
                try:
                    result = self.run_single_algorithm(algorithm_name, initial_layout, params)
                    results[algorithm_name] = result
                except Exception as e:
                    logger.error(f"跳过算法 {algorithm_name}，原因: {e}")
        
        self.results.update(results)
        return results
    
    def _run_algorithms_parallel(self, 
                               algorithm_names: List[str],
                               initial_layout: Optional[List[str]],
                               custom_params: Optional[Dict[str, Dict[str, Any]]]) -> Dict[str, OptimizationResult]:
        """并行运行算法"""
        results = {}
        
        with ThreadPoolExecutor(max_workers=min(len(algorithm_names), 3)) as executor:
            # 提交任务
            future_to_algorithm = {}
            for algorithm_name in algorithm_names:
                params = custom_params.get(algorithm_name, {}) if custom_params else {}
                future = executor.submit(self.run_single_algorithm, algorithm_name, initial_layout, params)
                future_to_algorithm[future] = algorithm_name
            
            # 收集结果
            for future in as_completed(future_to_algorithm):
                algorithm_name = future_to_algorithm[future]
                try:
                    result = future.result()
                    results[algorithm_name] = result
                except Exception as e:
                    logger.error(f"并行执行算法 {algorithm_name} 失败: {e}")
        
        return results
    
    def _create_optimizer(self, algorithm_name: str, custom_params: Optional[Dict[str, Any]] = None) -> BaseOptimizer:
        """创建优化器实例"""
        optimizer_class = self.algorithm_registry[algorithm_name]
        
        if algorithm_name == 'ppo':
            return optimizer_class(
                cost_calculator=self.cost_calculator,
                constraint_manager=self.constraint_manager,
                config=self.config,
                cache_manager=self.cache_manager
            )
        elif algorithm_name == 'simulated_annealing':
            # 获取SA特定的构造参数
            sa_params = {}
            if custom_params:
                if 'initial_temperature' in custom_params:
                    sa_params['initial_temperature'] = custom_params['initial_temperature']
                if 'final_temperature' in custom_params:
                    sa_params['final_temperature'] = custom_params['final_temperature']
                if 'cooling_rate' in custom_params:
                    sa_params['cooling_rate'] = custom_params['cooling_rate']
                if 'temperature_length' in custom_params:
                    sa_params['temperature_length'] = custom_params['temperature_length']
            
            return optimizer_class(
                cost_calculator=self.cost_calculator,
                constraint_manager=self.constraint_manager,
                **sa_params
            )
        elif algorithm_name == 'genetic_algorithm':
            # 获取GA特定的构造参数
            ga_params = {}
            if custom_params:
                if 'population_size' in custom_params:
                    ga_params['population_size'] = custom_params['population_size']
                if 'elite_size' in custom_params:
                    ga_params['elite_size'] = custom_params['elite_size']
                if 'mutation_rate' in custom_params:
                    ga_params['mutation_rate'] = custom_params['mutation_rate']
                if 'crossover_rate' in custom_params:
                    ga_params['crossover_rate'] = custom_params['crossover_rate']
                if 'tournament_size' in custom_params:
                    ga_params['tournament_size'] = custom_params['tournament_size']
                if 'max_age' in custom_params:
                    ga_params['max_age'] = custom_params['max_age']
            
            return optimizer_class(
                cost_calculator=self.cost_calculator,
                constraint_manager=self.constraint_manager,
                **ga_params
            )
        else:
            # 默认情况
            return optimizer_class(
                cost_calculator=self.cost_calculator,
                constraint_manager=self.constraint_manager
            )
    
    def get_algorithm_comparison(self) -> pd.DataFrame:
        """
        获取算法对比结果
        
        Returns:
            pd.DataFrame: 对比结果表格
        """
        if not self.results:
            logger.warning("没有算法执行结果可供对比")
            return pd.DataFrame()
        
        comparison_data = []
        
        for algorithm_name, result in self.results.items():
            row = {
                '算法名称': result.algorithm_name,
                '最优成本': result.best_cost,
                '执行时间(秒)': result.execution_time,
                '迭代次数': result.iterations,
                '收敛性': self._calculate_convergence_metric(result),
                '最终改进率(%)': self._calculate_improvement_rate(result)
            }
            
            # 添加算法特定指标
            if algorithm_name == 'simulated_annealing':
                metrics = result.additional_metrics
                row['接受率(%)'] = metrics.get('acceptance_rate', 0) * 100
                row['改进次数'] = metrics.get('improvement_count', 0)
            elif algorithm_name == 'genetic_algorithm':
                metrics = result.additional_metrics
                row['最终代数'] = metrics.get('final_generation', 0)
                row['种群多样性'] = metrics.get('population_diversity', 0)
                row['收敛率(%)'] = metrics.get('convergence_rate', 0) * 100
            elif algorithm_name == 'ppo':
                metrics = result.additional_metrics
                row['训练步数'] = metrics.get('total_timesteps', 0)
                row['环境数量'] = metrics.get('num_envs', 0)
        
            comparison_data.append(row)
        
        df = pd.DataFrame(comparison_data)
        
        # 按最优成本排序
        df = df.sort_values('最优成本')
        df = df.reset_index(drop=True)
        
        return df
    
    def _calculate_convergence_metric(self, result: OptimizationResult) -> float:
        """计算收敛性指标"""
        if not result.convergence_history or len(result.convergence_history) < 2:
            return 0.0
        
        # 计算后期收敛稳定性
        history = result.convergence_history
        if len(history) >= 100:
            # 取最后100次迭代的标准差作为收敛性指标
            recent_history = history[-100:]
            std_dev = pd.Series(recent_history).std()
            return 1.0 / (1.0 + std_dev)  # 标准差越小，收敛性越好
        else:
            # 对于较短的历史，计算总体收敛趋势
            initial_cost = history[0]
            final_cost = history[-1]
            if initial_cost > 0:
                return (initial_cost - final_cost) / initial_cost
            return 0.0
    
    def _calculate_improvement_rate(self, result: OptimizationResult) -> float:
        """计算改进率"""
        if not result.convergence_history or len(result.convergence_history) < 2:
            return 0.0
        
        initial_cost = result.convergence_history[0]
        final_cost = result.best_cost
        
        if initial_cost > 0:
            return ((initial_cost - final_cost) / initial_cost) * 100
        return 0.0
    
    def save_results(self, output_dir: str = "./results/comparison"):
        """
        保存算法对比结果
        
        Args:
            output_dir: 输出目录
        """
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        timestamp = time.strftime("%Y%m%d-%H%M%S")
        
        # 保存对比表格
        comparison_df = self.get_algorithm_comparison()
        if not comparison_df.empty:
            csv_path = output_path / f"algorithm_comparison_{timestamp}.csv"
            comparison_df.to_csv(csv_path, index=False, encoding='utf-8-sig')
            logger.info(f"算法对比结果已保存到: {csv_path}")
        
        # 保存详细结果
        for algorithm_name, result in self.results.items():
            result_dict = {
                'algorithm_name': result.algorithm_name,
                'best_cost': result.best_cost,
                'execution_time': result.execution_time,
                'iterations': result.iterations,
                'best_layout': result.best_layout,
                'convergence_history': result.convergence_history,
                'additional_metrics': result.additional_metrics
            }
            
            import json
            json_path = output_path / f"{algorithm_name}_result_{timestamp}.json"
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(result_dict, f, indent=2, ensure_ascii=False)
            
            logger.info(f"算法 {algorithm_name} 详细结果已保存到: {json_path}")
    
    def get_best_result(self) -> Optional[OptimizationResult]:
        """
        获取最佳结果
        
        Returns:
            Optional[OptimizationResult]: 最佳优化结果
        """
        if not self.results:
            return None
        
        best_result = min(self.results.values(), key=lambda x: x.best_cost)
        return best_result
    
    def clear_results(self):
        """清除所有结果"""
        self.results.clear()
        logger.info("已清除所有算法结果")
    
    def get_algorithm_names(self) -> List[str]:
        """获取所有可用的算法名称"""
        return list(self.algorithm_registry.keys())
    
    def register_algorithm(self, name: str, optimizer_class: Type[BaseOptimizer], config: Dict[str, Any]):
        """
        注册新算法
        
        Args:
            name: 算法名称
            optimizer_class: 优化器类
            config: 算法配置
        """
        self.algorithm_registry[name] = optimizer_class
        self.algorithm_configs[name] = config
        logger.info(f"已注册新算法: {name}")
    
    def update_algorithm_config(self, algorithm_name: str, config: Dict[str, Any]):
        """
        更新算法配置
        
        Args:
            algorithm_name: 算法名称
            config: 新配置
        """
        if algorithm_name in self.algorithm_configs:
            self.algorithm_configs[algorithm_name].update(config)
            logger.info(f"已更新算法 {algorithm_name} 的配置")
        else:
            logger.warning(f"算法 {algorithm_name} 不存在，无法更新配置")
</file>

<file path="src/core/network_generator.py">
"""
网络生成器 - 整合医院网络生成功能
"""

import logging
import pathlib
from typing import Dict, List, Optional

from src.config import NetworkConfig, COLOR_MAP
from src.network.super_network import SuperNetwork
from src.plotting.plotter import PlotlyPlotter
from src.analysis.travel_time import calculate_room_travel_times

logger = logging.getLogger(__name__)


class NetworkGenerator:
    """
    网络生成器类
    
    整合原有的医院网络生成功能，从楼层平面图生成多层医院网络图，
    包括图像处理、网络构建和行程时间计算。
    """
    
    def __init__(self, config: Optional[NetworkConfig] = None):
        """
        初始化网络生成器
        
        Args:
            config: 网络配置，如果为None则使用默认配置
        """
        self.config = config if config is not None else NetworkConfig(color_map_data=COLOR_MAP)
        self.color_map_data = COLOR_MAP
        self.super_network = None
        self.super_graph = None
        
        logger.info("网络生成器初始化完成")
        logger.info(f"结果将保存在: {self.config.RESULT_PATH}")
        logger.info(f"调试图像将保存在: {self.config.DEBUG_PATH}")
    
    def generate_network(self, 
                        image_dir: str = "./data/label/",
                        base_floor: int = -1,
                        num_processes: Optional[int] = None) -> bool:
        """
        生成多层医院网络
        
        Args:
            image_dir: 楼层标注图像目录
            base_floor: 基准楼层
            num_processes: 并行处理进程数
            
        Returns:
            bool: 生成是否成功
        """
        logger.info("=== 开始生成多层医院网络 ===")
        
        # 检查图像目录
        label_dir = pathlib.Path(image_dir)
        if not label_dir.is_dir():
            logger.error(f"标注图像目录不存在: {label_dir}")
            return False
        
        # 收集图像文件
        image_file_paths = [
            str(p) for p in sorted(label_dir.glob('*.png')) if p.is_file()
        ]
        
        if not image_file_paths:
            logger.warning(f"在 {label_dir} 中未找到图像文件")
            return False
        
        logger.info(f"找到 {len(image_file_paths)} 个图像文件: {image_file_paths}")
        
        try:
            # 初始化SuperNetwork构建器
            self.super_network = SuperNetwork(
                config=self.config,
                color_map_data=self.color_map_data,
                base_floor=base_floor,
                num_processes=num_processes
            )
            
            # 生成SuperNetwork
            self.super_graph = self.super_network.run(
                image_file_paths=image_file_paths
            )
            
            logger.info(f"SuperNetwork生成成功:")
            logger.info(f"  节点数: {self.super_graph.number_of_nodes()}")
            logger.info(f"  边数: {self.super_graph.number_of_edges()}")
            logger.info(f"  图像尺寸: 宽度={self.super_network.width}, 高度={self.super_network.height}")
            
            return True
            
        except Exception as e:
            logger.error(f"生成网络时发生错误: {e}", exc_info=True)
            return False
    
    def visualize_network(self, output_filename: str = "hospital_network_3d.html") -> bool:
        """
        可视化生成的网络
        
        Args:
            output_filename: 输出文件名
            
        Returns:
            bool: 可视化是否成功
        """
        if self.super_graph is None or self.super_network is None:
            logger.error("未生成网络，无法进行可视化")
            return False
        
        try:
            logger.info("正在生成网络可视化...")
            
            plotter = PlotlyPlotter(
                config=self.config, 
                color_map_data=self.color_map_data
            )
            
            plot_output_path = self.config.RESULT_PATH / output_filename
            plotter.plot(
                graph=self.super_graph,
                output_path=plot_output_path,
                title="多层医院网络",
                graph_width=self.super_network.width,
                graph_height=self.super_network.height,
                floor_z_map=self.super_network.floor_z_map
            )
            
            logger.info(f"网络可视化已保存到: {plot_output_path}")
            return True
            
        except Exception as e:
            logger.error(f"网络可视化时发生错误: {e}", exc_info=True)
            return False
    
    def calculate_travel_times(self, output_filename: str = "hospital_travel_times.csv") -> bool:
        """
        计算并保存房间间行程时间
        
        Args:
            output_filename: 输出文件名
            
        Returns:
            bool: 计算是否成功
        """
        if self.super_graph is None or self.super_network is None:
            logger.error("未生成网络，无法计算行程时间")
            return False
        
        try:
            logger.info("正在计算房间间行程时间...")
            
            # 获取地面楼层Z值用于过滤
            ground_floor_z = self.super_network.designated_ground_floor_z
            if ground_floor_z is not None:
                logger.info(f"使用地面楼层 Z={ground_floor_z:.2f} 进行外部区域过滤")
            else:
                logger.warning("无法确定地面楼层Z值，外部区域过滤可能受影响")
            
            # 计算并保存行程时间
            travel_times_output_dir = self.config.RESULT_PATH
            calculate_room_travel_times(
                graph=self.super_graph,
                config=self.config,
                output_dir=travel_times_output_dir,
                output_filename=output_filename,
                ground_floor_z=ground_floor_z
            )
            
            travel_times_path = travel_times_output_dir / output_filename
            logger.info(f"行程时间矩阵已保存到: {travel_times_path}")
            return True
            
        except Exception as e:
            logger.error(f"计算行程时间时发生错误: {e}", exc_info=True)
            return False
    
    def get_network_info(self) -> Dict[str, any]:
        """
        获取网络信息
        
        Returns:
            Dict: 网络信息字典
        """
        if self.super_graph is None or self.super_network is None:
            return {}
        
        return {
            'nodes_count': self.super_graph.number_of_nodes(),
            'edges_count': self.super_graph.number_of_edges(),
            'width': self.super_network.width,
            'height': self.super_network.height,
            'floor_z_map': self.super_network.floor_z_map,
            'ground_floor_z': self.super_network.designated_ground_floor_z
        }
    
    def run_complete_generation(self, 
                               image_dir: str = "./data/label/",
                               visualization_filename: str = "hospital_network_3d.html",
                               travel_times_filename: str = "hospital_travel_times.csv") -> bool:
        """
        运行完整的网络生成流程
        
        Args:
            image_dir: 楼层标注图像目录
            visualization_filename: 可视化输出文件名
            travel_times_filename: 行程时间输出文件名
            
        Returns:
            bool: 完整流程是否成功
        """
        logger.info("=== 开始完整网络生成流程 ===")
        
        # 1. 生成网络
        if not self.generate_network(image_dir):
            logger.error("网络生成失败，中止流程")
            return False
        
        # 2. 可视化网络
        if not self.visualize_network(visualization_filename):
            logger.error("网络可视化失败，但继续后续步骤")
        
        # 3. 计算行程时间
        if not self.calculate_travel_times(travel_times_filename):
            logger.error("行程时间计算失败，但网络生成成功")
            return False
        
        logger.info("=== 完整网络生成流程成功完成 ===")
        network_info = self.get_network_info()
        logger.info(f"网络统计信息: {network_info}")
        
        return True
</file>

<file path="src/graph/graph_manager.py">
"""Manages the graph structure, node storage, and ID generation."""

import itertools
import networkx as nx
import logging
from typing import Dict, Optional, Iterator

from .node import Node

logger = logging.getLogger(__name__)

class GraphManager:
    """
    Manages a networkx graph, stores nodes by ID, and generates unique node IDs.

    Attributes:
        graph (nx.Graph): The underlying networkx graph object.
    """

    def __init__(self, id_generator_start_value: int = 1):
        """
        Initializes the GraphManager.

        Args:
            id_generator_start_value: The starting value for the node ID generator.
                                      This is crucial for multi-processing to ensure
                                      unique IDs across different Network instances.
        """
        self.graph: nx.Graph = nx.Graph()
        self._id_to_node_map: Dict[int, Node] = {}
        # Using itertools.count for a thread-safe (in CPython due to GIL)
        # and efficient ID generator. For true multiprocessing, the start
        # value must be managed externally to ensure uniqueness.
        self._node_id_generator: Iterator[int] = itertools.count(
            id_generator_start_value)
        self._last_generated_id: int = id_generator_start_value - 1

    def generate_node_id(self) -> int:
        """
        Generates and returns a new unique node ID.

        Returns:
            A unique integer ID for a new node.
        """
        new_id = next(self._node_id_generator)
        self._last_generated_id = new_id
        return new_id

    def add_node(self, node: Node) -> None:
        """
        Adds a node to the graph and the ID-to-node map.

        Args:
            node: The Node object to add.

        Raises:
            ValueError: If a node with the same ID already exists.
        """
        if node.id in self._id_to_node_map:
            logger.error(
                f"Node with ID {node.id} already exists in this graph manager.")
        if self.graph.has_node(node):  # Should be redundant if ID is unique
            logger.error(
                f"Node object {node} (ID: {node.id}) already exists in the graph.")

        self.graph.add_node(node, type=node.node_type, pos=node.pos,
                            time=node.time, door_type=node.door_type)
        self._id_to_node_map[node.id] = node

    def get_node_by_id(self, node_id: int) -> Optional[Node]:
        """
        Retrieves a node by its ID.

        Args:
            node_id: The ID of the node to retrieve.

        Returns:
            The Node object if found, otherwise None.
        """
        return self._id_to_node_map.get(node_id)

    def connect_nodes_by_ids(self, node_id1: int, node_id2: int, **edge_attributes) -> bool:
        """
        Connects two nodes in the graph using their IDs.

        Args:
            node_id1: The ID of the first node.
            node_id2: The ID of the second node.
            **edge_attributes: Additional attributes for the edge.

        Returns:
            True if the connection was successful, False if one or both nodes
            were not found, or if they are the same node.
        """
        if node_id1 == node_id2:
            logger.warning(
                f"Warning: Attempted to connect node ID {node_id1} to itself. Skipping.")
            return False

        node1 = self.get_node_by_id(node_id1)
        node2 = self.get_node_by_id(node_id2)

        if node1 and node2:
            if not self.graph.has_edge(node1, node2):
                self.graph.add_edge(node1, node2, **edge_attributes)
            return True
        else:
            missing_ids = []
            if not node1:
                missing_ids.append(node_id1)
            if not node2:
                missing_ids.append(node_id2)
            logger.warning(
                f"Warning: Could not connect nodes. Missing node IDs: {missing_ids}")
            return False
        
    def get_all_nodes(self) -> list[Node]:
        """Returns a list of all Node objects in the graph."""
        return list(self._id_to_node_map.values())
    
    def get_graph_copy(self) -> nx.Graph:
        """
        Returns a deep copy of the internal networkx graph.
        This is important if the graph is to be modified externally
        without affecting the manager's internal state, or for passing
        to other processes.
        """
        return self.graph.copy() # networkx.Graph.copy() is a deep copy by default for node/edge attributes
    
    def get_next_available_node_id_estimate(self) -> int:
        """
        Returns an estimate of the next node ID that would be generated.
        This is primarily for `SuperNetwork` to estimate ID blocks for workers.
        It's `_last_generated_id + 1`.
        """
        return self._last_generated_id + 1
    
    def clear(self, id_generator_start_value: int = 1):
        """
        Clears the graph and resets the ID generator.
        Useful for reusing the manager instance.
        """
        self.graph.clear()
        self._id_to_node_map.clear()
        self._node_id_generator = itertools.count(id_generator_start_value)
        self._last_generated_id = id_generator_start_value - 1
</file>

<file path="src/graph/node.py">
"""Defines the Node class for the network graph."""

from typing import Tuple, Optional

class Node:
    """
    Represents a node in the network graph.

    Attributes:
        id (int): A unique identifier for the node.
        node_type (str): The type of the node (e.g., 'Room', 'Door', 'Corridor').
        pos (Tuple[int, int, int]): The (x, y, z) coordinates of the node.
        time (float): The time cost associated with traversing this node.
        door_type (Optional[str]): Specifies the type of door connection
            (e.g., 'room', 'in', 'out'), if the node is a door.
            Defaults to None.
        area (float): The area occupied by the node in pixel units.
                      For point-like nodes (e.g., mesh centroids), this might be
                      an estimated representative area or a standard small value.
    """
    def __init__(self, node_id: int, node_type: str, pos: Tuple[int, int, int],
                 default_time: float, area: float = 1.0): # Default area to 1 pixel if not specified
        """
        Initializes a Node object.

        Args:
            node_id: The unique identifier for the node.
            node_type: The type of the node.
            pos: The (x, y, z) coordinates of the node.
            default_time: The default time cost for this node type.
            area: The area of the node in pixel units.
        """
        self.id: int = node_id
        self.node_type: str = node_type
        self.pos: Tuple[int, int, int] = pos
        self.time: float = default_time
        self.door_type: Optional[str] = None
        self.area: float = area

    def __repr__(self) -> str:
        """Returns a string representation of the Node."""
        return (f"Node(id={self.id}, type='{self.node_type}', pos={self.pos}, "
                f"time={self.time:.2f}, area={self.area:.2f}, door_type='{self.door_type}')")
    
    def __hash__(self) -> int:
        """Returns the hash of the node based on its ID."""
        return hash(self.id)
    
    def __eq__(self, other: object) -> bool:
        """Checks equality with another Node based on ID."""
        if isinstance(other, Node):
            return self.id == other.id
        return False
</file>

<file path="src/image_processing/processor.py">
"""Handles image loading, preprocessing, and basic morphological operations."""

import cv2
import logging
import numpy as np
from PIL import Image
from scipy.spatial import KDTree
from typing import Tuple, Dict, Any, Optional

from src.config import NetworkConfig

logger = logging.getLogger(__name__)


class ImageProcessor:
    """
    Provides functionalities for image loading, color quantization,
    and morphological operations.
    """

    def __init__(self, config: NetworkConfig, color_map_data: Dict[Tuple[int, int, int], Dict[str, Any]]):
        """
        Initializes the ImageProcessor.

        Args:
            config: The NetworkConfig object.
            color_map_data: The color map dictionary.
        """
        self.config = config
        self.color_map_data = color_map_data
        self._current_image_data: Optional[np.ndarray] = None
        self._image_height: Optional[int] = None
        self._image_width: Optional[int] = None

    def load_and_prepare_image(self, image_path: str) -> np.ndarray:
        """
        Loads an image, rotates it, and stores its dimensions.

        The processed image is stored internally and also returned.

        Args:
            image_path: Path to the image file.

        Returns:
            A NumPy array representing the processed image (RGB).

        Raises:
            FileNotFoundError: If the image_path does not exist.
            IOError: If the image cannot be opened or read.
        """
        try:
            img = Image.open(image_path).convert('RGB')  # Ensure RGB
            if self.config.IMAGE_ROTATE != 0:
                img = img.rotate(self.config.IMAGE_ROTATE)
        except FileNotFoundError:
            raise FileNotFoundError(f"Image file not found: {image_path}")
        except IOError:
            raise IOError(f"Could not open or read image file: {image_path}")

        self._current_image_data = np.asarray(img, dtype=np.uint8)
        if self._current_image_data is None:
            raise ValueError(
                f"Failed to convert image to numpy array: {image_path}")

        self._image_height, self._image_width = self._current_image_data.shape[:2]
        return self._current_image_data.copy()  # Return a copy

    def get_image_dimensions(self) -> Tuple[int, int]:
        """
        Returns the dimensions of the last loaded image.

        Returns:
            A tuple (height, width).

        Raises:
            ValueError: If no image has been loaded yet.
        """
        if self._image_height is None or self._image_width is None:
            raise ValueError(
                "Image not loaded. Call load_and_prepare_image() first.")
        return self._image_height, self._image_width

    def quantize_colors(self, image_data: np.ndarray) -> np.ndarray:
        """
        Replaces each pixel's color in the image with the nearest color
        from the provided color_map using a KDTree for efficiency.

        Args:
            image_data: A NumPy array representing the image (H, W, 3) in RGB.

        Returns:
            A NumPy array of the same shape with colors replaced.
        """
        if not self.color_map_data:
            # If color map is empty, return original image to avoid errors
            logger.warning(
                "Warning: Color map is empty. Returning original image from quantize_colors.")
            return image_data.copy()

        pixels = image_data.reshape(-1, 3)
        map_colors_rgb = list(self.color_map_data.keys())

        if not map_colors_rgb:  # Should not happen if self.color_map_data is not empty
            logger.warning(
                "Warning: No colors in color_map_data keys. Returning original image.")
            return image_data.copy()

        kdtree = KDTree(map_colors_rgb)
        _, closest_indices = kdtree.query(pixels)

        new_pixels = np.array(map_colors_rgb, dtype=np.uint8)[closest_indices]
        new_image = new_pixels.reshape(image_data.shape).astype(np.uint8)
        return new_image

    def apply_morphology(self, mask: np.ndarray, operation: str = 'close_open',
                         kernel_size: Optional[Tuple[int, int]] = None) -> np.ndarray:
        """
        Applies morphological operations to a binary mask.

        Args:
            mask: The input binary mask (NumPy array, dtype=uint8).
            operation: The type of operation.
                       'close_open': MORPH_CLOSE then MORPH_OPEN (default)
                       'open': MORPH_OPEN
                       'close': MORPH_CLOSE
                       'dilate': MORPH_DILATE
                       'erode': MORPH_ERODE
            kernel_size: Tuple (k_height, k_width) for the morphological kernel.
                         Defaults to `config.MORPHOLOGY_KERNEL_SIZE`.

        Returns:
            The processed binary mask.
        """
        if kernel_size is None:
            k_size = self.config.MORPHOLOGY_KERNEL_SIZE
        else:
            k_size = kernel_size

        kernel = np.ones(k_size, np.uint8)
        processed_mask = mask.copy()

        if operation == 'close_open':
            processed_mask = cv2.morphologyEx(
                processed_mask, cv2.MORPH_CLOSE, kernel)
            processed_mask = cv2.morphologyEx(
                processed_mask, cv2.MORPH_OPEN, kernel)
        elif operation == 'open':
            processed_mask = cv2.morphologyEx(
                processed_mask, cv2.MORPH_OPEN, kernel)
        elif operation == 'close':
            processed_mask = cv2.morphologyEx(
                processed_mask, cv2.MORPH_CLOSE, kernel)
        elif operation == 'dilate':
            processed_mask = cv2.dilate(processed_mask, kernel, iterations=1)
        elif operation == 'erode':
            processed_mask = cv2.erode(processed_mask, kernel, iterations=1)
        else:
            raise ValueError(
                f"Unsupported morphological operation: {operation}")

        return processed_mask
    

class DebugImage:
    """Helper class for saving and displaying debug images."""
    count = 0

    def __init__(self, image_data: np.ndarray, save: bool = False,
                 show_napari: bool = False, suffix: str = '',
                 config: NetworkConfig = NetworkConfig()):
        """
        Initializes DebugImage.

        Args:
            image_data: NumPy array of the image to debug.
            save: If True, saves the image.
            show_napari: If True, shows the image using napari (requires napari installed).
            suffix: Suffix for the saved filename.
            debug_path_base: Base directory for saving debug images.
        """
        self.image_to_debug = image_data.copy() # Work with a copy
        self.debug_path = config.DEBUG_PATH
        self.debug_path.mkdir(parents=True, exist_ok=True)

        if save:
            self._save_image(suffix)
        if show_napari:
            self._show_with_napari()

    def _save_image(self, suffix: str = ''):
        """Saves the debug image."""
        filename = f'debug_{DebugImage.count}_{suffix}.png'
        save_path = self.debug_path / filename
        try:
            # Convert to BGR if it's RGB for Pillow saving, or handle grayscale
            if self.image_to_debug.ndim == 3 and self.image_to_debug.shape[2] == 3:
                # Assume RGB from PIL, convert to BGR for OpenCV-style saving or save as is with PIL
                img_to_save_pil = Image.fromarray(self.image_to_debug)
            elif self.image_to_debug.ndim == 2: # Grayscale
                 img_to_save_pil = Image.fromarray(self.image_to_debug, mode='L')
            else:
                logger.warning(f"Warning: Unsupported image format for saving: {self.image_to_debug.shape}")
                return

            img_to_save_pil.save(save_path)
            DebugImage.count += 1
            logger.info(f"Debug image saved to {save_path}")
        except Exception as e:
            logger.error(f"Error saving debug image {save_path}: {e}")


    def _show_with_napari(self):
        """Shows the image using napari."""
        try:
            import napari
            viewer = napari.Viewer()
            viewer.add_image(self.image_to_debug)
            napari.run()
        except ImportError:
            logger.warning("Napari is not installed. Skipping napari display.")
        except Exception as e:
            logger.error(f"Error showing image with napari: {e}")
</file>

<file path="src/network/floor_manager.py">
"""
Manages floor detection from filenames and Z-level calculations for SuperNetwork.
"""

import os
import re
import pathlib
from typing import List, Dict, Tuple, Optional
import logging

logger = logging.getLogger(__name__)

class FloorManager:
    """
    Handles detection of floor numbers from image filenames and calculation
    of their corresponding Z-coordinate levels.
    """

    def __init__(self, base_floor_default: int = 0, default_floor_height: float = 10.0):
        """
        Initializes the FloorManager.

        Args:
            base_floor_default: The default starting floor number if none can be detected.
            default_floor_height: The default height difference between adjacent floors.
        """
        self.base_floor_default = base_floor_default
        self.default_floor_height = default_floor_height
        self._floor_patterns = {
            # Order matters: more specific patterns first
            # B-1, B1, b-1, b1 -> negative
            r'([Bb])-?(\d+)': lambda m: -int(m.group(2)),
            # -1F, -2f -> negative
            r'-(\d+)[Ff]': lambda m: -int(m.group(1)),
            r'([Ll])(\d+)': lambda m: int(m.group(2)),    # L1, l2 -> positive
            r'(\d+)[Ff]': lambda m: int(m.group(1)),      # 1F, 2f -> positive
            # Add more general patterns if needed, like just a number if no prefix/suffix
            # r'_(\d+)_': lambda m: int(m.group(1)) # Example: floor_1_plan.png
        }

    def detect_floor_from_filename(self, file_path: pathlib.Path) -> Optional[int]:
        """
        Detects the floor number from a filename.

        Args:
            file_path: Path object of the image file.

        Returns:
            The detected floor number (integer) or None if not detected.
        """
        filename = file_path.stem  # Get filename without extension
        for pattern, converter in self._floor_patterns.items():
            match = re.search(pattern, filename)
            if match:
                try:
                    return converter(match)
                except ValueError:
                    continue  # Conversion failed, try next pattern
        return None

    def auto_assign_floors(self, image_paths: List[pathlib.Path]) \
            -> Tuple[Dict[pathlib.Path, int], Dict[int, pathlib.Path]]:
        """
        Assigns floor numbers to a list of image paths.

        Attempts to detect from filenames first. If unsuccessful for some or all,
        assigns sequentially based on sort order or a defined starting floor.

        Args:
            image_paths: A list of Path objects for the images.

        Returns:
            A tuple containing:
                - path_to_floor_map (Dict[pathlib.Path, int]): Maps image path to floor number.
                - floor_to_path_map (Dict[int, pathlib.Path]): Maps floor number to image path.
                                                               (Assumes one image per floor for this map)
        """
        path_to_floor_map: Dict[pathlib.Path, int] = {}
        detected_floors: Dict[pathlib.Path, int] = {}
        undetected_paths: List[pathlib.Path] = []

        for p_path in image_paths:
            floor = self.detect_floor_from_filename(p_path)
            if floor is not None:
                if floor in path_to_floor_map.values():
                    # Handle duplicate floor detection if necessary (e.g., error or rename)
                    # For now, we might overwrite or just take the first one.
                    # Let's assume for now floor numbers detected are unique or take the first.
                    # A more robust solution would collect all paths per floor number.
                    logger.warning(
                        f"Warning: Duplicate floor number {floor} detected. Check filenames.")
                detected_floors[p_path] = floor
            else:
                undetected_paths.append(p_path)

        path_to_floor_map.update(detected_floors)

        # Assign floors to undetected paths sequentially
        if undetected_paths:
            # Sort undetected paths to ensure consistent assignment order
            # (e.g., alphabetically or by modification time if relevant)
            undetected_paths.sort()

            # Determine starting floor for sequential assignment
            if detected_floors:
                # Start from one above the highest detected floor, or one below the lowest if all are negative
                all_detected_nos = list(detected_floors.values())
                if all(f < 0 for f in all_detected_nos):  # if all are basement floors
                    start_floor = min(all_detected_nos) - 1 if min(all_detected_nos) - \
                        1 not in all_detected_nos else max(all_detected_nos) + 1
                else:
                    start_floor = max(all_detected_nos) + 1

                # Ensure start_floor is not already taken
                while start_floor in path_to_floor_map.values():
                    start_floor += 1  # simple increment, could be smarter
            else:
                start_floor = self.base_floor_default

            for i, p_path in enumerate(undetected_paths):
                current_assigned_floor = start_floor + i
                while current_assigned_floor in path_to_floor_map.values():  # Avoid collision
                    current_assigned_floor += 1
                path_to_floor_map[p_path] = current_assigned_floor

        # Create the reverse map (floor_to_path_map)
        # This assumes one unique image per floor for this specific map.
        # If multiple images could correspond to the same floor, this needs adjustment.
        floor_to_path_map: Dict[int, pathlib.Path] = {
            v: k for k, v in path_to_floor_map.items()}

        # Verify uniqueness for floor_to_path_map
        if len(floor_to_path_map) != len(path_to_floor_map):
            logger.warning("Non-unique floor numbers assigned or detected, "
                  "floor_to_path_map may not represent all images.")
            # Potentially rebuild floor_to_path_map to store List[pathlib.Path] per floor
            # For now, this structure is kept simple as per original design.

        return path_to_floor_map, floor_to_path_map

    def calculate_z_levels(self, floor_to_path_map: Dict[int, pathlib.Path]) \
            -> Dict[int, float]:
        """
        Calculates the Z-coordinate for each floor.

        Args:
            floor_to_path_map: A map from floor number to image path (used to get sorted floors).

        Returns:
            A dictionary mapping floor number to its Z-coordinate.
        """
        if not floor_to_path_map:
            return {}

        sorted_floor_numbers = sorted(floor_to_path_map.keys())

        # Simple Z level calculation: floor_number * default_floor_height
        # This assumes a consistent floor height and that floor numbers represent relative positions.
        # For example, Floor 0 is at Z=0, Floor 1 at Z=10, Floor -1 at Z=-10.
        z_levels: Dict[int, float] = {
            floor_num: float(floor_num * self.default_floor_height)
            for floor_num in sorted_floor_numbers
        }
        return z_levels
</file>

<file path="src/network/node_creators.py">
"""
Defines strategies for creating different types of nodes in the network.

This module uses the Strategy design pattern where each node type (Room,
Connection, Pedestrian, etc.) has its own creator class derived from a
base class.
"""

import abc
import cv2
import numpy as np
import logging
from scipy.spatial import KDTree
from typing import Dict, Tuple, List, Any, Optional

from src.config import NetworkConfig
from src.graph.node import Node
from src.graph.graph_manager import GraphManager
from src.image_processing.processor import ImageProcessor

logger = logging.getLogger(__name__)


class BaseNodeCreator(abc.ABC):
    """
    Abstract base class for node creators.
    """

    def __init__(self,
                 config: NetworkConfig,
                 color_map_data: Dict[Tuple[int, int, int], Dict[str, Any]],
                 image_processor: ImageProcessor,
                 graph_manager: GraphManager):
        self.config = config
        self.color_map_data = color_map_data
        self.image_processor = image_processor
        self.graph_manager = graph_manager
        self.types_map_name_to_rgb: Dict[str, Tuple[int, int, int]] = \
            {details['name']: rgb for rgb, details in color_map_data.items()}
        self.types_map_name_to_time: Dict[str, float] = \
            {details['name']: details.get('time', 1.0)
             for rgb, details in color_map_data.items()}

    def _get_color_rgb_by_name(self, type_name: str) -> Optional[Tuple[int, int, int]]:
        return self.types_map_name_to_rgb.get(type_name)

    def _get_time_by_name(self, type_name: str) -> float:
        return self.types_map_name_to_time.get(type_name, self.config.PEDESTRIAN_TIME)

    def _create_mask_for_type(self,
                              image_data: np.ndarray,
                              target_type_name: str,
                              apply_morphology: bool = True,
                              morphology_operation: str = 'close_open',
                              morphology_kernel_size: Optional[Tuple[int, int]] = None
                              ) -> Optional[np.ndarray]:
        """Creates a binary mask for a single specified node type."""
        color_rgb = self._get_color_rgb_by_name(target_type_name)
        if color_rgb is None:
            logger.warning(f"Warning: Color for type '{target_type_name}' not found. Cannot create mask.")
            return None

        mask = np.all(image_data == np.array(
            color_rgb, dtype=np.uint8).reshape(1, 1, 3), axis=2)
        mask = mask.astype(np.uint8) * 255

        if apply_morphology:
            kernel_size = morphology_kernel_size or self.config.MORPHOLOGY_KERNEL_SIZE
            mask = self.image_processor.apply_morphology(
                mask,
                operation=morphology_operation,
                kernel_size=kernel_size
            )
        return mask

    def _find_connected_components(self, mask: np.ndarray, connectivity: int = 4) \
            -> Tuple[int, np.ndarray, np.ndarray, np.ndarray]:
        return cv2.connectedComponentsWithStats(mask, connectivity=connectivity)

    @abc.abstractmethod
    def create_nodes(self,
                     processed_image_data: np.ndarray,
                     id_map: np.ndarray,
                     z_level: int):
        pass

class RoomNodeCreator(BaseNodeCreator):
    """Creates nodes for room-type areas."""
    def create_nodes(self, processed_image_data: np.ndarray, id_map: np.ndarray, z_level: int):
        target_room_types = self.config.ROOM_TYPES
        if not target_room_types: return

        for room_type_name in target_room_types:
            mask = self._create_mask_for_type(processed_image_data, room_type_name)
            if mask is None: continue

            retval, labels, stats, centroids = self._find_connected_components(mask)
            if retval <= 1: continue

            node_time = self._get_time_by_name(room_type_name)

            for i in range(1, retval):
                area = float(stats[i, cv2.CC_STAT_AREA])
                if area < self.config.AREA_THRESHOLD: continue

                centroid_x, centroid_y = centroids[i]
                position = (int(centroid_x), int(centroid_y), z_level)
                node_id = self.graph_manager.generate_node_id()
                room_node = Node(node_id=node_id, node_type=room_type_name, pos=position,
                                 default_time=node_time, area=area)
                self.graph_manager.add_node(room_node)
                id_map[labels == i] = room_node.id

class VerticalNodeCreator(BaseNodeCreator):
    """Creates nodes for vertical transport areas (stairs, elevators, escalators)."""
    def create_nodes(self, processed_image_data: np.ndarray, id_map: np.ndarray, z_level: int):
        target_vertical_types = self.config.VERTICAL_TYPES
        if not target_vertical_types: return

        for vertical_type_name in target_vertical_types:
            mask = self._create_mask_for_type(processed_image_data, vertical_type_name)
            if mask is None: continue

            retval, labels, stats, centroids = self._find_connected_components(mask)
            if retval <= 1: continue

            node_time = self._get_time_by_name(vertical_type_name)

            for i in range(1, retval):
                area = float(stats[i, cv2.CC_STAT_AREA])
                if area < self.config.AREA_THRESHOLD: continue

                centroid_x, centroid_y = centroids[i]
                position = (int(centroid_x), int(centroid_y), z_level)
                node_id = self.graph_manager.generate_node_id()
                v_node = Node(node_id=node_id, node_type=vertical_type_name, pos=position,
                              default_time=node_time, area=area)
                self.graph_manager.add_node(v_node)
                id_map[labels == i] = v_node.id

class MeshBasedNodeCreator(BaseNodeCreator): # New base for Pedestrian and Outside
    """Base class for creators that generate a mesh of nodes within areas."""

    def _create_mesh_nodes_for_mask(self,
                                    mask: np.ndarray,
                                    region_type_name: str,
                                    id_map: np.ndarray, # Pass id_map to update
                                    id_map_value_for_area: int, # Value to mark the area in id_map
                                    z_level: int,
                                    node_time: float,
                                    grid_size_multiplier: int):
        """Helper to create mesh nodes within a given mask and connect them."""
        # First, mark the entire area in id_map with the special area identifier
        id_map[mask != 0] = id_map_value_for_area

        retval, labels, stats, _ = self._find_connected_components(mask, connectivity=8)
        grid_size = self.config.GRID_SIZE * grid_size_multiplier
        # Estimated area for a single mesh node
        mesh_node_area = float(1)


        for i in range(1, retval): # For each connected component
            component_area = stats[i, cv2.CC_STAT_AREA]
            if component_area < self.config.AREA_THRESHOLD: # Ensure component itself is large enough
                continue

            x_stat, y_stat, w_stat, h_stat, _ = stats[i]

            gx = np.arange(x_stat + grid_size / 2, x_stat + w_stat, grid_size) # Center points in grid cells
            gy = np.arange(y_stat + grid_size / 2, y_stat + h_stat, grid_size)
            if len(gx) == 0 or len(gy) == 0: continue # Avoid empty grid

            grid_points_x, grid_points_y = np.meshgrid(gx, gy)

            grid_points_y_int = grid_points_y.astype(int).clip(0, mask.shape[0] - 1)
            grid_points_x_int = grid_points_x.astype(int).clip(0, mask.shape[1] - 1)
            
            valid_mask_indices = labels[grid_points_y_int, grid_points_x_int] == i
            
            valid_x_coords = grid_points_x[valid_mask_indices]
            valid_y_coords = grid_points_y[valid_mask_indices]

            component_nodes: List[Node] = []
            for vx, vy in zip(valid_x_coords, valid_y_coords):
                pos = (int(vx), int(vy), z_level)
                node_id = self.graph_manager.generate_node_id()
                mesh_node = Node(node_id, region_type_name, pos, node_time, area=mesh_node_area)
                self.graph_manager.add_node(mesh_node)
                component_nodes.append(mesh_node)
                # Optionally, mark the exact grid cell in id_map with the mesh_node.id
                # id_map[int(vy-grid_size/2):int(vy+grid_size/2), int(vx-grid_size/2):int(vx+grid_size/2)] = mesh_node.id
                # For now, the broader area is already marked.

            if not component_nodes or len(component_nodes) < 2:
                continue

            node_positions_2d = np.array([node.pos[:2] for node in component_nodes])
            kdtree = KDTree(node_positions_2d)
            # Max distance to connect (diagonal of a grid cell, plus a small tolerance)
            max_distance_connect = np.sqrt(2) * grid_size * 1.05

            for j, current_node in enumerate(component_nodes):
                # Query for k-nearest, then filter by distance
                # k=9 includes self + 8 neighbors in a square grid
                distances, indices_k_nearest = kdtree.query(
                    current_node.pos[:2],
                    k=min(len(component_nodes), self.config.MESH_NODE_CONNECTIVITY_K), # Ensure k is not > num_points
                    distance_upper_bound=max_distance_connect
                )

                for dist_val, neighbor_idx in zip(distances, indices_k_nearest):
                    if neighbor_idx >= len(component_nodes) or dist_val > max_distance_connect :
                        continue # Out of bounds or too far

                    neighbor_node = component_nodes[neighbor_idx]
                    if current_node.id == neighbor_node.id:
                        continue
                    
                    self.graph_manager.connect_nodes_by_ids(current_node.id, neighbor_node.id)

class PedestrianNodeCreator(MeshBasedNodeCreator):
    """Creates mesh nodes for pedestrian areas (e.g., corridors)."""
    def create_nodes(self, processed_image_data: np.ndarray, id_map: np.ndarray, z_level: int):
        target_pedestrian_types = self.config.PEDESTRIAN_TYPES
        if not target_pedestrian_types: return

        for ped_type_name in target_pedestrian_types:
            mask = self._create_mask_for_type(processed_image_data, ped_type_name)
            if mask is None: continue
            
            self._create_mesh_nodes_for_mask(
                mask=mask,
                region_type_name=ped_type_name,
                id_map=id_map,
                id_map_value_for_area=self.config.PEDESTRIAN_ID_MAP_VALUE,
                z_level=z_level,
                node_time=self.config.PEDESTRIAN_TIME,
                grid_size_multiplier=1
            )

class OutsideNodeCreator(MeshBasedNodeCreator):
    """Creates mesh nodes for outside areas."""
    def create_nodes(self, processed_image_data: np.ndarray, id_map: np.ndarray, z_level: int):
        target_outside_types = self.config.OUTSIDE_TYPES
        if not target_outside_types: return

        for outside_type_name in target_outside_types: # Should typically be just one '室外'
            mask = self._create_mask_for_type(processed_image_data, outside_type_name)
            if mask is None: continue
            
            self._create_mesh_nodes_for_mask(
                mask=mask,
                region_type_name=outside_type_name,
                id_map=id_map,
                id_map_value_for_area=self.config.OUTSIDE_ID_MAP_VALUE,
                z_level=z_level,
                node_time=self._get_time_by_name(outside_type_name) * self.config.OUTSIDE_MESH_TIMES_FACTOR,
                grid_size_multiplier=self.config.OUTSIDE_MESH_TIMES_FACTOR
            )

class ConnectionNodeCreator(BaseNodeCreator):
    """Creates nodes for connections (e.g., doors) and links them to adjacent areas."""
    def create_nodes(self, processed_image_data: np.ndarray, id_map: np.ndarray, z_level: int):
        target_connection_types = self.config.CONNECTION_TYPES # Typically '门'
        if not target_connection_types: return

        pass_through_ids_in_id_map = [self.config.BACKGROUND_ID_MAP_VALUE]
        dilation_kernel_np = np.ones(self.config.CONNECTION_DILATION_KERNEL_SIZE, np.uint8)

        for conn_type_name in target_connection_types:
            mask = self._create_mask_for_type(processed_image_data, conn_type_name)
            if mask is None: continue

            retval, labels, stats, centroids = self._find_connected_components(mask)
            if retval <= 1: continue

            for i in range(1, retval): # For each door component
                area = float(stats[i, cv2.CC_STAT_AREA])
                # Doors can be smaller, adjust threshold if needed, e.g., AREA_THRESHOLD / 4
                if area < self.config.AREA_THRESHOLD / 10 and area < 5: # Allow very small doors
                    continue

                centroid_x, centroid_y = centroids[i]
                position = (int(centroid_x), int(centroid_y), z_level)

                node_id = self.graph_manager.generate_node_id()
                conn_node = Node(node_id, conn_type_name, position,
                                 self.config.CONNECTION_TIME, area=area)
                self.graph_manager.add_node(conn_node)

                component_mask_pixels = (labels == i)
                id_map[component_mask_pixels] = conn_node.id # Mark door pixels with its own ID

                # Dilate the door component mask to find neighboring regions/nodes in id_map
                # Need to convert boolean mask `component_mask_pixels` to uint8 for dilate
                uint8_component_mask = component_mask_pixels.astype(np.uint8) * 255
                dilated_component_mask = cv2.dilate(uint8_component_mask, dilation_kernel_np, iterations=1)
                
                neighbor_ids_in_map = np.unique(id_map[dilated_component_mask != 0])

                # Determine door type
                is_connected_to_outside = self.config.OUTSIDE_ID_MAP_VALUE in neighbor_ids_in_map
                is_connected_to_pedestrian = self.config.PEDESTRIAN_ID_MAP_VALUE in neighbor_ids_in_map

                if is_connected_to_outside:
                    conn_node.door_type = 'out'
                elif is_connected_to_pedestrian:
                    # If it connects to pedestrian and NOT to outside, it's an 'in' door (e.g. from corridor to room)
                    # Or if it connects pedestrian to room.
                    # If a door connects pedestrian area to an outside area, it's more like an 'out' door.
                    # This needs careful definition based on your use case.
                    # For now: if it sees pedestrian and not outside, consider it 'in' (towards rooms/internal).
                    # If it sees pedestrian AND outside, the 'out' takes precedence.
                    conn_node.door_type = 'in'
                else: # Connects only to actual nodes (rooms, vertical, other doors)
                    conn_node.door_type = 'room' # Default for internal doors

                # Connect the door node to the identified neighboring ACTUAL nodes
                for neighbor_id_val in neighbor_ids_in_map:
                    if neighbor_id_val == conn_node.id or neighbor_id_val in pass_through_ids_in_id_map:
                        continue
                    
                    # Check if it's an actual node ID (positive)
                    # Special area IDs (OUTSIDE_ID_MAP_VALUE, PEDESTRIAN_ID_MAP_VALUE) are negative or large positive.
                    if neighbor_id_val > 0: # Assuming actual node IDs are positive and start from 1
                        target_node = self.graph_manager.get_node_by_id(neighbor_id_val)
                        if target_node and target_node.id != conn_node.id:
                            self.graph_manager.connect_nodes_by_ids(conn_node.id, target_node.id)
                
                # Special handling: if a door is 'out' and also touches a pedestrian area,
                # it might still need a direct link to that pedestrian area's mesh nodes later.
                # Similarly for 'in' doors. This will be handled in a later connection phase.
</file>

<file path="src/plotting/__init__.py">
# src/plotting/__init__.py
"""Plotting module for network visualization."""

from .plotter import BasePlotter, PlotlyPlotter # MatplotlibPlotter can be added later

__all__ = ["BasePlotter", "PlotlyPlotter"]
</file>

<file path="src/rl_optimizer/data/cache_manager.py">
import pandas as pd
from pathlib import Path
from typing import Dict, List, Tuple, Any
from collections import defaultdict
import itertools

from src.config import RLConfig
from src.rl_optimizer.utils.setup import setup_logger, save_json, load_json, save_pickle, load_pickle

logger = setup_logger(__name__)

class CacheManager:

    def __init__(self, config: RLConfig):
        """
        初始化缓存管理器，并处理所有数据的加载和预计算。

        Args:
            config (RLConfig): RL优化器的配置对象。
        """
        self.config = config

        # --- 1. 加载和处理基础数据 ---
        self.raw_travel_times_df = self._load_raw_travel_times()
        self.node_data = self._load_and_process_node_data(self.raw_travel_times_df)
        self.travel_times_matrix = self.raw_travel_times_df.drop('面积', errors='ignore')

        # --- 2. 节点分类与封装 ---
        # 核心逻辑：所有节点分类都在此处完成并封装
        fixed_mask = self.node_data['generic_name'].isin(config.FIXED_NODE_TYPES)
        self.all_nodes_list: List[str] = self.node_data['node_id'].tolist()
        self.placeable_nodes_df: pd.DataFrame = self.node_data[~fixed_mask].copy()
        self.fixed_nodes_df: pd.DataFrame = self.node_data[fixed_mask].copy()

        # 公开给外部使用的、清晰的属性
        self.placeable_departments: List[str] = self.placeable_nodes_df['node_id'].tolist()
        self.placeable_slots: List[str] = self.placeable_nodes_df['node_id'].tolist()

        # --- 3. 解析流程与流线 ---
        self.variants = self.get_node_variants()
        self.traffic = self.get_traffic_distribution()
        self.resolved_pathways = self.get_resolved_pathways()

    def _load_raw_travel_times(self) -> pd.DataFrame:
        """加载原始CSV文件。"""
        if not self.config.TRAVEL_TIMES_CSV.exists():
            logger.error(f"致命错误: 原始通行时间文件未找到: {self.config.TRAVEL_TIMES_CSV}")
            raise FileNotFoundError(f"原始通行时间文件未找到: {self.config.TRAVEL_TIMES_CSV}")
        return pd.read_csv(self.config.TRAVEL_TIMES_CSV, index_col=0)

    def _load_and_process_node_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """从已加载的DataFrame中解析节点和面积。"""
        logger.info("正在处理节点数据...")
        node_ids = df.columns.tolist()
        
        if '面积' not in df.index:
            raise ValueError("CSV文件中必须包含名为'面积'的最后一行。")
        area_series = df.loc['面积']

        if not all(node_id in area_series.index for node_id in node_ids):
             raise ValueError("CSV文件的列标题与'面积'行的索引不完全匹配。请检查文件格式。")

        nodes_list = [{'node_id': node_id, 'area': area_series[node_id]} for node_id in node_ids]
        all_nodes_df = pd.DataFrame(nodes_list)
        all_nodes_df['generic_name'] = all_nodes_df['node_id'].apply(lambda x: str(x).split('_')[0])
        
        logger.info(f"成功处理了 {len(all_nodes_df)} 个唯一节点及其面积。")
        return all_nodes_df

    def get_node_variants(self) -> Dict[str, List[str]]:
        """
        获取节点变体映射。如果缓存存在则加载，否则从节点数据生成。
        """
        if self.config.NODE_VARIANTS_JSON.exists():
            logger.info(f"从缓存加载节点变体: {self.config.NODE_VARIANTS_JSON}")
            return load_json(self.config.NODE_VARIANTS_JSON)
        
        logger.info("缓存未找到，正在生成节点变体...")
        variants = self.node_data.groupby('generic_name')['node_id'].apply(list).to_dict()
        save_json(variants, self.config.NODE_VARIANTS_JSON)
        logger.info(f"节点变体已生成并缓存至: {self.config.NODE_VARIANTS_JSON}")
        return variants
    
    def get_traffic_distribution(self) -> Dict[str, Dict[str, float]]:
        """
        获取流量分布。如果缓存存在则加载，否则基于变体生成初始权重。
        """
        if self.config.TRAFFIC_DISTRIBUTION_JSON.exists():
            logger.info(f"从缓存加载流量分布: {self.config.TRAFFIC_DISTRIBUTION_JSON}")
            return load_json(self.config.TRAFFIC_DISTRIBUTION_JSON)
            
        logger.info("缓存未找到，正在生成初始流量分布...")
        traffic_distribution = {
            generic_name: {node: 1.0 for node in specific_nodes}
            for generic_name, specific_nodes in self.variants.items()
        }
        save_json(traffic_distribution, self.config.TRAFFIC_DISTRIBUTION_JSON)
        logger.info(f"初始流量分布已生成并缓存至: {self.config.TRAFFIC_DISTRIBUTION_JSON}")
        return traffic_distribution
    
    def get_resolved_pathways(self) -> List[Tuple[List[str], float]]:
        """
        获取最终解析出的流线列表。这是调度核心，如果缓存不存在则触发解析。
        """
        if self.config.RESOLVED_PATHWAYS_PKL.exists():
            logger.info(f"从缓存加载已解析的流线: {self.config.RESOLVED_PATHWAYS_PKL}")
            return load_pickle(self.config.RESOLVED_PATHWAYS_PKL)

        logger.info("缓存未找到，正在解析就医流程以生成具体流线...")
        
        try:
            templates = load_json(self.config.PROCESS_TEMPLATES_JSON)
        except FileNotFoundError:
            logger.error(f"致命错误: 就医流程模板文件未找到: {self.config.PROCESS_TEMPLATES_JSON}")
            raise
        
        resolved_pathways = []
        for template in templates:
            resolved_pathways.extend(self._resolve_single_template(template))
        
        save_pickle(resolved_pathways, self.config.RESOLVED_PATHWAYS_PKL)
        logger.info(f"所有流线已解析并缓存 ({len(resolved_pathways)}条): {self.config.RESOLVED_PATHWAYS_PKL}")
        return resolved_pathways
    
    def _resolve_single_template(self, template: Dict[str, Any]) -> List[Tuple[List[str], float]]:
        """
        根据单个流程模板，高效地解析出所有可能的具体流线及其权重。
        该方法能正确处理端点（如门）和核心路径节点权重的不同计算逻辑。

        Args:
            template (Dict[str, Any]): 单个就医流程的模板字典。

        Returns:
            List[Tuple[List[str], float]]: 一个包含(具体流线, 最终权重)元组的列表。
        """
        base_weight = template.get('base_weight', 1.0)

        # 1. 准备所有部分的选项列表
        start_nodes_options = [self.variants.get(gn, [gn]) for gn in template['start_nodes']]
        core_sequences_options = [self.variants.get(gn, [gn]) for gn in template['core_sequence']]
        end_nodes_options = [self.variants.get(gn, [gn]) for gn in template['end_nodes']]

        # 2. 一次性计算所有组合的笛卡尔积
        # 注意：我们将起点、核心和终点的组合分开处理，以便应用不同的权重逻辑
        start_combinations = list(itertools.product(*start_nodes_options))
        core_combinations = list(itertools.product(*core_sequences_options))
        end_combinations = list(itertools.product(*end_nodes_options))

        pathways = []
        # 3. 组合并计算权重 (恢复使用清晰的三重循环，因为逻辑已变得复杂)
        for start_combo in start_combinations:
            for core_combo in core_combinations:
                for end_combo in end_combinations:
                    
                    # --- 权重计算开始 ---
                    final_weight = base_weight
                    
                    # a. 计算起点的权重 (总是计算)
                    for node in start_combo:
                        final_weight *= self._get_normalized_weight(node)
                        
                    # b. 计算核心路径的权重 (每个通用名只计算一次)
                    processed_core_generics = set()
                    for node in core_combo:
                        generic_name = str(node).split('_')[0]
                        if generic_name not in processed_core_generics:
                            final_weight *= self._get_normalized_weight(node)
                            processed_core_generics.add(generic_name)

                    # c. 计算终点的权重 (总是计算)
                    for node in end_combo:
                        final_weight *= self._get_normalized_weight(node)

                    # --- 权重计算结束 ---

                    # 拼接成完整流线
                    full_path = list(start_combo) + list(core_combo) + list(end_combo)
                    pathways.append({
                        "process_id": template['process_id'],
                        "path": full_path,
                        "weight": final_weight
                    })
            
        return pathways
    
    def _get_normalized_weight(self, node_name: str) -> float:
        """
        计算单个节点的归一化流量权重。
        如果一个通用名下只有一个变体，其权重因子为1.0。
        """
        generic_name = str(node_name).split('_')[0]
        # 使用 self.traffic，它是在 __init__ 中加载或生成的
        distribution_map = self.traffic.get(generic_name)
        
        # 如果找不到分布或只有一个变体，则权重因子为1
        if not distribution_map or len(distribution_map) <= 1:
            return 1.0

        total_weight = sum(distribution_map.values())
        if total_weight == 0:
            logger.warning(f"通用名 '{generic_name}' 的总权重为0，无法进行归一化。")
            return 0.0

        raw_weight = distribution_map.get(node_name, 0.0)
        return raw_weight / total_weight
</file>

<file path="src/rl_optimizer/env/cost_calculator.py">
# src/rl_optimizer/env/cost_calculator.py

import numpy as np
import pandas as pd
from scipy.sparse import dok_matrix, csr_matrix
from typing import List, Dict, Tuple, Any
from collections import defaultdict
import itertools

from src.config import RLConfig
from src.rl_optimizer.utils.setup import setup_logger, load_pickle, save_pickle

logger = setup_logger(__name__)

class CostCalculator:
    """
    一个高效的成本计算器，用于评估给定布局下的加权总通行时间。
    它通过预计算一个稀疏矩阵来实现高性能计算。
    """

    def __init__(self, 
                 config: RLConfig, 
                 resolved_pathways: List[Dict[str, Any]], 
                 travel_times: pd.DataFrame, 
                 placeable_slots: List[str],
                 placeable_departments: List[str]):
        """
        初始化成本计算器。

        Args:
            config (RLConfig): RL优化器的配置对象。
            resolved_pathways (List[Dict]): 已解析的流线字典列表。
            travel_times (pd.DataFrame): 纯通行时间矩阵，其行列为原始节点名。
            placeable_slots (List[str]): 可用物理槽位的原始节点名称列表。
            placeable_departments (List[str]): 所有需要被布局的科室的完整列表。
        """
        self.config = config
        self.travel_times = travel_times
        self.placeable_slots = placeable_slots
        self.num_slots = len(placeable_slots)
        
        self.dept_list = sorted(placeable_departments)
        self.pair_to_col, self.num_dept_pairs = self._create_dept_pair_mapping()
        
        self.M, self.W, self.pathway_to_process_id = self._build_cost_matrices(resolved_pathways)

    def _create_dept_pair_mapping(self) -> Tuple[Dict[Tuple[str, str], int], int]:
        """创建科室对到列索引的映射。"""
        dept_pairs = list(itertools.permutations(self.dept_list, 2))
        return {pair: i for i, pair in enumerate(dept_pairs)}, len(dept_pairs)
    
    def _build_cost_matrices(self, pathways: List[Dict[str, Any]]) -> Tuple[csr_matrix, np.ndarray, List[str]]:
        """构建成本矩阵。"""
        num_pathways = len(pathways)
        pathway_to_process_id = [p['process_id'] for p in pathways]
        
        M_dok = dok_matrix((num_pathways, self.num_dept_pairs), dtype=np.float32)
        W_arr = np.zeros(num_pathways, dtype=np.float32)

        for i, p_data in enumerate(pathways):
            path = p_data['path']
            W_arr[i] = p_data['weight']
            for j in range(len(path) - 1):
                pair = (path[j], path[j+1])
                if pair in self.pair_to_col:
                    M_dok[i, self.pair_to_col[pair]] += 1
        
        return M_dok.tocsr(), W_arr, pathway_to_process_id
    
    def calculate_total_cost(self, layout: List[str]) -> float:
        """
        给定一个布局，高效计算总加权通行时间。

        Args:
            layout (List[str]): 一个表示当前布局的科室名称列表。
                               其索引对应于 self.placeable_nodes 的索引。

        Returns:
            float: 计算出的总加权成本。
        """
        time_vector = self._get_time_vector(layout)
        time_per_pathway = self.M.dot(time_vector)
        total_cost = time_per_pathway.dot(self.W)
        return float(total_cost)
    
    def calculate_per_process_cost(self, layout: List[str]) -> Dict[str, float]:
        """
        评估每个就医流程模板的单独通行时间（未加权）。
        """
        time_vector = self._get_time_vector(layout)
        time_per_pathway = self.M.dot(time_vector)
        
        process_costs = defaultdict(float)
        # 遍历所有流线，将它们的时间累加到对应的流程模板上
        for i, process_id in enumerate(self.pathway_to_process_id):
            process_costs[process_id] += time_per_pathway[i]
            
        return dict(process_costs)
    
    def _get_time_vector(self, layout: List[str]) -> np.ndarray:
        """
        内部辅助函数，根据布局计算所有科室对的通行时间向量。
        """
        time_vector = np.zeros(self.num_dept_pairs, dtype=np.float32)

        # 创建一个从科室到其当前所在槽位（原始节点名）的映射
        # layout 的索引是槽位索引，值是科室名
        # self.placeable_slots 的索引是槽位索引，值是原始节点名
        dept_to_slot_node = {dept: self.placeable_slots[i] for i, dept in enumerate(layout) if dept is not None}

        # 遍历所有需要计算时间的科室对
        for pair, col_idx in self.pair_to_col.items():
            dept_from, dept_to = pair
            
            # 查找科室所在的原始节点名
            node_from = dept_to_slot_node.get(dept_from)
            node_to = dept_to_slot_node.get(dept_to)
            
            # 如果两个科室都已放置，则从通行时间矩阵中查找时间
            if node_from is not None and node_to is not None:
                time_vector[col_idx] = self.travel_times.loc[node_from, node_to]
                
        return time_vector
</file>

<file path="src/rl_optimizer/utils/checkpoint_callback.py">
# src/rl_optimizer/utils/checkpoint_callback.py

import os
import json
import pickle
import time
from pathlib import Path
from typing import Dict, Any, Optional

from stable_baselines3.common.callbacks import BaseCallback
from stable_baselines3.common.logger import Logger

from src.rl_optimizer.utils.setup import setup_logger

logger = setup_logger(__name__)


class CheckpointCallback(BaseCallback):
    """
    自定义checkpoint回调，用于定期保存完整的训练状态。
    
    支持保存：
    - 模型参数
    - 优化器状态
    - 学习率调度器状态
    - 训练元数据（步数、时间等）
    """
    
    def __init__(
        self,
        save_freq: int,
        save_path: str,
        name_prefix: str = "checkpoint",
        save_replay_buffer: bool = False,
        save_vecnormalize: bool = False,
        verbose: int = 1
    ):
        """
        初始化checkpoint回调。
        
        Args:
            save_freq (int): 每多少训练步保存一次checkpoint
            save_path (str): checkpoint保存目录
            name_prefix (str): checkpoint文件名前缀
            save_replay_buffer (bool): 是否保存replay buffer（如果有）
            save_vecnormalize (bool): 是否保存VecNormalize状态（如果有）
            verbose (int): 日志详细级别
        """
        super().__init__(verbose)
        self.save_freq = save_freq
        self.save_path = Path(save_path)
        self.name_prefix = name_prefix
        self.save_replay_buffer = save_replay_buffer
        self.save_vecnormalize = save_vecnormalize
        
        # 确保保存目录存在
        self.save_path.mkdir(parents=True, exist_ok=True)
        
        # 训练状态跟踪
        self.start_time = time.time()
        self.checkpoint_count = 0
        
    def _init_callback(self) -> None:
        """初始化回调时的操作。"""
        if self.save_path is not None:
            os.makedirs(self.save_path, exist_ok=True)
            
    def _on_step(self) -> bool:
        """每个训练步后的回调。"""
        if self.n_calls % self.save_freq == 0:
            self._save_checkpoint()
        return True
        
    def _save_checkpoint(self) -> None:
        """保存完整的checkpoint。"""
        self.checkpoint_count += 1
        checkpoint_name = f"{self.name_prefix}_{self.n_calls:08d}_steps"
        checkpoint_path = self.save_path / checkpoint_name
        
        if self.verbose >= 1:
            logger.info(f"正在保存checkpoint: {checkpoint_name}")
            
        try:
            # 1. 保存模型
            model_path = checkpoint_path.with_suffix('.zip')
            self.model.save(str(model_path))
            
            # 2. 保存训练元数据
            metadata = self._collect_metadata()
            metadata_path = checkpoint_path.with_suffix('.json')
            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)
                
            # 3. 保存训练状态（如果启用）
            if hasattr(self.model, 'get_training_state'):
                state_path = checkpoint_path.with_suffix('.pkl')
                training_state = self.model.get_training_state()
                with open(state_path, 'wb') as f:
                    pickle.dump(training_state, f)
                    
            # 4. 保存VecNormalize状态（如果有）
            if self.save_vecnormalize and hasattr(self.training_env, 'normalize_obs'):
                vecnorm_path = checkpoint_path.with_suffix('.vecnorm.pkl')
                self.training_env.save(str(vecnorm_path))
                
            if self.verbose >= 1:
                logger.info(f"Checkpoint保存成功: {checkpoint_name}")
                
        except Exception as e:
            logger.error(f"保存checkpoint时发生错误: {e}", exc_info=True)
            
    def _collect_metadata(self) -> Dict[str, Any]:
        """收集训练元数据。"""
        current_time = time.time()
        training_duration = current_time - self.start_time
        
        metadata = {
            "checkpoint_info": {
                "timestamp": current_time,
                "checkpoint_count": self.checkpoint_count,
                "training_duration_seconds": training_duration,
                "training_duration_hours": training_duration / 3600,
            },
            "training_progress": {
                "n_calls": self.n_calls,
                "num_timesteps": self.model.num_timesteps,
                "total_timesteps": getattr(self.model, '_total_timesteps', None),
                "progress_percent": (
                    self.model.num_timesteps / getattr(self.model, '_total_timesteps', 1) * 100
                    if hasattr(self.model, '_total_timesteps') and self.model._total_timesteps > 0
                    else 0
                ),
            },
            "model_info": {
                "algorithm": self.model.__class__.__name__,
                "learning_rate": self.model.learning_rate,
                "n_envs": getattr(self.model.env, 'num_envs', 1),
            }
        }
        
        # 添加学习率调度器信息（如果有）
        if hasattr(self.model, 'lr_schedule'):
            try:
                current_lr = self.model.lr_schedule(self.model.num_timesteps / 
                                                  getattr(self.model, '_total_timesteps', self.model.num_timesteps))
                metadata["model_info"]["current_learning_rate"] = float(current_lr)
            except:
                pass
                
        return metadata
        
    def get_latest_checkpoint(self) -> Optional[Path]:
        """获取最新的checkpoint路径。"""
        if not self.save_path.exists():
            return None
            
        checkpoint_files = list(self.save_path.glob(f"{self.name_prefix}_*_steps.zip"))
        if not checkpoint_files:
            return None
            
        # 按文件名中的步数排序，获取最新的
        checkpoint_files.sort(key=lambda x: int(x.stem.split('_')[-2]))
        return checkpoint_files[-1]
        
    @staticmethod
    def load_checkpoint_metadata(checkpoint_path: Path) -> Optional[Dict[str, Any]]:
        """加载checkpoint的元数据。"""
        metadata_path = checkpoint_path.with_suffix('.json')
        if not metadata_path.exists():
            return None
            
        try:
            with open(metadata_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"加载checkpoint元数据失败: {e}")
            return None
</file>

<file path="src/rl_optimizer/utils/lr_scheduler.py">
# src/rl_optimizer/utils/lr_scheduler.py

from typing import Callable


def linear_schedule(initial_value: float, final_value: float = 0.0) -> Callable[[float], float]:
    """
    创建线性学习率调度器。
    
    该调度器会从初始学习率线性衰减到最终学习率。在训练开始时使用初始值，
    在训练结束时使用最终值，中间过程线性插值。
    
    Args:
        initial_value (float): 初始学习率值
        final_value (float): 最终学习率值，默认为0.0
    
    Returns:
        Callable[[float], float]: 学习率调度函数，接收progress_remaining参数
                                  (从1.0衰减到0.0表示训练进度)
    
    Example:
        >>> # 创建从3e-4线性衰减到1e-5的调度器
        >>> lr_scheduler = linear_schedule(3e-4, 1e-5)
        >>> # 训练开始时 (progress_remaining=1.0)
        >>> lr_scheduler(1.0)  # 返回 3e-4
        >>> # 训练中期 (progress_remaining=0.5)
        >>> lr_scheduler(0.5)  # 返回约 1.5e-4
        >>> # 训练结束时 (progress_remaining=0.0)
        >>> lr_scheduler(0.0)  # 返回 1e-5
    """
    def schedule_func(progress_remaining: float) -> float:
        """
        根据训练进度计算当前学习率。
        
        Args:
            progress_remaining (float): 剩余训练进度，从1.0（开始）递减到0.0（结束）
        
        Returns:
            float: 当前应使用的学习率值
        """
        # 线性插值公式：current_lr = final_value + progress_remaining * (initial_value - final_value)
        return final_value + progress_remaining * (initial_value - final_value)
    
    return schedule_func


def constant_schedule(value: float) -> Callable[[float], float]:
    """
    创建常数学习率调度器。
    
    该调度器在整个训练过程中保持固定的学习率不变。
    
    Args:
        value (float): 固定的学习率值
    
    Returns:
        Callable[[float], float]: 学习率调度函数，始终返回固定值
    
    Example:
        >>> # 创建固定3e-4学习率的调度器
        >>> lr_scheduler = constant_schedule(3e-4)
        >>> lr_scheduler(1.0)  # 返回 3e-4
        >>> lr_scheduler(0.5)  # 返回 3e-4
        >>> lr_scheduler(0.0)  # 返回 3e-4
    """
    def schedule_func(progress_remaining: float) -> float:
        """
        返回固定的学习率值，不受训练进度影响。
        
        Args:
            progress_remaining (float): 剩余训练进度（此参数被忽略）
        
        Returns:
            float: 固定的学习率值
        """
        return value
    
    return schedule_func


def get_lr_scheduler(schedule_type: str, initial_lr: float, final_lr: float = 0.0) -> Callable[[float], float]:
    """
    根据配置创建相应的学习率调度器。
    
    Args:
        schedule_type (str): 调度器类型，支持 "linear" 和 "constant"
        initial_lr (float): 初始学习率
        final_lr (float): 最终学习率，仅在线性调度器中使用
    
    Returns:
        Callable[[float], float]: 相应的学习率调度函数
    
    Raises:
        ValueError: 当调度器类型不支持时抛出异常
    
    Example:
        >>> # 创建线性衰减调度器
        >>> scheduler = get_lr_scheduler("linear", 3e-4, 1e-5)
        >>> # 创建常数调度器
        >>> scheduler = get_lr_scheduler("constant", 3e-4)
    """
    if schedule_type == "linear":
        return linear_schedule(initial_lr, final_lr)
    elif schedule_type == "constant":
        return constant_schedule(initial_lr)
    else:
        raise ValueError(
            f"不支持的学习率调度器类型: '{schedule_type}'. "
            f"支持的类型: 'linear', 'constant'"
        )
</file>

<file path="src/analysis/process_flow.py">
import pandas as pd
import logging
from typing import List, Dict, Optional, Tuple, Set, Any, Mapping
from itertools import product

from src.analysis.word_detect import WordDetect
from src.config import NetworkConfig

logger = logging.getLogger(__name__)


class PeopleFlow:
    """
    Represents a specific, resolved flow of an entity through a sequence of nodes.

    Each node in the flow is represented by its unique "Name_ID" string.
    This class primarily stores a fully determined path.

    Attributes:
        identify (Any): An identifier for this specific flow instance or the
            workflow it belongs to.
        actual_node_id_sequence (List[str]): The complete, ordered list of
            "Name_ID" strings representing the flow.
        _cached_hash (Optional[int]): Cached hash value for performance.
    """

    def __init__(self, identify: Any, actual_node_id_sequence: List[str]):
        """
        Initializes a PeopleFlow instance with a specific node ID sequence.

        Args:
            identify (Any): An identifier for this flow.
            actual_node_id_sequence (List[str]): The fully resolved sequence
                of "Name_ID" strings for this flow.
        """
        self.identify: Any = identify
        self.actual_node_id_sequence: List[str] = actual_node_id_sequence
        self.total_time: Optional[float] = None
        self._cached_hash: Optional[int] = None

    def update_total_time(self, total_time: float) -> None:
        """
        Updates the total travel time for this flow.
        """
        self.total_time = total_time

    @property
    def start_node_id(self) -> Optional[str]:
        """Optional[str]: The 'Name_ID' of the first node in the sequence, if any."""
        return self.actual_node_id_sequence[0] if self.actual_node_id_sequence else None

    @property
    def end_node_id(self) -> Optional[str]:
        """Optional[str]: The 'Name_ID' of the last node in the sequence, if any."""
        if len(self.actual_node_id_sequence) > 0:
            return self.actual_node_id_sequence[-1]
        return None

    @property
    def intermediate_node_ids(self) -> List[str]:
        """List[str]: A list of 'Name_ID's for intermediate nodes."""
        if len(self.actual_node_id_sequence) > 2:
            return self.actual_node_id_sequence[1:-1]
        return []

    def __eq__(self, other: object) -> bool:
        """
        Checks equality based on the `identify` and `actual_node_id_sequence`.
        """
        if not isinstance(other, PeopleFlow):
            return NotImplemented
        return (self.identify == other.identify and
                self.actual_node_id_sequence == other.actual_node_id_sequence)

    def __hash__(self) -> int:
        """
        Computes hash based on `identify` and the tuple of `actual_node_id_sequence`.
        """
        if self._cached_hash is None:
            # Making sequence a tuple makes it hashable
            self._cached_hash = hash(
                (self.identify, tuple(self.actual_node_id_sequence)))
        return self._cached_hash

    def __repr__(self) -> str:
        """
        Returns a string representation of the PeopleFlow instance.
        """
        flow_str = " -> ".join(
            self.actual_node_id_sequence) if self.actual_node_id_sequence else "Empty"
        return (
            f"PeopleFlow(identify={self.identify}, "
            f"path=[{flow_str}], "
            f"total_time={self.total_time})"
        )


class PathFinder:
    """
    Finds all possible PeopleFlow paths based on a sequence of node names
    and a CSV file defining available "Name_ID"s and their connections/travel times.
    """

    def __init__(self, csv_filepath: str = None, config: NetworkConfig = None):
        """
        Initializes the PathFinder by loading and processing the CSV file.

        Args:
            csv_filepath (str): Path to the CSV file. The CSV should have
                "Name_ID" formatted strings as column headers and row index.
        """
        if config:
            self.config: NetworkConfig = config
        else:
            self.config: NetworkConfig = NetworkConfig()

        if csv_filepath:
            self.csv_filepath: str = csv_filepath
        else:
            self.csv_filepath: str = self.config.RESULT_PATH / 'super_network_travel_times.csv'

        self.name_to_ids_map: Dict[str, List[str]] = {}
        self.all_name_ids: Set[str] = set()
        self.travel_times_df: Optional[pd.DataFrame] = None
        self._load_and_process_csv()

    def _parse_name_id(self, name_id_str: str) -> Tuple[str, str]:
        """
        Parses a "Name_ID" string into its name and ID components.

        Args:
            name_id_str (str): The string in "Name_ID" format (e.g., "门_11072").

        Returns:
            Tuple[str, str]: (name, id)
        """
        parts = name_id_str.split('_', 1)
        name = parts[0]
        # if no '_', id is same as name
        node_id = parts[1] if len(parts) > 1 else name
        return name, node_id

    def _load_and_process_csv(self) -> None:
        """
        Loads the CSV, extracts "Name_ID"s, and populates the name_to_ids_map.
        """
        try:
            # Assuming the first column is the index and also contains Name_ID
            df = pd.read_csv(self.csv_filepath, index_col=0)
            self.travel_times_df = df
        except FileNotFoundError:
            logger.error(f"Error: CSV file not found at {self.csv_filepath}")
            return
        except Exception as e:
            logger.error(f"Error loading CSV {self.csv_filepath}: {e}")
            return

        # Process column headers (assuming they are the primary source of Name_IDs)
        # and also the index if it's different or more comprehensive
        all_headers = list(df.columns)
        if df.index.name is not None or df.index.dtype == 'object':  # Check if index is meaningful
            all_headers.extend(list(df.index))

        unique_name_ids = sorted(list(set(all_headers)))  # Get unique Name_IDs

        for name_id_str in unique_name_ids:
            if not isinstance(name_id_str, str) or '_' not in name_id_str:
                # Skip non-string headers or headers not in expected format, like "面积"
                # print(f"Skipping header/index: {name_id_str} as it's not in Name_ID format.")
                continue

            self.all_name_ids.add(name_id_str)
            name, _ = self._parse_name_id(name_id_str)
            if name not in self.name_to_ids_map:
                self.name_to_ids_map[name] = []
            # Ensure no duplicates if a Name_ID appears in both columns and index
            if name_id_str not in self.name_to_ids_map[name]:
                self.name_to_ids_map[name].append(name_id_str)

    def transform_workflow_name(self, workflow_names: List[str]) -> str:
        """
        Transforms a workflow name by detecting the nearest word in the CSV.
        """
        word_detect = WordDetect(config=self.config)
        all_type_names = self.config.ALL_TYPES
        return word_detect.detect_nearest_word(workflow_names, all_type_names)

    def generate_flows(self,
                       workflow_names: List[str],
                       workflow_identifier: Any = 0,
                       # Changed Dict to Mapping for broader type hint
                       custom_assignment_map: Optional[Mapping[str,
                                                               List[str]]] = None
                       ) -> List[PeopleFlow]:
        """Generates all possible PeopleFlow objects for a given sequence of node names.

        Uses custom_assignment_map if provided for resolving functional names to
        physical Name_IDs, otherwise defaults to the instance's self.name_to_ids_map
        which is loaded from the CSV during initialization.

        Args:
            workflow_names: An ordered list of functional node names.
            workflow_identifier: An identifier for the generated PeopleFlow objects.
            custom_assignment_map: An optional mapping where keys are functional type
                names (e.g., '妇科') and values are lists of physical Name_ID strings
                (e.g., ['妇科_101', '妇科_102']) that currently fulfill that function.

        Returns:
            A list of PeopleFlow objects.
        """
        if not workflow_names:
            return []

        # Use the provided custom_assignment_map if available, otherwise use the default one
        assignment_to_use = custom_assignment_map if custom_assignment_map is not None else self.name_to_ids_map

        # Convert node names in `workflow_names` to the closest node types (e.g., from config.ALL_TYPES)
        # This step helps normalize user input if workflow_names might not exactly match official types.

        possible_ids_per_step: List[List[str]] = []
        for name in workflow_names:
            # Use the determined assignment map
            ids_for_name = assignment_to_use.get(name)
            if not ids_for_name:
                logger.warning(
                    f"Functional type '{name}' (from original '{workflow_names[workflow_names.index(name)] if name in workflow_names else 'N/A'}') "
                    f"not found in the current assignment map. Workflow '{workflow_identifier}' cannot be fully resolved."
                )
                return []  # Cannot generate flows if a step is unresolvable
            possible_ids_per_step.append(ids_for_name)

        # Use itertools.product to get all combinations of Name_IDs
        all_possible_id_sequences = product(*possible_ids_per_step)

        generated_flows: List[PeopleFlow] = []
        # Use a local counter for unique flow IDs within this specific generation call,
        # prefixed by the workflow_identifier.
        flow_counter_for_identifier = 0
        for id_sequence_tuple in all_possible_id_sequences:
            # Create a more unique identifier for the flow if multiple flows are generated for one workflow_identifier
            current_flow_id = f"{workflow_identifier}_{flow_counter_for_identifier}" \
                if isinstance(workflow_identifier, str) else (workflow_identifier, flow_counter_for_identifier)

            flow = PeopleFlow(identify=current_flow_id,
                              actual_node_id_sequence=list(id_sequence_tuple))
            flow_counter_for_identifier += 1
            generated_flows.append(flow)

        return generated_flows

    def get_travel_time(self, from_name_id: str, to_name_id: str) -> Optional[float]:
        """
        Gets the travel time between two specific "Name_ID" nodes.

        Args:
            from_name_id (str): The "Name_ID" of the source node.
            to_name_id (str): The "Name_ID" of the target node.

        Returns:
            Optional[float]: The travel time if connection exists, else None.
                           Returns None also if dataframe isn't loaded.
        """
        if self.travel_times_df is None:
            logger.warning("Warning: Travel times DataFrame not loaded.")
            return None
        try:
            # Ensure both from_name_id and to_name_id are in the DataFrame's
            # index and columns to avoid KeyError
            if from_name_id in self.travel_times_df.index and \
               to_name_id in self.travel_times_df.columns:
                time = self.travel_times_df.loc[from_name_id, to_name_id]
                return float(time)  # Ensure it's a float
            else:
                logger.warning(
                    f"Warning: Node {from_name_id} or {to_name_id} not in travel matrix.")
                return None  # Or handle as an error, e.g., raise ValueError
        except KeyError:
            logger.warning(
                f"KeyError: Node {from_name_id} or {to_name_id} not found in travel matrix.")
            return None
        except ValueError:  # If conversion to float fails for some reason
            logger.warning(
                f"ValueError: Travel time for {from_name_id} to {to_name_id} is not a valid number.")
            return None

    def calculate_flow_total_time(self, flow: PeopleFlow) -> Optional[float]:
        """
        Calculates the total travel time for a given PeopleFlow.

        Args:
            flow (PeopleFlow): The PeopleFlow object.

        Returns:
            Optional[float]: The total travel time for the flow.
                             Returns None if any segment has no travel time.
                             Returns 0.0 for single-node flows.
        """
        if not flow.actual_node_id_sequence:
            return 0.0
        if len(flow.actual_node_id_sequence) == 1:
            return 0.0  # No travel for a single point

        total_time = 0.0
        for i in range(len(flow.actual_node_id_sequence) - 1):
            from_node = flow.actual_node_id_sequence[i]
            to_node = flow.actual_node_id_sequence[i+1]
            segment_time = self.get_travel_time(from_node, to_node)
            if segment_time is None:
                logger.warning(
                    f"Warning: No travel time for segment {from_node} -> {to_node} in flow {flow.identify}.")
                return None  # Or handle this case differently, e.g. infinite time
            total_time += segment_time
        flow.update_total_time(total_time)
        return total_time
</file>

<file path="src/analysis/word_detect.py">
import torch
import logging
from sentence_transformers import SentenceTransformer, util
from src.config import NetworkConfig

logger = logging.getLogger(__name__)

class WordDetect:
    def __init__(self, model: SentenceTransformer = None, config: NetworkConfig = None):
        self.model = model
        self.config = config
        self._initialize_model()

    def _initialize_model(self):
        if not self.model:
            model_name = 'paraphrase-multilingual-MiniLM-L12-v2'
            self.model = SentenceTransformer(model_name)
            logger.info(f'Loaded model: {model_name}')

    def _detect_nearest_word(self, query_word: str, word_list: list[str] = None):
        if word_list is None:
            word_list = self.config.ALL_TYPES
        query_embedding = self.model.encode(query_word, convert_to_tensor=True)
        list_embedding = self.model.encode(word_list, convert_to_tensor=True)
        cosine_scores = util.pytorch_cos_sim(query_embedding, list_embedding)
        max_score_index = torch.argmax(cosine_scores)
        return word_list[max_score_index]

    def detect_nearest_word(self, query_word: str | list[str], word_list: list[str] = None):
        if isinstance(query_word, str):
            return self._detect_nearest_word(query_word, word_list)
        elif isinstance(query_word, list):
            return [self._detect_nearest_word(word, word_list) for word in query_word]
        else:
            raise ValueError(f"Invalid query_word type: {type(query_word)}")
</file>

<file path="src/network/super_network.py">
"""
Manages the construction of a multi-floor network by orchestrating
individual Network instances, potentially in parallel.
"""

import multiprocessing
import os  # For os.cpu_count()
import pathlib
import networkx as nx
import numpy as np
import logging
from typing import List, Dict, Tuple, Optional, Any
from scipy.spatial import KDTree

from src.config import NetworkConfig
from src.graph.node import Node
from .network import Network  # The single-floor network builder
from .floor_manager import FloorManager

logger = logging.getLogger(__name__)

# Worker function for multiprocessing - must be defined at the top-level or picklable


def _process_floor_worker(task_args: Tuple[pathlib.Path, float, int, Dict[str, Any], Dict[Tuple[int, int, int], Dict[str, Any]], bool]) \
        -> Tuple[Optional[nx.Graph], Optional[int], Optional[int], int, pathlib.Path, float]:
    """
    Worker function to process a single floor's network generation.

    Args:
        task_args: A tuple containing:
            - image_path (pathlib.Path): Path to the floor image.
            - z_level (float): Z-coordinate for this floor.
            - id_start_value (int): Starting node ID for this floor.
            - config_dict (Dict): Dictionary representation of NetworkConfig.
            - color_map_data (Dict): The color map.
            - process_outside_nodes (bool): Flag to process outside nodes.

    Returns:
        A tuple containing:
            - graph (Optional[nx.Graph]): Generated graph for the floor, or None on error.
            - width (Optional[int]): Image width, or None on error.
            - height (Optional[int]): Image height, or None on error.
            - next_id_val_from_worker (int): The next available ID from this worker's GraphManager.
            - image_path (pathlib.Path): Original image path (for result matching).
            - z_level (float): Original z_level (for result matching).
    """
    image_path, z_level, id_start_value, config_dict, color_map_data, process_outside_nodes = task_args
    try:
        # Reconstruct config from dict for the worker process
        # Note: This assumes NetworkConfig can be reconstructed from its __dict__
        # and COLOR_MAP is passed directly.
        # A more robust way might be to pass necessary primitive types or use a dedicated
        # config serialization if NetworkConfig becomes very complex.
        # Initialize with color_map
        worker_config = NetworkConfig(color_map_data=color_map_data)
        # Update other attributes from the passed dictionary
        for key, value in config_dict.items():
            # Avoid re-assigning COLOR_MAP
            if key != "COLOR_MAP" and hasattr(worker_config, key):
                setattr(worker_config, key, value)

        network_builder = Network(
            config=worker_config,
            color_map_data=color_map_data,
            id_generator_start_value=id_start_value
        )
        graph, width, height, next_id = network_builder.run(
            image_path=str(image_path),  # network.run expects str path
            z_level=z_level,
            process_outside_nodes=process_outside_nodes
        )
        return graph, width, height, next_id, image_path, z_level
    except Exception as e:
        logger.error(
            f"Error processing floor {image_path.name} in worker: {e}")
        # Return next_id_val as id_start_value + config.ESTIMATED_MAX_NODES_PER_FLOOR
        # to ensure main process ID allocation remains consistent even on worker failure.
        # A more sophisticated error handling might be needed.
        est_next_id = id_start_value + \
            config_dict.get("ESTIMATED_MAX_NODES_PER_FLOOR", 10000)
        return None, None, None, est_next_id, image_path, z_level


class SuperNetwork:
    """
    Orchestrates the creation of a multi-floor network graph.

    It manages multiple Network instances, one for each floor, and combines
    their graphs. It supports parallel processing of floors.
    """

    def __init__(self,
                 config: NetworkConfig,
                 color_map_data: Dict[Tuple[int, int, int], Dict[str, Any]],
                 num_processes: Optional[int] = None,
                 base_floor: int = 0,
                 default_floor_height: Optional[float] = None,
                 vertical_connection_tolerance: Optional[int] = None):
        """
        Initializes the SuperNetwork.

        Args:
            config: The main configuration object.
            color_map_data: The RGB color to type mapping.
            num_processes: Number of processes to use for parallel floor processing.
                           Defaults to os.cpu_count().
            base_floor: Default base floor number if not detected from filename.
            default_floor_height: Default height between floors. Uses config value if None.
            vertical_connection_tolerance: Pixel distance tolerance for connecting
                                           vertical nodes between floors. Uses config if None.
        """
        self.config = config
        self.color_map_data = color_map_data
        self.super_graph: nx.Graph = nx.Graph()
        self.designated_ground_floor_number: Optional[int] = None
        self.designated_ground_floor_z: Optional[float] = None

        self.num_processes: int = num_processes if num_processes is not None else (
            os.cpu_count() or 1)

        _floor_height = default_floor_height if default_floor_height is not None else config.DEFAULT_FLOOR_HEIGHT
        self.floor_manager = FloorManager(
            base_floor_default=base_floor, default_floor_height=_floor_height)

        self.vertical_connection_tolerance: int = vertical_connection_tolerance \
            if vertical_connection_tolerance is not None else config.DEFAULT_VERTICAL_CONNECTION_TOLERANCE

        self.floor_z_map: Dict[int, float] = {}  # floor_number -> z_coordinate
        self.path_to_floor_map: Dict[pathlib.Path,
                                     int] = {}  # image_path -> floor_number

        self.width: Optional[int] = None
        self.height: Optional[int] = None

    def _prepare_floor_data(self, image_file_paths: List[pathlib.Path],
                           z_levels_override: Optional[List[float]] = None) \
            -> List[Tuple[pathlib.Path, float, bool]]:
        """
        Determines floor numbers and Z-levels for each image path.
        Also determines if outside nodes should be processed for that floor.
        Outside nodes are processed ONLY for the designated ground/first floor.

        Returns:
            A list of tuples: (image_path, z_level, process_outside_nodes_flag)
        """
        image_paths_as_pathlib = [pathlib.Path(p) for p in image_file_paths]
        
        self.path_to_floor_map, floor_to_path_map = self.floor_manager.auto_assign_floors(image_paths_as_pathlib)
        
        if z_levels_override and len(z_levels_override) == len(image_paths_as_pathlib):
            # ... (z_levels_override 逻辑保持不变) ...
            sorted_paths_by_floor = sorted(self.path_to_floor_map.keys(), key=lambda p: self.path_to_floor_map[p])
            temp_path_to_z = {path: z for path, z in zip(sorted_paths_by_floor, z_levels_override)} # Make sure this aligns correctly
            self.floor_z_map = {self.path_to_floor_map[p]: temp_path_to_z.get(p) for p in self.path_to_floor_map.keys() if temp_path_to_z.get(p) is not None}

        else:
            self.floor_z_map = self.floor_manager.calculate_z_levels(floor_to_path_map)

        if not self.floor_z_map:
            # logger.error("Could not determine Z-levels for floors.") # Ensure logger is available
            raise ValueError("Could not determine Z-levels for floors.")

        floor_tasks_data = []
        all_floor_nums = list(self.floor_z_map.keys()) # These are the actual floor numbers (e.g., -1, 0, 1, 2)

        if not all_floor_nums:
            return []

        # Determine the ground floor number for processing outside nodes.
        # Strategy: Lowest non-negative floor number. If all are negative, no ground floor with outside.
        # If you have a specific config for ground floor, use that.
        # Example: self.config.GROUND_FLOOR_NUMBER (e.g., 1 or 0)
        
        # Let's find the designated ground floor number.
        # Assuming '1' is the typical first/ground floor if positive floors exist.
        # If '0' exists and is the lowest non-negative, it's the ground floor.
        # Otherwise, if only positive floors, the smallest positive is ground.
        # If only negative floors, then no outside nodes unless specifically handled.
        
        designated_ground_floor_num: Optional[int] = self.config.GROUND_FLOOR_NUMBER_FOR_OUTSIDE
        
        if designated_ground_floor_num is None: # If not set in config, try auto-detection
            positive_or_zero_floors = sorted([fn for fn in all_floor_nums if fn >= 0])
            if 0 in all_floor_nums:
                designated_ground_floor_num = 0
            elif 1 in all_floor_nums and not positive_or_zero_floors : # Check if 1 is the only positive candidate
                 if not any(0 <= fn < 1 for fn in all_floor_nums): # Ensure no 0.x floors if 1 is chosen
                    designated_ground_floor_num = 1
            elif positive_or_zero_floors:
                designated_ground_floor_num = positive_or_zero_floors[0]
        
        # logger.info(f"Designated ground floor for outside nodes: {designated_ground_floor_num}")

        for p_path, floor_num in self.path_to_floor_map.items():
            z_level = self.floor_z_map.get(floor_num)
            if z_level is None:
                # logger.warning(f"Z-level for floor {floor_num} (path {p_path}) not found. Skipping task.")
                continue

            # Process outside nodes ONLY if the current floor is the designated ground floor
            # AND if there's at least one "OUTSIDE_TYPE" defined in config.
            process_outside = False
            if designated_ground_floor_num is not None and floor_num == designated_ground_floor_num \
               and self.config.OUTSIDE_TYPES:
                process_outside = True
            
            # Override via config if a global "always process outside" is set (though less likely now)
            if self.config.DEFAULT_OUTSIDE_PROCESSING_IN_SUPERNETWORK: # This flag might be re-purposed or removed
                # If this flag is true, it might override the ground-floor-only logic.
                # For "only on ground floor", this should typically be false.
                # Let's assume the ground_floor_num logic is primary.
                # So, if DEFAULT_OUTSIDE_PROCESSING_IN_SUPERNETWORK is true, it processes for all.
                # If false (typical for this new requirement), then only for designated_ground_floor_num.
                if self.config.DEFAULT_OUTSIDE_PROCESSING_IN_SUPERNETWORK:
                    process_outside = True 
                # else: it remains as determined by ground_floor_num logic

            # logger.debug(f"Floor task: Path={p_path.name}, FloorNum={floor_num}, Z={z_level:.2f}, ProcessOutside={process_outside}")
            floor_tasks_data.append((p_path, z_level, process_outside))
        
        floor_tasks_data.sort(key=lambda item: item[1]) # Sort by Z-level
        
        self.designated_ground_floor_number = designated_ground_floor_num
        if self.designated_ground_floor_number is not None:
            self.designated_ground_floor_z = self.floor_z_map.get(self.designated_ground_floor_number)

        return floor_tasks_data

    def run(self,
            image_file_paths: List[str],  # List of string paths from main
            z_levels_override: Optional[List[float]] = None,
            force_vertical_tolerance: Optional[int] = None) -> nx.Graph:
        """
        Builds the multi-floor network.

        Args:
            image_file_paths: List of string paths to floor images.
            z_levels_override: Optional list to manually set Z-levels for each image.
                               Order should correspond to sorted floor order or be a path-to-z map.
            force_vertical_tolerance: Optionally override the vertical connection tolerance.

        Returns:
            The combined multi-floor NetworkX graph.
        """
        self.super_graph.clear()  # Clear previous graph if any
        image_paths_pl = [pathlib.Path(p) for p in image_file_paths]

        floor_run_data = self._prepare_floor_data(
            image_paths_pl, z_levels_override)
        if not floor_run_data:
            logger.warning("Warning: No floor data to process.")
            return self.super_graph

        tasks_for_pool = []
        current_id_start = 1
        config_dict_serializable = self.config.__dict__.copy()
        # COLOR_MAP is already part of config_dict_serializable if NetworkConfig init stores it.
        # If COLOR_MAP is global, it's fine for multiprocessing on systems where memory is copied (fork).
        # For spawn, it needs to be picklable or passed. Here, color_map_data is passed.

        for p_path, z_level, process_outside_flag in floor_run_data:
            tasks_for_pool.append((
                p_path, z_level, current_id_start,
                config_dict_serializable, self.color_map_data, process_outside_flag
            ))
            current_id_start += self.config.ESTIMATED_MAX_NODES_PER_FLOOR

        logging.info(
            f"Starting parallel processing of {len(tasks_for_pool)} floors using {self.num_processes} processes...")

        results = []
        # Use with statement for Pool to ensure proper cleanup
        # Only use pool if multiple tasks and processes
        if self.num_processes > 1 and len(tasks_for_pool) > 1:
            with multiprocessing.Pool(processes=self.num_processes) as pool:
                results = pool.map(_process_floor_worker, tasks_for_pool)
        else:  # Run sequentially for single process or single task
            logging.info("Running floor processing sequentially...")
            for task in tasks_for_pool:
                results.append(_process_floor_worker(task))

        first_floor_processed = True
        # To store (graph, width, height) for valid results
        processed_graphs_data = []

        for graph_result, width_res, height_res, _next_id, res_path, res_z in results:
            if graph_result is None or width_res is None or height_res is None:
                logger.warning(
                    f"Warning: Failed to process floor image {res_path.name} (z={res_z}). Skipping.")
                continue

            if first_floor_processed:
                self.width = width_res
                self.height = height_res
                first_floor_processed = False
            elif self.width != width_res or self.height != height_res:
                raise ValueError(
                    f"Image dimensions mismatch for {res_path.name}. "
                    f"Expected ({self.width},{self.height}), got ({width_res},{height_res}). "
                    "All floor images must have the same dimensions."
                )

            processed_graphs_data.append(
                graph_result)  # Store the graph itself

        # Combine graphs
        for floor_graph in processed_graphs_data:
            # Nodes in floor_graph should already have all attributes from Node class
            # and GraphManager.add_node should have added them to nx.Graph.
            # Make sure node objects themselves are added, not just IDs.
            self.super_graph.add_nodes_from(floor_graph.nodes(data=True))
            self.super_graph.add_edges_from(floor_graph.edges(data=True))

        if force_vertical_tolerance is not None:
            self.vertical_connection_tolerance = force_vertical_tolerance
        elif self.config.DEFAULT_VERTICAL_CONNECTION_TOLERANCE == 0:  # 0 might mean auto-calculate
            self.vertical_connection_tolerance = self._auto_calculate_vertical_tolerance()

        self._connect_floors()

        logger.info(
            f"SuperNetwork construction complete. Total nodes: {self.super_graph.number_of_nodes()}")
        return self.super_graph

    def _auto_calculate_vertical_tolerance(self) -> int:
        """
        Automatically calculates a tolerance for connecting vertical nodes
        based on their typical proximity (if not specified).
        """
        vertical_nodes = [
            node for node in self.super_graph.nodes()  # Get node objects
            if isinstance(node, Node) and node.node_type in self.config.VERTICAL_TYPES
        ]
        if not vertical_nodes or len(vertical_nodes) < 2:
            return self.config.DEFAULT_VERTICAL_CONNECTION_TOLERANCE  # Fallback

        # Consider only XY positions for tolerance calculation
        positions_xy = np.array([node.pos[:2] for node in vertical_nodes])
        if len(positions_xy) < 2:
            return self.config.DEFAULT_VERTICAL_CONNECTION_TOLERANCE

        try:
            tree = KDTree(positions_xy)
            # Find distance to the nearest neighbor for each vertical node (excluding itself)
            # k=2 includes self and nearest
            distances, _ = tree.query(positions_xy, k=2)

            # Use distances to the actual nearest neighbor (second column)
            # Filter out zero distances if k=1 was used or if duplicates exist
            # Avoid self-match if k=1
            nearest_distances = distances[:, 1][distances[:, 1] > 1e-6]

            if nearest_distances.size == 0:
                return self.config.DEFAULT_VERTICAL_CONNECTION_TOLERANCE

            avg_min_distance = np.mean(nearest_distances)
            # Tolerance could be a factor of this average minimum distance
            # Example: 50% of avg min distance
            calculated_tolerance = int(avg_min_distance * 0.5)
            logger.info(
                f"Auto-calculated vertical tolerance: {calculated_tolerance} (based on avg_min_dist: {avg_min_distance:.2f})")
            return max(10, calculated_tolerance)  # Ensure a minimum tolerance
        except Exception as e:
            logger.error(
                f"Error in auto-calculating tolerance: {e}. Using default.")
            return self.config.DEFAULT_VERTICAL_CONNECTION_TOLERANCE

    def _connect_floors(self) -> None:
        """
        Connects vertical transport nodes (e.g., stairs, elevators) between
        different floors if they are of the same type and spatially close in XY.
        """
        all_vertical_nodes_in_graph = [
            node for node in self.super_graph.nodes()  # Iterating actual Node objects
            if isinstance(node, Node) and node.node_type in self.config.VERTICAL_TYPES
        ]

        if not all_vertical_nodes_in_graph:
            logger.info("No vertical nodes found to connect between floors.")
            return

        # Group vertical nodes by their specific type (e.g., 'Stairs', 'Elevator')
        nodes_by_type: Dict[str, List[Node]] = {}
        for node in all_vertical_nodes_in_graph:
            nodes_by_type.setdefault(node.node_type, []).append(node)

        logger.info(
            f"Attempting to connect floors. Tolerance: {self.vertical_connection_tolerance} pixels.")
        connected_pairs_count = 0

        for node_type, nodes_of_this_type in nodes_by_type.items():
            if len(nodes_of_this_type) < 2:
                continue  # Not enough nodes of this type to form a connection

            # Sort nodes by Z-level, then by Y, then by X for potentially more stable pairing
            # Though KDTree approach doesn't strictly need pre-sorting.
            nodes_of_this_type.sort(
                key=lambda n: (n.pos[2], n.pos[1], n.pos[0]))

            # Build KDTree for XY positions of nodes of this specific type
            positions_xy = np.array([node.pos[:2]
                                    for node in nodes_of_this_type])
            if positions_xy.shape[0] < 2:
                continue  # Need at least 2 points for KDTree sensible query

            try:
                kdtree = KDTree(positions_xy)
            except Exception as e:
                logger.error(
                    f"Could not build KDTree for vertical node type {node_type}: {e}")
                continue

            processed_nodes_indices = set()  # To avoid redundant checks

            for i, current_node in enumerate(nodes_of_this_type):
                if i in processed_nodes_indices:
                    continue

                # Query for other nodes of the SAME TYPE within the XY tolerance
                # query_ball_point returns indices into the `positions_xy` array
                indices_in_ball = kdtree.query_ball_point(
                    current_node.pos[:2], r=self.vertical_connection_tolerance)

                for neighbor_idx in indices_in_ball:
                    if neighbor_idx == i:  # Don't connect to self
                        continue

                    neighbor_node = nodes_of_this_type[neighbor_idx]

                    # Crucial check: Ensure they are on different floors (Z-levels differ significantly)
                    # Z-levels are too close (same floor)
                    if abs(current_node.pos[2] - neighbor_node.pos[2]) < 1.0:
                        continue

                    # Connect if not already connected
                    if not self.super_graph.has_edge(current_node, neighbor_node):
                        self.super_graph.add_edge(
                            current_node, neighbor_node, type='vertical_connection')
                        connected_pairs_count += 1
                        # Mark both as processed for this type of pairing to avoid re-pairing B with A if A-B done
                        # This might be too aggressive if a node can connect to multiple above/below.
                        # A simpler approach is to just let KDTree find pairs.
                        # The has_edge check prevents duplicate edges.

                processed_nodes_indices.add(i)

        logger.info(
            f"Inter-floor connections made for {connected_pairs_count} pairs of vertical nodes.")
</file>

<file path="src/plotting/plotter.py">
"""
Defines plotter classes for visualizing network graphs using Matplotlib and Plotly.
"""
import abc
import pathlib
import logging
import networkx as nx
import numpy as np
import plotly.graph_objects as go
from typing import Dict, Tuple, Any, List, Optional

from src.config import NetworkConfig  # 依赖配置类
from src.graph.node import Node     # 依赖节点类

logger = logging.getLogger(__name__)


class BasePlotter(abc.ABC):
    """
    Abstract base class for graph plotters.
    """

    def __init__(self,
                 config: NetworkConfig,
                 color_map_data: Dict[Tuple[int, int, int], Dict[str, Any]]):
        """
        Initializes the BasePlotter.

        Args:
            config: The network configuration object.
            color_map_data: The global color map dictionary.
        """
        self.config = config
        self.color_map_data = color_map_data
        self.type_to_plot_color_cache: Dict[str, str] = {}  # 缓存节点类型到绘图颜色的映射

    def _get_node_color(self, node_type: str) -> str:
        """
        Determines the plotting color for a given node type.

        Uses colors from `color_map_data` if `NODE_COLOR_FROM_MAP` is True in config,
        otherwise uses a default Plotly color. Caches results.

        Args:
            node_type: The type of the node (e.g., 'Room', 'Door').

        Returns:
            A string representing the color (e.g., 'rgb(R,G,B)' or a named Plotly color).
        """
        if node_type in self.type_to_plot_color_cache:
            return self.type_to_plot_color_cache[node_type]

        default_plotly_color = '#1f77b4'  # Plotly's default blue

        if self.config.NODE_COLOR_FROM_MAP and self.color_map_data:
            for rgb_tuple, details in self.color_map_data.items():
                if details.get('name') == node_type:
                    color_str = f'rgb{rgb_tuple}'
                    self.type_to_plot_color_cache[node_type] = color_str
                    return color_str

        self.type_to_plot_color_cache[node_type] = default_plotly_color
        return default_plotly_color

    @abc.abstractmethod
    def plot(self,
             graph: nx.Graph,
             output_path: Optional[pathlib.Path] = None,
             title: str = "Network Graph",
             # For Plotly layout, original image width
             graph_width: Optional[int] = None,
             # For Plotly layout, original image height
             graph_height: Optional[int] = None,
             # For SuperNetwork floor labels
             floor_z_map: Optional[Dict[int, float]] = None
             ):
        """
        Abstract method to plot the graph.

        Args:
            graph: The NetworkX graph to plot.
            output_path: Optional path to save the plot. If None, displays the plot.
            title: The title for the plot.
            graph_width: Original width of the (floor plan) image space. Used by Plotly.
            graph_height: Original height of the (floor plan) image space. Used by Plotly.
            floor_z_map: Mapping from floor number to Z-coordinate, for floor slider labels.
        """
        pass


class PlotlyPlotter(BasePlotter):
    """
    Generates interactive 3D network graph visualizations using Plotly.
    """

    def _create_floor_selection_controls(self,
                                         all_z_levels: List[float],
                                         min_z: float, max_z: float,
                                         floor_z_map_for_labels: Optional[Dict[int,
                                                                               float]] = None,
                                         base_floor_for_labels: int = 0
                                         ) -> Dict[str, Any]:
        """
        Creates slider controls for selecting and viewing individual floors or all floors.
        Args:
            all_z_levels: Sorted list of unique Z-coordinates present in the graph.
            min_z: Minimum Z-coordinate.
            max_z: Maximum Z-coordinate.
            floor_z_map_for_labels: Mapping from actual floor number to Z-coordinate.
            base_floor_for_labels: The base floor number for labeling (e.g. 0 for ground, 1 for first).
        """
        if not all_z_levels:
            return {"sliders": []}

        # Create floor labels. Try to map Z-levels back to "human-readable" floor numbers.
        z_to_floor_label_map: Dict[float, str] = {}
        if floor_z_map_for_labels:
            # Invert floor_z_map_for_labels to map z -> floor_num for easier lookup
            # Handle potential multiple floors at the same Z (unlikely with good input)
            z_to_floor_num: Dict[float, List[int]] = {}
            for fn, z_val in floor_z_map_for_labels.items():
                z_to_floor_num.setdefault(z_val, []).append(fn)

            for z_level in all_z_levels:
                floor_nums_at_z = z_to_floor_num.get(z_level)
                if floor_nums_at_z:
                    # If multiple floor numbers map to the same z_level, list them or take first
                    f_num_str = "/".join(map(str, sorted(floor_nums_at_z)))
                    # e.g., F1, F-1/B1
                    z_to_floor_label_map[z_level] = f"F{f_num_str}"
                # Fallback if z_level not in map (should not happen if map is complete)
                else:
                    z_to_floor_label_map[z_level] = f"Z={z_level:.1f}"
        else:  # Fallback if no floor_z_map is provided
            for i, z_level in enumerate(all_z_levels):
                # Attempt simple labeling if base_floor is known
                floor_num_guess = base_floor_for_labels + i  # This is a rough guess
                z_to_floor_label_map[z_level] = f"F{floor_num_guess} (Z={z_level:.1f})"

        slider_steps = []
        for z_level in all_z_levels:
            label = z_to_floor_label_map.get(z_level, f"Z={z_level:.1f}")
            slider_steps.append(dict(
                label=label,
                method="relayout",
                args=[{"scene.zaxis.range": [z_level - self.config.DEFAULT_FLOOR_HEIGHT / 2 + 0.1,
                                             z_level + self.config.DEFAULT_FLOOR_HEIGHT / 2 - 0.1]}]  # View single floor
            ))

        # Add a step to show all floors
        slider_steps.append(dict(
            label="所有楼层",
            method="relayout",
            args=[{"scene.zaxis.range": [min_z - self.config.DEFAULT_FLOOR_HEIGHT * 0.5,
                                         max_z + self.config.DEFAULT_FLOOR_HEIGHT * 0.5]}]  # View all
        ))

        sliders = [dict(
            active=len(all_z_levels),  # Default to "All Floors"
            currentvalue={"prefix": "当前显示: "},
            pad={"t": 50},
            steps=slider_steps,
            name="楼层选择"
        )]
        return {"sliders": sliders}

    def plot(self,
             graph: nx.Graph,
             output_path: Optional[pathlib.Path] = None,
             title: str = "3D Network Graph",
             graph_width: Optional[int] = None,
             graph_height: Optional[int] = None,
             floor_z_map: Optional[Dict[int, float]] = None
             ):
        if not graph.nodes:
            # Ensure logger is defined/imported
            logger.warning("PlotlyPlotter: Graph has no nodes to plot.")
            return

        node_traces = []
        edge_traces = []  # Renamed from edge_trace to edge_traces as it's a list

        nodes_data_by_type: Dict[str, Dict[str, list]] = {}
        all_node_objects = [data.get('node_obj', node_id)
                            for node_id, data in graph.nodes(data=True)]
        all_node_objects = [n for n in all_node_objects if isinstance(n, Node)]

        if not all_node_objects:
            logger.warning(
                "PlotlyPlotter: No Node objects found in graph nodes. Cannot plot.")
            return

        all_z_coords_present = sorted(
            list(set(n.pos[2] for n in all_node_objects)))
        min_z = min(all_z_coords_present) if all_z_coords_present else 0
        max_z = max(all_z_coords_present) if all_z_coords_present else 0

        for node_obj in all_node_objects:
            node_type = node_obj.node_type
            if node_type not in nodes_data_by_type:
                nodes_data_by_type[node_type] = {
                    'x': [], 'y': [], 'z': [],
                    'visible_text': [],  # For text always visible next to node
                    'hover_text': [],   # For text visible on hover
                    'sizes': [],
                    'ids': []
                }

            x, y, z = node_obj.pos
            plot_x = (
                graph_width - x) if self.config.IMAGE_MIRROR and graph_width is not None else x

            nodes_data_by_type[node_type]['x'].append(plot_x)
            nodes_data_by_type[node_type]['y'].append(y)
            nodes_data_by_type[node_type]['z'].append(z)
            nodes_data_by_type[node_type]['ids'].append(node_obj.id)

            # --- Text Configuration ---
            # 1. Visible text (always shown next to the marker if mode includes 'text')
            #    Only show node_type if SHOW_PEDESTRIAN_LABELS is True or it's not a pedestrian node.
            #    Otherwise, show empty string to hide permanent text for certain types.
            is_ped_type = node_type in self.config.PEDESTRIAN_TYPES
            can_show_permanent_label = not is_ped_type or self.config.SHOW_PEDESTRIAN_LABELS

            nodes_data_by_type[node_type]['visible_text'].append(
                node_type if can_show_permanent_label else "")

            # 2. Hover text (always detailed)
            hover_label = (
                f"ID: {node_obj.id}<br>"
                f"Type: {node_type}<br>"
                f"Pos: ({x},{y},{z})<br>"
                f"Time: {node_obj.time:.2f}<br>"
                f"Area: {node_obj.area:.2f}"
            )
            if node_obj.door_type:
                hover_label += f"<br>Door: {node_obj.door_type}"
            nodes_data_by_type[node_type]['hover_text'].append(hover_label)

            # Node size
            size = self.config.NODE_SIZE_DEFAULT
            if is_ped_type:
                size = self.config.NODE_SIZE_PEDESTRIAN
            elif node_type in self.config.CONNECTION_TYPES:
                size = self.config.NODE_SIZE_CONNECTION
            elif node_type in self.config.VERTICAL_TYPES:
                size = self.config.NODE_SIZE_VERTICAL
            elif node_type in self.config.ROOM_TYPES:
                size = self.config.NODE_SIZE_ROOM
            elif node_type in self.config.OUTSIDE_TYPES:
                size = self.config.NODE_SIZE_OUTSIDE
            nodes_data_by_type[node_type]['sizes'].append(size)

        for node_type, data in nodes_data_by_type.items():
            if not data['x']:
                continue

            # Determine mode: if all 'visible_text' for this type are empty, just use 'markers'
            # Otherwise, use 'markers+text' to show the type.
            current_mode = 'markers'
            # Check if any visible text is non-empty
            if any(vt for vt in data['visible_text']):
                current_mode = 'markers+text'

            # If SHOW_PEDESTRIAN_LABELS is False and it's a pedestrian type, override to 'markers'
            if node_type in self.config.PEDESTRIAN_TYPES and not self.config.SHOW_PEDESTRIAN_LABELS:
                current_mode = 'markers'

            node_trace = go.Scatter3d(
                x=data['x'], y=data['y'], z=data['z'],
                mode=current_mode,  # Dynamically set mode
                marker=dict(
                    size=data['sizes'],
                    sizemode='diameter',  # This should make size in screen pixels
                    color=self._get_node_color(node_type),
                    opacity=self.config.NODE_OPACITY,
                    line=dict(width=1, color='DarkSlateGrey')
                ),
                # Text to display next to markers if mode includes 'text'
                text=data['visible_text'],
                hovertext=data['hover_text'],  # Text for hover box
                # Use 'text' from hovertext (Plotly default is 'all')
                hoverinfo='text',
                # if hovertext is set, hoverinfo='text' uses hovertext.
                # if hovertext is not set, hoverinfo='text' uses the 'text' property.
                name=node_type,
                customdata=data['ids'],
                textposition="top center",
                textfont=dict(  # Optional: style the permanently visible text
                    size=9,  # Smaller font for permanent labels
                    # color='black'
                )
            )
            node_traces.append(node_trace)

        # --- Prepare Edge Data (remains largely the same) ---
        edge_x_horiz, edge_y_horiz, edge_z_horiz = [], [], []
        edge_x_vert, edge_y_vert, edge_z_vert = [], [], []

        for edge_start_node, edge_end_node in graph.edges():
            if not (isinstance(edge_start_node, Node) and isinstance(edge_end_node, Node)):
                continue

            x0, y0, z0 = edge_start_node.pos
            x1, y1, z1 = edge_end_node.pos
            plot_x0 = (
                graph_width - x0) if self.config.IMAGE_MIRROR and graph_width is not None else x0
            plot_x1 = (
                graph_width - x1) if self.config.IMAGE_MIRROR and graph_width is not None else x1

            if abs(z0 - z1) < 0.1:
                edge_x_horiz.extend([plot_x0, plot_x1, None])
                edge_y_horiz.extend([y0, y1, None])
                edge_z_horiz.extend([z0, z1, None])
            else:
                edge_x_vert.extend([plot_x0, plot_x1, None])
                edge_y_vert.extend([y0, y1, None])
                edge_z_vert.extend([z0, z1, None])

        if edge_x_horiz:
            edge_traces.append(go.Scatter3d(
                x=edge_x_horiz, y=edge_y_horiz, z=edge_z_horiz,
                mode='lines',
                line=dict(color=self.config.HORIZONTAL_EDGE_COLOR,
                          width=self.config.EDGE_WIDTH),
                hoverinfo='none', name='水平连接'
            ))
        if edge_x_vert:
            edge_traces.append(go.Scatter3d(
                x=edge_x_vert, y=edge_y_vert, z=edge_z_vert,
                mode='lines',
                line=dict(color=self.config.VERTICAL_EDGE_COLOR,
                          width=self.config.EDGE_WIDTH),
                hoverinfo='none', name='垂直连接'
            ))

        # --- Layout and Figure (remains largely the same) ---
        layout = go.Layout(
            title=title,
            showlegend=True,
            hovermode='closest',  # Important for hover behavior
            margin=dict(b=20, l=5, r=5, t=40),
            scene=dict(
                xaxis=dict(
                    title='X', autorange='reversed' if self.config.IMAGE_MIRROR else True),
                yaxis=dict(
                    title='Y',
                    autorange='reversed', # 反转Y轴
                ),
                zaxis=dict(title='Z (楼层)', range=[min_z - 1, max_z + 1]),
                aspectmode='data',  # 'data' is often good for spatial data
                camera=dict(eye=dict(x=1.25, y=1.25, z=1.25))
            ),
            legend=dict(
                orientation="v",    # 垂直排列
                x=0.02,             # X 位置 (靠近左边缘)
                y=1.0,              # Y 位置 (靠近顶部)
                xanchor="left",     # X 锚点
                yanchor="top",      # Y 锚点
                bgcolor="rgba(255, 255, 255, 0.7)", # 可选：浅色背景提高可读性
                bordercolor="rgba(120, 120, 120, 0.7)", # 可选：边框颜色
                borderwidth=1         # 可选：边框宽度
            )
        )

        if len(all_z_coords_present) > 1:
            floor_controls = self._create_floor_selection_controls(
                all_z_coords_present, min_z, max_z, floor_z_map)
            layout.update(floor_controls)

        fig = go.Figure(data=node_traces + edge_traces, layout=layout)

        if output_path:
            output_path.parent.mkdir(parents=True, exist_ok=True)
            fig.write_html(str(output_path))
            # Ensure logger
            logger.info(f"Plotly graph saved to {output_path}")
        else:
            fig.show()
</file>

<file path="src/rl_optimizer/data/cache/node_variants.json">
{
  "ICU": [
    "ICU_30013"
  ],
  "NICU": [
    "NICU_30012"
  ],
  "中医科": [
    "中医科_10007"
  ],
  "中心供应室": [
    "中心供应室_10003"
  ],
  "产前诊断门诊": [
    "产前诊断门诊_30009"
  ],
  "产科": [
    "产科_30006"
  ],
  "介入科": [
    "介入科_20013"
  ],
  "体检科": [
    "体检科_10009"
  ],
  "儿科": [
    "儿科_10006"
  ],
  "全科": [
    "全科_10004"
  ],
  "内诊药房": [
    "内诊药房_10001"
  ],
  "内镜中心": [
    "内镜中心_20002"
  ],
  "口腔一区": [
    "口腔一区_40004"
  ],
  "口腔科二区": [
    "口腔科二区_40008"
  ],
  "呼吸内科": [
    "呼吸内科_20011"
  ],
  "妇科": [
    "妇科_30005"
  ],
  "心血管内科": [
    "心血管内科_20006"
  ],
  "急诊科": [
    "急诊科_1"
  ],
  "手术室": [
    "手术室_30007",
    "手术室_40006"
  ],
  "挂号收费": [
    "挂号收费_10002",
    "挂号收费_20001",
    "挂号收费_30001",
    "挂号收费_40001"
  ],
  "放射科": [
    "放射科_10005"
  ],
  "检验中心": [
    "检验中心_20003"
  ],
  "泌尿外科": [
    "泌尿外科_30003"
  ],
  "消化内科": [
    "消化内科_20004"
  ],
  "烧伤整形科": [
    "烧伤整形科_30011"
  ],
  "生殖医学科": [
    "生殖医学科_30010"
  ],
  "甲状腺外科": [
    "甲状腺外科_30002"
  ],
  "病理科": [
    "病理科_20008"
  ],
  "皮肤科": [
    "皮肤科_40005"
  ],
  "眼科": [
    "眼科_40002"
  ],
  "神经内科": [
    "神经内科_20010"
  ],
  "综合激光科": [
    "综合激光科_40009"
  ],
  "耳鼻喉科": [
    "耳鼻喉科_40003"
  ],
  "肝胆胰外科": [
    "肝胆胰外科_30004"
  ],
  "肾内科": [
    "肾内科_20005"
  ],
  "肿瘤科": [
    "肿瘤科_20012"
  ],
  "超声科": [
    "超声科_10008"
  ],
  "透析中心": [
    "透析中心_30008"
  ],
  "采血处": [
    "采血处_20007"
  ],
  "门": [
    "门_11072",
    "门_11083",
    "门_11086",
    "门_11087",
    "门_11093",
    "门_11094",
    "门_11112",
    "门_11113",
    "门_11116",
    "门_11118",
    "门_11119",
    "门_11122",
    "门_11145",
    "门_11146",
    "门_11147",
    "门_11149",
    "门_11153",
    "门_11154",
    "门_11155",
    "门_11165"
  ],
  "门诊手术室": [
    "门诊手术室_40007"
  ],
  "静配中心": [
    "静配中心_40010"
  ],
  "骨科": [
    "骨科_20009"
  ]
}
</file>

<file path="src/rl_optimizer/data/process_templates.json">
[
    {
        "process_id": "STD_CHECKUP",
        "description": "全科流程",
        "core_sequence": ["全科", "采血处", "心血管内科", "全科", "药房"],
        "start_nodes": ["门"],
        "end_nodes": ["门"],
        "base_weight": 1.0
    },
    {
        "process_id": "EMERGENCY_TRAUMA",
        "description": "体检流程",
        "core_sequence": ["体检科", "采血处", "检验中心", "放射科", "心血管内科", "超声科", "体检科"],
        "start_nodes": ["门"],
        "end_nodes": ["门"],
        "base_weight": 1.0
    },
    {
        "process_id": "GYN_ROUTINE",
        "description": "儿科",
        "core_sequence": ["儿科", "药房"],
        "start_nodes": ["门"],
        "end_nodes": ["门"],
        "base_weight": 1.0
    },
    {
        "process_id": "GYN_ROUTINE",
        "description": "中医科",
        "core_sequence": ["中医科", "药房"],
        "start_nodes": ["门"],
        "end_nodes": ["门"],
        "base_weight": 1.0
    },
    {
        "process_id": "STD_CHECKUP",
        "description": "呼吸科",
        "core_sequence": ["呼吸科", "采血处", "放射科", "呼吸科", "药房"],
        "start_nodes": ["门"],
        "end_nodes": ["门"],
        "base_weight": 1.0
    },
    {
        "process_id": "STD_CHECKUP",
        "description": "肾内科",
        "core_sequence": ["肾内科", "采血处", "检验中心", "超声科", "肾内科", "药房"],
        "start_nodes": ["门"],
        "end_nodes": ["门"],
        "base_weight": 1.0
    },
    {
        "process_id": "STD_CHECKUP",
        "description": "肾透析",
        "core_sequence": ["肾内科", "透析中心", "药房"],
        "start_nodes": ["门"],
        "end_nodes": ["门"],
        "base_weight": 1.0
    },
    {
        "process_id": "STD_CHECKUP",
        "description": "骨科",
        "core_sequence": ["骨科", "采血处", "放射科", "骨科", "药房"],
        "start_nodes": ["门"],
        "end_nodes": ["门"],
        "base_weight": 1.0
    },
    {
        "process_id": "STD_CHECKUP",
        "description": "心血管内科",
        "core_sequence": ["心血管内科", "放射科", "心血管内科", "药房"],
        "start_nodes": ["门"],
        "end_nodes": ["门"],
        "base_weight": 1.0
    },
    {
        "process_id": "STD_CHECKUP",
        "description": "神经科",
        "core_sequence": ["神经科", "采血处", "放射科", "神经科", "药房"],
        "start_nodes": ["门"],
        "end_nodes": ["门"],
        "base_weight": 1.0
    },
    {
        "process_id": "STD_CHECKUP",
        "description": "消化内科",
        "core_sequence": ["消化内科", "采血处", "检验中心", "消化内科", "内镜中心", "药房"],
        "start_nodes": ["门"],
        "end_nodes": ["门"],
        "base_weight": 1.0
    },
    {
        "process_id": "STD_CHECKUP",
        "description": "内镜中心",
        "core_sequence": ["内镜中心", "消化内科", "药房"],
        "start_nodes": ["门"],
        "end_nodes": ["门"],
        "base_weight": 1.0
    },
    {
        "process_id": "STD_CHECKUP",
        "description": "肿瘤科",
        "core_sequence": ["肿瘤科", "采血处", "放射科", "病理科", "肿瘤科", "药房"],
        "start_nodes": ["门"],
        "end_nodes": ["门"],
        "base_weight": 1.0
    },
    {
        "process_id": "STD_CHECKUP",
        "description": "泌尿外科",
        "core_sequence": ["泌尿外科", "采血处", "检验中心", "超声科", "泌尿外科", "药房"],
        "start_nodes": ["门"],
        "end_nodes": ["门"],
        "base_weight": 1.0
    },
    {
        "process_id": "STD_CHECKUP",
        "description": "烧伤整形科",
        "core_sequence": ["烧伤整形科", "药房"],
        "start_nodes": ["门"],
        "end_nodes": ["门"],
        "base_weight": 1.0
    },
    {
        "process_id": "STD_CHECKUP",
        "description": "甲状腺外科",
        "core_sequence": ["甲状腺外科", "采血处", "超声科", "甲状腺外科", "药房"],
        "start_nodes": ["门"],
        "end_nodes": ["门"],
        "base_weight": 1.0
    },
    {
        "process_id": "STD_CHECKUP",
        "description": "肝胆胰外科",
        "core_sequence": ["肝胆胰外科", "放射科", "肝胆胰外科", "内镜中心", "药房"],
        "start_nodes": ["门"],
        "end_nodes": ["门"],
        "base_weight": 1.0
    },
    {
        "process_id": "STD_CHECKUP",
        "description": "肝胆内镜",
        "core_sequence": ["内镜中心", "肝胆胰外科", "药房"],
        "start_nodes": ["门"],
        "end_nodes": ["门"],
        "base_weight": 1.0
    },
    {
        "process_id": "STD_CHECKUP",
        "description": "生殖医学科",
        "core_sequence": ["生殖医学科", "检验中心", "生殖医学科", "药房"],
        "start_nodes": ["门"],
        "end_nodes": ["门"],
        "base_weight": 1.0
    },
    {
        "process_id": "STD_CHECKUP",
        "description": "产前诊断门诊",
        "core_sequence": ["产前诊断门诊", "检验中心", "超声科", "产前诊断门诊", "药房"],
        "start_nodes": ["门"],
        "end_nodes": ["门"],
        "base_weight": 1.0
    },
    {
        "process_id": "STD_CHECKUP",
        "description": "产科",
        "core_sequence": ["产科", "采血处", "检验中心", "超声科", "产科", "药房"],
        "start_nodes": ["门"],
        "end_nodes": ["门"],
        "base_weight": 1.0
    },
    {
        "process_id": "STD_CHECKUP",
        "description": "妇科",
        "core_sequence": ["妇科", "采血处", "检验中心", "超声科", "妇科", "药房"],
        "start_nodes": ["门"],
        "end_nodes": ["门"],
        "base_weight": 1.0
    },
    {
        "process_id": "STD_CHECKUP",
        "description": "皮肤科",
        "core_sequence": ["皮肤科", "药房"],
        "start_nodes": ["门"],
        "end_nodes": ["门"],
        "base_weight": 1.0
    },
    {
        "process_id": "STD_CHECKUP",
        "description": "综合激光科",
        "core_sequence": ["综合激光科", "药房"],
        "start_nodes": ["门"],
        "end_nodes": ["门"],
        "base_weight": 1.0
    },
    {
        "process_id": "STD_CHECKUP",
        "description": "眼科",
        "core_sequence": ["眼科", "药房"],
        "start_nodes": ["门"],
        "end_nodes": ["门"],
        "base_weight": 1.0
    },
    {
        "process_id": "STD_CHECKUP",
        "description": "耳鼻喉科",
        "core_sequence": ["耳鼻喉科", "放射科", "耳鼻喉科", "药房"],
        "start_nodes": ["门"],
        "end_nodes": ["门"],
        "base_weight": 1.0
    },
    {
        "process_id": "STD_CHECKUP",
        "description": "口腔一区",
        "core_sequence": ["口腔一区", "放射科", "口腔一区", "药房"],
        "start_nodes": ["门"],
        "end_nodes": ["门"],
        "base_weight": 1.0
    },
    {
        "process_id": "STD_CHECKUP",
        "description": "口腔科二区",
        "core_sequence": ["口腔科二区", "放射科", "口腔科二区", "药房"],
        "start_nodes": ["门"],
        "end_nodes": ["门"],
        "base_weight": 1.0
    },
    {
        "process_id": "STD_CHECKUP",
        "description": "急诊科",
        "core_sequence": ["急诊科", "超声科", "放射科", "急诊科", "药房"],
        "start_nodes": ["门"],
        "end_nodes": ["门"],
        "base_weight": 1.0
    }
]
</file>

<file path="src/rl_optimizer/env/vec_env_wrapper.py">
# src/rl_optimizer/env/vec_env_wrapper.py

from typing import Any, Dict, List, Optional, Tuple
import numpy as np
from stable_baselines3.common.vec_env import VecEnvWrapper
from stable_baselines3.common.vec_env.base_vec_env import VecEnvStepReturn
import multiprocessing

from src.rl_optimizer.utils.setup import setup_logger

logger = setup_logger(__name__)


class EpisodeInfoVecEnvWrapper(VecEnvWrapper):
    """
    VecEnv包装器，确保episode信息能正确传递给回调函数。
    
    解决SB3中VecEnv环境episode信息传递的问题，特别是自定义info字典的传递。
    """
    
    def __init__(self, venv):
        """
        初始化包装器
        
        Args:
            venv: 要包装的VecEnv
        """
        super().__init__(venv)
        self.episode_rewards = np.zeros(self.num_envs)
        self.episode_lengths = np.zeros(self.num_envs, dtype=int)
        self.episode_count = 0
        
        # 检查是否为主进程，只有主进程才输出详细日志
        try:
            self.is_main_process = multiprocessing.current_process().name == 'MainProcess'
        except:
            self.is_main_process = True
        
    def step_wait(self) -> VecEnvStepReturn:
        """
        等待并处理环境步骤结果，确保episode信息正确传递
        """
        observations, rewards, dones, infos = self.venv.step_wait()
        
        # 更新episode统计
        self.episode_rewards += rewards
        self.episode_lengths += 1
        
        # 处理episode结束
        for i, (done, info) in enumerate(zip(dones, infos)):
            if done:
                self.episode_count += 1
                
                # 如果环境提供了episode信息，确保它能被正确传递
                if 'episode' in info:
                    episode_info = info['episode'].copy()
                    
                    # 添加标准的SB3 episode信息字段
                    episode_info['r'] = float(self.episode_rewards[i])
                    episode_info['l'] = int(self.episode_lengths[i])
                    
                    # 确保episode信息在正确的位置
                    info['episode'] = episode_info
                    
                    # 只有主进程输出调试日志，且仅在真正需要时输出
                    if self.is_main_process and logger.isEnabledFor(10):  # DEBUG级别
                        logger.debug(f"环境{i} Episode {self.episode_count}结束")
                        if 'time_cost' in episode_info:
                            logger.debug(f"时间成本: {episode_info['time_cost']:.2f}")
                
                # 重置统计
                self.episode_rewards[i] = 0
                self.episode_lengths[i] = 0
        
        return observations, rewards, dones, infos
    
    def reset(self) -> np.ndarray:
        """重置环境"""
        self.episode_rewards.fill(0)
        self.episode_lengths.fill(0)
        return self.venv.reset()
</file>

<file path="src/rl_optimizer/model/policy_network.py">
# src/rl_optimizer/model/policy_network.py

import torch
import torch.nn as nn
from gymnasium import spaces
from stable_baselines3.common.torch_layers import BaseFeaturesExtractor
from typing import Dict

from src.config import RLConfig
from src.rl_optimizer.utils.setup import setup_logger

logger = setup_logger(__name__)

class LayoutTransformer(BaseFeaturesExtractor):
    """
    基于Transformer的特征提取器，用于处理布局状态。

    该网络接收环境的字典式观测，通过嵌入层和Transformer编码器，
    学习布局中已放置科室与待放置科室之间的复杂空间和逻辑关系，
    最终为策略网络（Actor-Critic）输出一个固定维度的特征向量。

    该设计特别适用于自回归构建任务，因为它能有效地处理序列信息
    和元素之间的长程依赖。
    """

    def __init__(self, observation_space: spaces.Dict, features_dim: int, config: RLConfig):
        """
        初始化Transformer特征提取器。

        Args:
            observation_space (spaces.Dict): 环境的观测空间。
            features_dim (int): 输出特征的维度。
            config (RLConfig): RL优化器的配置对象。
        """
        super().__init__(observation_space, features_dim)

        self.config = config

        # 从观测空间中提取维度信息
        # 槽位数量，决定了序列长度
        num_slots = observation_space["layout"].shape[0]
        
        # 科室种类数量 (包括0，代表"未放置"或"空")
        # 支持两种观测空间定义方式
        if hasattr(observation_space["layout"], 'nvec'):
            # MultiDiscrete 空间
            num_depts = int(observation_space["layout"].nvec[0])
        else:
            # Box 空间
            num_depts = int(observation_space["layout"].high[0]) + 1 # high是最大值，种类数是最大值+1

        embedding_dim = self.config.EMBEDDING_DIM

        # --- 网络层定义 ---
        
        # 1. 科室嵌入层 (Dept Embedding)
        # 将每个科室ID (包括0) 映射为一个高维向量
        self.dept_embedding = nn.Embedding(num_embeddings=num_depts, embedding_dim=embedding_dim)

        # 2. 槽位位置嵌入层 (Slot Positional Embedding)
        # 为每个槽位（位置）学习一个唯一的嵌入向量，以区分位置信息
        self.slot_position_embedding = nn.Embedding(num_embeddings=num_slots, embedding_dim=embedding_dim)

        # 4. Transformer 编码器层
        # 这是网络的核心，用于处理序列信息
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=embedding_dim,
            nhead=self.config.TRANSFORMER_HEADS,
            dim_feedforward=embedding_dim * 4,
            dropout=0.1,
            activation='gelu',
            batch_first=True  # 确保输入张量的维度顺序为 (batch, sequence, features)
        )
        self.transformer_encoder = nn.TransformerEncoder(
            encoder_layer, 
            num_layers=self.config.TRANSFORMER_LAYERS
        )

        # 5. 输出线性层 (Output Layer)
        # 将处理后的特征与当前待决策槽位的特征拼接，然后映射到最终的特征维度
        self.linear = nn.Sequential(
            nn.LayerNorm(embedding_dim * num_slots + embedding_dim),
            nn.Linear(embedding_dim * num_slots + embedding_dim, features_dim),
            nn.ReLU()
        )

        logger.info(f"LayoutTransformer 初始化成功。")
        logger.info(f"  - 槽位数量: {num_slots}")
        logger.info(f"  - 科室种类数量: {num_depts}")
        logger.info(f"  - 嵌入维度: {embedding_dim}")

    def forward(self, observations: Dict[str, torch.Tensor]) -> torch.Tensor:
        """
        定义模型的前向传播逻辑。

        Args:
            observations (Dict[str, torch.Tensor]): 从环境中获得的观测数据字典。
                - "layout": (batch_size, num_slots) 当前布局，值为科室ID
                - "current_slot_idx": (batch_size, 1) 当前待填充的槽位索引

        Returns:
            torch.Tensor: 提取出的状态特征，维度为 (batch_size, features_dim)。
        """
        # --- 1. 准备输入嵌入 ---
        layout_ids = observations["layout"]
        current_slot_idx = observations["current_slot_idx"].squeeze(-1)

        # 确保数据类型正确：嵌入层需要整数类型
        layout_ids = layout_ids.long()  # 转换为 LongTensor
        current_slot_idx = current_slot_idx.long()  # 转换为 LongTensor

        batch_size, num_slots = layout_ids.shape
        device = layout_ids.device

        # 获取科室嵌入
        dept_embeds = self.dept_embedding(layout_ids) # (B, num_slots, D_emb)

        # 获取槽位位置嵌入
        slot_positions = torch.arange(0, num_slots, device=device).unsqueeze(0).expand(batch_size, -1)
        slot_pos_embeds = self.slot_position_embedding(slot_positions) # (B, num_slots, D_emb)

        # 组合输入嵌入 (这是Transformer的标准做法)
        input_embeds = dept_embeds + slot_pos_embeds

        # --- 2. 通过 Transformer 处理布局信息 ---
        # Transformer的输出包含了每个槽位位置的上下文感知特征
        transformer_output = self.transformer_encoder(input_embeds) # (B, num_slots, D_emb)

        # 将所有槽位的特征展平，形成一个代表整个布局的向量
        flattened_layout_features = transformer_output.reshape(batch_size, -1) # (B, num_slots * D_emb)

        # --- 3. 整合当前待决策槽位的信息 ---
        # 获取当前待决策槽位的嵌入
        current_slot_embed = self.slot_position_embedding(current_slot_idx) # (B, D_emb)

        # --- 4. 拼接并输出最终特征 ---
        # 将布局特征和当前科室特征拼接在一起
        combined_features = torch.cat([flattened_layout_features, current_slot_embed], dim=1)
        
        # 通过最后的线性层得到最终的特征向量
        final_features = self.linear(combined_features)
        
        return final_features
</file>

<file path="src/rl_optimizer/utils/setup.py">
# src/rl_optimizer/utils/setup.py

import logging
import sys
from typing import Any, Optional
import json
import pathlib
import pickle
import numpy as np

class NpEncoder(json.JSONEncoder):
    """自定义JSON编码器，以处理Numpy数据类型和路径对象。"""
    def default(self, obj: Any) -> Any:
        if isinstance(obj, np.integer):
            return int(obj)
        if isinstance(obj, np.floating):
            return float(obj)
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        if isinstance(obj, (pathlib.Path, pathlib.PosixPath, pathlib.WindowsPath)):
            return str(obj)
        return super(NpEncoder, self).default(obj)
    
def setup_logger(name: str, log_file: Optional[pathlib.Path] = None, level: int = logging.INFO) -> logging.Logger:
    """配置并返回一个标准化的日志记录器。

    Args:
        name (str): 日志记录器的名称。
        log_file (Optional[pathlib.Path]): 可选的日志文件路径。
        level (int): 日志级别，默认为INFO。

    Returns:
        logging.Logger: 配置好的日志记录器实例。
    """
    import os
    import multiprocessing
    
    logger = logging.getLogger(name)
    logger.setLevel(level)
    
    # 防止日志向上级传播，避免重复输出
    logger.propagate = False
    
    # 检查是否已经配置过handler，避免重复添加
    if logger.handlers:
        return logger
    
    # 在多进程环境中，只有主进程输出详细日志
    # 这里通过检查进程名来判断是否为主进程
    is_main_process = True
    try:
        # 检查是否在多进程环境中
        current_process = multiprocessing.current_process()
        if current_process.name != 'MainProcess':
            is_main_process = False
            # 子进程只使用WARNING级别以上的日志
            logger.setLevel(logging.WARNING)
    except:
        # 如果无法确定进程信息，默认为主进程
        pass
    
    # 创建formatter
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    
    # 控制台处理器
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setFormatter(formatter)
    console_handler.setLevel(level if is_main_process else logging.WARNING)
    # 给handler添加唯一标识，防止重复添加
    console_handler.set_name(f"console_{name}")
    logger.addHandler(console_handler)
    
    # 文件处理器（只在主进程中添加，避免文件冲突）
    if log_file and is_main_process:
        log_file.parent.mkdir(parents=True, exist_ok=True)
        file_handler = logging.FileHandler(log_file, mode='a', encoding='utf-8')
        file_handler.setFormatter(formatter)
        file_handler.setLevel(level)
        file_handler.set_name(f"file_{name}")
        logger.addHandler(file_handler)
    
    return logger

def save_json(data: dict, path: pathlib.Path):
    """使用自定义编码器将字典保存为JSON文件。"""
    with open(path, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2, cls=NpEncoder)

def load_json(path: pathlib.Path) -> dict:
    """从JSON文件加载字典。"""
    with open(path, 'r', encoding='utf-8') as f:
        return json.load(f)
    
def save_pickle(data: Any, path: pathlib.Path):
    """将任何Python对象序列化为pickle文件。"""
    with open(path, 'wb') as f:
        pickle.dump(data, f)

def load_pickle(path: pathlib.Path) -> Any:
    """从pickle文件加载Python对象。"""
    with open(path, 'rb') as f:
        return pickle.load(f)
</file>

<file path="src/algorithms/ppo_optimizer.py">
"""
PPO优化器 - 基于强化学习的布局优化算法
"""

import torch
import time
import numpy as np
from pathlib import Path
from typing import List, Optional, Dict, Any
from stable_baselines3.common.vec_env import VecNormalize
from stable_baselines3.common.env_util import make_vec_env
from sb3_contrib.common.maskable.callbacks import MaskableEvalCallback as EvalCallback
from sb3_contrib import MaskablePPO
from sb3_contrib.common.wrappers import ActionMasker
from sb3_contrib.common.maskable.policies import MaskableActorCriticPolicy

from src.algorithms.base_optimizer import BaseOptimizer, OptimizationResult
from src.algorithms.constraint_manager import ConstraintManager
from src.rl_optimizer.env.cost_calculator import CostCalculator
from src.rl_optimizer.env.layout_env import LayoutEnv
from src.rl_optimizer.env.vec_env_wrapper import EpisodeInfoVecEnvWrapper
from src.rl_optimizer.model.policy_network import LayoutTransformer
from src.rl_optimizer.utils.setup import setup_logger, save_json
from src.rl_optimizer.utils.lr_scheduler import get_lr_scheduler
from src.rl_optimizer.utils.checkpoint_callback import CheckpointCallback
from src.rl_optimizer.data.cache_manager import CacheManager
from src.config import RLConfig

logger = setup_logger(__name__)


def get_action_mask_from_info(infos: List[Dict]) -> np.ndarray:
    """从矢量化环境的info字典列表中提取动作掩码"""
    return np.array([info.get("action_mask", []) for info in infos])


class PPOOptimizer(BaseOptimizer):
    """
    PPO优化器
    
    基于强化学习的PPO算法实现布局优化，使用MaskablePPO来处理动作掩码约束。
    """
    
    def __init__(self, 
                 cost_calculator: CostCalculator,
                 constraint_manager: ConstraintManager,
                 config: RLConfig,
                 cache_manager: CacheManager):
        """
        初始化PPO优化器
        
        Args:
            cost_calculator: 成本计算器
            constraint_manager: 约束管理器
            config: RL配置
            cache_manager: 缓存管理器
        """
        super().__init__(cost_calculator, constraint_manager, "PPO")
        self.config = config
        self.cache_manager = cache_manager
        
        # 环境参数
        self.env_kwargs = {
            "config": self.config,
            "cache_manager": self.cache_manager,
            "cost_calculator": self.cost_calculator
        }
        
        # 训练状态
        self.model = None
        self.vec_env = None
        self.resume_model_path = None
        self.completed_steps = 0
    
    def optimize(self, 
                 initial_layout: Optional[List[str]] = None,
                 max_iterations: int = None,
                 total_timesteps: int = None,
                 **kwargs) -> OptimizationResult:
        """
        执行PPO优化
        
        Args:
            initial_layout: 初始布局（PPO会自动探索）
            max_iterations: 最大迭代次数（使用total_timesteps代替）
            total_timesteps: 总训练步数
            **kwargs: 其他PPO参数
            
        Returns:
            OptimizationResult: 优化结果
        """
        self.start_optimization()
        
        # 使用配置中的参数或传入的参数
        if total_timesteps is None:
            total_timesteps = self.config.TOTAL_TIMESTEPS
        
        logger.info(f"开始PPO优化，总训练步数: {total_timesteps}")
        
        try:
            # 检查是否需要恢复训练
            self._check_for_resume()
            
            # 计算剩余训练步数
            remaining_steps = max(0, total_timesteps - self.completed_steps)
            if remaining_steps == 0:
                logger.info("训练已完成，加载最佳模型进行评估")
                best_layout, best_cost = self._evaluate_best_model()
                self.update_best_solution(best_layout, best_cost)
                return self.finish_optimization()
            
            # 创建环境和模型
            self._setup_environment_and_model(remaining_steps)
            
            # 执行训练
            self._train_model(remaining_steps)
            
            # 评估最佳模型
            best_layout, best_cost = self._evaluate_best_model()
            self.update_best_solution(best_layout, best_cost)
            
        except KeyboardInterrupt:
            logger.warning("训练被用户中断")
            if self.model:
                self._save_interrupted_model()
        except Exception as e:
            logger.error(f"PPO优化过程中发生错误: {e}", exc_info=True)
            raise
        
        return self.finish_optimization()
    
    def _check_for_resume(self):
        """检查是否需要从checkpoint恢复训练"""
        if not self.config.RESUME_TRAINING:
            return
            
        if self.config.PRETRAINED_MODEL_PATH:
            model_path = Path(self.config.PRETRAINED_MODEL_PATH)
            if not model_path.exists():
                logger.warning(f"指定的预训练模型不存在: {model_path}")
                return
            self.resume_model_path = str(model_path)
        else:
            # 自动查找最新的checkpoint
            checkpoint_callback = CheckpointCallback(
                save_freq=1,
                save_path=self.config.LOG_PATH
            )
            model_path = checkpoint_callback.get_latest_checkpoint()
            if model_path:
                self.resume_model_path = str(model_path)
            else:
                logger.info("未找到可用的checkpoint，将开始全新训练")
                return
        
        # 加载checkpoint元数据
        metadata = CheckpointCallback.load_checkpoint_metadata(self.resume_model_path)
        if metadata:
            self.completed_steps = metadata.get("training_progress", {}).get("num_timesteps", 0)
            logger.info(f"从checkpoint恢复训练，已完成步数: {self.completed_steps}")
        else:
            logger.warning("无法加载checkpoint元数据，从步数0开始")
    
    def _setup_environment_and_model(self, remaining_steps: int):
        """设置环境和模型"""
        logger.info(f"正在创建 {self.config.NUM_ENVS} 个并行环境...")
        
        # 创建矢量化环境
        vec_env = make_vec_env(
            lambda: ActionMasker(LayoutEnv(**self.env_kwargs), LayoutEnv._action_mask_fn),
            n_envs=self.config.NUM_ENVS
        )
        
        # 使用自定义包装器确保episode信息正确传递
        self.vec_env = EpisodeInfoVecEnvWrapper(vec_env)
        
        logger.info("矢量化环境创建成功，已添加episode信息包装器")
        
        # 创建或加载模型
        if self.resume_model_path:
            self._load_pretrained_model()
        else:
            self._create_new_model()
    
    def _load_pretrained_model(self):
        """加载预训练模型"""
        logger.info(f"正在加载预训练模型: {self.resume_model_path}")
        
        try:
            self.model = MaskablePPO.load(
                self.resume_model_path,
                env=self.vec_env,
                device='cuda' if torch.cuda.is_available() else 'cpu'
            )
            
            # 重新设置学习率调度器
            if hasattr(self.model, 'lr_schedule'):
                lr_scheduler = get_lr_scheduler(
                    schedule_type=self.config.LEARNING_RATE_SCHEDULE_TYPE,
                    initial_lr=self.config.LEARNING_RATE_INITIAL,
                    final_lr=self.config.LEARNING_RATE_FINAL
                )
                self.model.lr_schedule = lr_scheduler
                logger.info("学习率调度器已重新设置")
                
            logger.info("预训练模型加载成功")
            
        except Exception as e:
            logger.error(f"加载预训练模型失败: {e}")
            raise
    
    def _create_new_model(self):
        """创建新的PPO模型"""
        logger.info("创建全新的PPO模型...")
        
        # 创建学习率调度器
        lr_scheduler = get_lr_scheduler(
            schedule_type=self.config.LEARNING_RATE_SCHEDULE_TYPE,
            initial_lr=self.config.LEARNING_RATE_INITIAL,
            final_lr=self.config.LEARNING_RATE_FINAL
        )
        
        logger.info(f"使用学习率调度器: {self.config.LEARNING_RATE_SCHEDULE_TYPE}")
        logger.info(f"初始学习率: {self.config.LEARNING_RATE_INITIAL}, 最终学习率: {self.config.LEARNING_RATE_FINAL}")
        
        # 定义策略网络参数
        policy_kwargs = {
            "features_extractor_class": LayoutTransformer,
            "features_extractor_kwargs": {
                "features_dim": self.config.EMBEDDING_DIM,  # 使用统一的配置属性
                "config": self.config
            },
            "net_arch": dict(pi=[self.config.POLICY_NET_ARCH] * self.config.POLICY_NET_LAYERS,
                            vf=[self.config.VALUE_NET_ARCH] * self.config.VALUE_NET_LAYERS)
        }
        
        # 创建PPO模型
        self.model = MaskablePPO(
            MaskableActorCriticPolicy,
            self.vec_env,
            learning_rate=lr_scheduler,
            n_steps=self.config.N_STEPS,
            batch_size=self.config.BATCH_SIZE,
            n_epochs=self.config.N_EPOCHS,
            gamma=self.config.GAMMA,
            gae_lambda=self.config.GAE_LAMBDA,
            clip_range=self.config.CLIP_RANGE,
            ent_coef=self.config.ENT_COEF,
            vf_coef=self.config.VF_COEF,
            max_grad_norm=self.config.MAX_GRAD_NORM,
            policy_kwargs=policy_kwargs,
            verbose=1,
            device='cuda' if torch.cuda.is_available() else 'cpu',
            tensorboard_log=str(self.config.LOG_PATH)
        )
        
        logger.info("PPO模型创建成功")
    
    def _train_model(self, remaining_steps: int):
        """训练模型"""
        timestamp = time.strftime("%Y%m%d-%H%M%S")
        log_dir = self.config.LOG_PATH / f"ppo_layout_{timestamp}"
        result_dir = self.config.RESULT_PATH / "model" / f"ppo_layout_{timestamp}"
        
        # 如果是恢复训练，使用原有目录
        if self.resume_model_path:
            checkpoint_path = Path(self.resume_model_path)
            if "ppo_layout_" in checkpoint_path.parent.name:
                log_dir = checkpoint_path.parent
                result_dir = self.config.RESULT_PATH / checkpoint_path.parent.name
        
        # 创建目录
        log_dir.mkdir(parents=True, exist_ok=True)
        result_dir.mkdir(parents=True, exist_ok=True)
        
        # 设置回调
        callbacks = []
        
        
        # Checkpoint回调
        checkpoint_callback = CheckpointCallback(
            save_freq=self.config.CHECKPOINT_FREQUENCY,
            save_path=str(log_dir / "checkpoints"),
            name_prefix="checkpoint"
        )
        callbacks.append(checkpoint_callback)
        
        # 评估回调
        eval_vec_env = make_vec_env(
            lambda: ActionMasker(LayoutEnv(**self.env_kwargs), LayoutEnv._action_mask_fn),
            n_envs=1
        )
        eval_env = EpisodeInfoVecEnvWrapper(eval_vec_env)
        
        eval_callback = EvalCallback(
            eval_env,
            best_model_save_path=str(result_dir / "best_model"),
            log_path=str(log_dir / "eval_logs"),
            eval_freq=self.config.EVAL_FREQUENCY,
            deterministic=True,
            render=False
        )
        callbacks.append(eval_callback)
        
        # 开始训练
        logger.info(f"开始训练，剩余步数: {remaining_steps}")
        logger.info(f"日志保存路径: {log_dir}")
        
        self.model.learn(
            total_timesteps=remaining_steps,
            callback=callbacks,
            tb_log_name="PPO",
            progress_bar=True,
            reset_num_timesteps=False if self.resume_model_path else True
        )
        
        # 训练完成
        logger.info("🎉 训练完成！")
        logger.info("=" * 80)
        
        # 保存最终模型
        final_model_path = log_dir / "final_model.zip"
        self.model.save(str(final_model_path))
        logger.info(f"最终模型已保存到: {final_model_path}")
        
        # 保存训练配置
        config_path = log_dir / "training_config.json"
        config_data = self.config.__dict__.copy()
        save_json(config_data, str(config_path))
        logger.info(f"训练配置已保存到: {config_path}")
    
    def _evaluate_best_model(self) -> tuple[List[str], float]:
        """评估最佳模型并返回最优布局和成本"""
        # 这里简化实现，实际应该加载最佳模型进行评估
        # 返回一个示例布局和成本
        best_layout = self.generate_initial_layout()
        best_cost = self.evaluate_layout(best_layout)
        
        logger.info(f"最佳模型评估完成，成本: {best_cost:.2f}")
        return best_layout, best_cost
    
    def _save_interrupted_model(self):
        """保存被中断的模型"""
        timestamp = time.strftime("%Y%m%d-%H%M%S")
        interrupted_path = self.config.LOG_PATH / f"interrupted_model_{timestamp}.zip"
        self.model.save(str(interrupted_path))
        logger.info(f"中断的模型已保存到: {interrupted_path}")
    
    def get_additional_metrics(self) -> Dict[str, Any]:
        """获取PPO特定的额外指标"""
        metrics = {
            "total_timesteps": self.config.TOTAL_TIMESTEPS,
            "completed_steps": self.completed_steps,
            "num_envs": self.config.NUM_ENVS,
            "learning_rate_schedule": self.config.LEARNING_RATE_SCHEDULE_TYPE,
            "resume_training": self.config.RESUME_TRAINING
        }
        
        if self.model is not None:
            metrics.update({
                "model_device": str(self.model.device),
                "policy_class": str(type(self.model.policy))
            })
        
        return metrics
</file>

<file path="src/analysis/travel_time.py">
"""
Calculates travel times between specified room-like nodes in the graph.
"""
import csv
import pathlib
import networkx as nx
import logging  # Added for logging
from typing import Dict, List, Union, Optional

from src.config import NetworkConfig
from src.graph.node import Node

# Get a logger for this module
logger = logging.getLogger(__name__)


def calculate_room_travel_times(
    graph: nx.Graph,
    config: NetworkConfig,
    output_dir: pathlib.Path,
    output_filename: str = "room_travel_times.csv",
    ground_floor_z: Optional[float] = None
) -> Dict[str, Dict[str, Union[float, str]]]:
    """
    Calculates the shortest travel times between all pairs of individual "room" instances
    and designated "outward-facing door" nodes in the graph.

    Each room instance and each relevant outward-facing door is treated as a unique location.
    Room instance names will be 'NodeType_NodeID'.
    Outward-facing door names will be 'OutDoor_NodeID'.
    The output CSV will also include a final row with the area of each unique location.

    Args:
        graph: The input NetworkX graph. Nodes are expected to be `Node` objects
               or have a `node_obj` attribute pointing to a `Node` object.
        config: The NetworkConfig object.
        output_dir: The directory to save the resulting CSV file.
        output_filename: The name of the output CSV file.
        ground_floor_z: The Z-coordinate of the designated ground floor.
                        Only 'out' doors on this floor will be considered.

    Returns:
        A dictionary where keys are source location names and values are
        dictionaries mapping target location names to travel times.
    """
    if not graph.nodes:
        logger.warning("Graph is empty. Cannot calculate travel times.")
        return {}

    room_nodes: List[Node] = []
    out_door_nodes: List[Node] = []

    if ground_floor_z is None:
        logger.warning(
            "ground_floor_z not provided to calculate_room_travel_times. "
            "No 'out' doors will be specifically included as distinct locations for travel time analysis "
            "unless this behavior is changed in the filtering logic below."
        )

    for G_node_id, G_node_data in graph.nodes(data=True):
        node_obj = G_node_data.get('node_obj', G_node_id)
        if not isinstance(node_obj, Node):
            continue

        if node_obj.node_type in config.ROOM_TYPES:
            room_nodes.append(node_obj)
        elif node_obj.node_type in config.CONNECTION_TYPES and node_obj.door_type == 'out':
            if ground_floor_z is not None:
                # Tolerance for Z comparison
                if abs(node_obj.pos[2] - ground_floor_z) < 0.1:
                    out_door_nodes.append(node_obj)
            # If ground_floor_z is None, no out_door_nodes are added from this path based on current logic.
            # If you want a fallback, it would be here. For "only ground floor", this is correct.

    location_nodes: List[Node] = room_nodes + out_door_nodes
    # Map Node object to its unique name
    location_names_map: Dict[Node, str] = {}
    # Map unique location name to its area
    location_areas_map: Dict[str, float] = {}

    for node_obj in room_nodes:
        unique_name = f"{node_obj.node_type}_{node_obj.id}"
        location_names_map[node_obj] = unique_name
        location_areas_map[unique_name] = node_obj.area

    for node_obj in out_door_nodes:  # These are already filtered for ground floor
        unique_name = f"门_{node_obj.id}"
        location_names_map[node_obj] = unique_name
        location_areas_map[unique_name] = node_obj.area

    if not location_nodes:
        logger.warning(
            "No room instances or designated (ground floor) outward-facing door nodes found to calculate travel times.")
        return {}

    def weight_function(u_node_obj, v_node_obj, edge_data):  # u,v are Node objects
        if not isinstance(v_node_obj, Node):  # Should not happen if graph is consistent
            logger.error(
                f"Target node {v_node_obj} in edge is not a valid Node object for weight func.")
            raise ValueError(
                f"Invalid node object for weight function: {v_node_obj}")
        return v_node_obj.time

    travel_times_data: Dict[str, Dict[str, Union[float, str]]] = {}
    logger.info(
        f"Calculating travel times for {len(location_nodes)} unique locations...")

    for start_node_obj in location_nodes:
        start_location_name = location_names_map[start_node_obj]
        travel_times_data.setdefault(start_location_name, {})

        try:
            lengths = nx.single_source_dijkstra_path_length(
                graph,
                source=start_node_obj,
                weight=weight_function
            )
        except nx.NodeNotFound:
            logger.warning(
                f"Start node {start_location_name} (ID: {start_node_obj.id}) not in graph for Dijkstra. Skipping.")
            continue
        except Exception as e:
            logger.error(
                f"Error during Dijkstra for {start_location_name} (ID: {start_node_obj.id}): {e}")
            continue

        for target_node_obj in location_nodes:
            target_location_name = location_names_map[target_node_obj]

            if start_node_obj == target_node_obj:
                travel_times_data[start_location_name][target_location_name] = round(
                    start_node_obj.time, 2)
                continue

            if target_node_obj in lengths:
                total_time = start_node_obj.time + lengths[target_node_obj]
                travel_times_data[start_location_name][target_location_name] = round(
                    total_time, 2)
            else:
                travel_times_data[start_location_name][target_location_name] = '∞'

    logger.info("Travel time calculation complete.")

    output_dir.mkdir(parents=True, exist_ok=True)
    csv_file_path = output_dir / output_filename

    # all_location_names will now be unique identifiers like "RoomType_ID" or "OutDoor_ID"
    all_location_names = sorted(list(location_names_map.values()))

    if not all_location_names:
        logger.warning(
            "No unique location names generated to write to CSV for travel times and areas.")
        return travel_times_data

    try:
        with open(csv_file_path, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)

            header = ['来源/目标'] + all_location_names
            writer.writerow(header)

            for source_name in all_location_names:
                row_data = [source_name]
                for target_name in all_location_names:
                    time_val = travel_times_data.get(
                        source_name, {}).get(target_name, 'N/A')
                    row_data.append(time_val)
                writer.writerow(row_data)

            # "Area (px²)" or "面积 (m²)" depending on your unit
            area_row_label = "面积"
            area_row_data = [area_row_label]
            for loc_name in all_location_names:  # loc_name is now unique
                area_val = location_areas_map.get(
                    loc_name)  # Get area by unique name
                if area_val is not None:
                    area_row_data.append(f"{area_val:.2f}")
                else:
                    area_row_data.append("N/A")  # Should ideally not happen
            writer.writerow(area_row_data)

        logger.info(f"Travel times and areas saved to {csv_file_path}")
    except IOError as e:
        logger.error(
            f"Failed to write travel times and areas CSV to {csv_file_path}: {e}")

    return travel_times_data
</file>

<file path="src/rl_optimizer/data/cache/traffic_distribution.json">
{
    "ICU": {
      "ICU_30013": 1.0
    },
    "NICU": {
      "NICU_30012": 1.0
    },
    "中医科": {
      "中医科_10007": 0.00805131
    },
    "中心供应室": {
      "中心供应室_10003": 1.0
    },
    "产前诊断门诊": {
      "产前诊断门诊_30009": 0.00900655
    },
    "产科": {
      "产科_30006": 0.032887555
    },
    "介入科": {
      "介入科_20013": 0.021697598
    },
    "体检科": {
      "体检科_10009": 0.059224891
    },
    "儿科": {
      "儿科_10006": 0.095796943
    },
    "全科": {
      "全科_10004": 0.034525109
    },
    "内诊药房": {
      "内诊药房_10001": 0.077783843
    },
    "内镜中心": {
      "内镜中心_20002": 0.022379913
    },
    "口腔一区": {
      "口腔一区_40004": 0.023335153
    },
    "口腔科二区": {
      "口腔科二区_40008": 0.009688865
    },
    "呼吸内科": {
      "呼吸内科_20011": 0.022516376
    },
    "妇科": {
      "妇科_30005": 0.063318777
    },
    "心血管内科": {
      "心血管内科_20006": 0.069868996
    },
    "急诊科": {
      "急诊科_1": 1.0
    },
    "手术室": {
      "手术室_30007": 1.0,
      "手术室_40006": 1.0
    },
    "挂号收费": {
      "挂号收费_10002": 0.01388509825,
      "挂号收费_20001": 0.01388509825,
      "挂号收费_30001": 0.01388509825,
      "挂号收费_40001": 0.01388509825
    },
    "放射科": {
      "放射科_10005": 0.03029476
    },
    "检验中心": {
      "检验中心_20003": 1.0
    },
    "泌尿外科": {
      "泌尿外科_30003": 0.00996179
    },
    "消化内科": {
      "消化内科_20004": 0.020196507
    },
    "烧伤整形科": {
      "烧伤整形科_30011": 0.002865721
    },
    "生殖医学科": {
      "生殖医学科_30010": 0.007641921
    },
    "甲状腺外科": {
      "甲状腺外科_30002": 0.025791485
    },
    "病理科": {
      "病理科_20008": 1.0
    },
    "皮肤科": {
      "皮肤科_40005": 0.031113537
    },
    "眼科": {
      "眼科_40002": 0.034798035
    },
    "神经内科": {
      "神经内科_20010": 0.028247817
    },
    "综合激光科": {
      "综合激光科_40009": 0.007368996
    },
    "耳鼻喉科": {
      "耳鼻喉科_40003": 0.044077511
    },
    "肝胆胰外科": {
      "肝胆胰外科_30004": 0.015829694
    },
    "肾内科": {
      "肾内科_20005": 0.013782751
    },
    "肿瘤科": {
      "肿瘤科_20012": 0.00996179
    },
    "超声科": {
      "超声科_10008": 0.034525109
    },
    "透析中心": {
      "透析中心_30008": 0.007641921
    },
    "采血处": {
      "采血处_20007": 0.036844978
    },
    "门": {
      "门_11072": 0.046019042,
      "门_11083": 0.182080284,
      "门_11086": 0.046019042,
      "门_11087": 0.046019042,
      "门_11093": 0.182080284,
      "门_11094": 0.046019042,
      "门_11112": 0.037939511,
      "门_11113": 0.081614043,
      "门_11116": 0.081614043,
      "门_11118": 0.037939511,
      "门_11119": 0.081614043,
      "门_11122": 0.037939511,
      "门_11145": 0.063429585,
      "门_11146": 0.053365481,
      "门_11147": 0.063429585,
      "门_11149": 0.053365481,
      "门_11153": 0.063429585,
      "门_11154": 0.053365481,
      "门_11155": 0.063429585,
      "门_11165": 0.063429585
    },
    "门诊手术室": {
      "门诊手术室_40007": 1.0
    },
    "静配中心": {
      "静配中心_40010": 1.0
    },
    "骨科": {
      "骨科_20009": 0.033433406
    }
  }
</file>

<file path="src/optimization/optimizer.py">
"""
Module for facility layout optimization by reassigning functional types
to physical locations to minimize total travel times for defined workflows.
"""

import copy
import logging
from typing import List, Dict, Optional, Tuple, Set, Any, Sequence
import pandas as pd
import os
from joblib import Parallel, delayed

from src.analysis.process_flow import PeopleFlow, PathFinder
from src.config import NetworkConfig

# Code Time Profiling
import cProfile
import pstats
profiler = cProfile.Profile()

logger = logging.getLogger(__name__)


class PhysicalLocation:
    """Represents a physical space/node in the facility.

    Attributes:
        name_id: The unique identifier of the physical location (e.g., 'RoomType_123').
        original_functional_type: The functional type initially associated with this
            physical location based on the input data (e.g., ' Radiology').
        area: The area of this physical location.
        is_swappable: Boolean indicating if this location can have its function reassigned.
                      Typically, connection types like 'Door' are not swappable.
    """

    def __init__(self, name_id: str, original_functional_type: str, area: float, is_swappable: bool = True):
        self.name_id: str = name_id
        self.original_functional_type: str = original_functional_type
        self.area: float = area
        self.is_swappable: bool = is_swappable

    def __repr__(self) -> str:
        return (f"PhysicalLocation(name_id='{self.name_id}', "
                f"original_type='{self.original_functional_type}', area={self.area:.2f}, "
                f"swappable={self.is_swappable})")

    def __eq__(self, other: object) -> bool:
        if not isinstance(other, PhysicalLocation):
            return NotImplemented
        return self.name_id == other.name_id

    def __hash__(self) -> int:
        return hash(self.name_id)


class FunctionalAssignment:
    """Manages the assignment of functional types to lists of physical locations.
     This class represents the current "layout" by defining which physical
    locations (Name_IDs) are currently serving each functional type.
    It supports creating new assignment states by swapping the functional roles
    of two physical locations.

    Attributes:
        assignment_map: A dictionary where keys are functional type strings (e.g., "Radiology")
                        and values are lists of Name_ID strings of PhysicalLocations
                        currently assigned to that function.
    """

    def __init__(self, initial_assignment_map: Dict[str, List[str]]):
        # Deepcopy to ensure independence from the source map
        self.assignment_map: Dict[str, List[str]
                                  ] = copy.deepcopy(initial_assignment_map)

    def get_physical_ids_for_function(self, functional_type: str) -> List[str]:
        """Returns the list of physical Name_IDs assigned to the given functional type."""
        return self.assignment_map.get(functional_type, [])

    def get_functional_type_at_physical_id(self, physical_name_id: str) -> Optional[str]:
        """Finds which functional type, if any, the given physical_name_id is currently assigned to."""
        for func_type, id_list in self.assignment_map.items():
            if physical_name_id in id_list:
                return func_type
        return None

    def get_map_copy(self) -> Dict[str, List[str]]:
        """Returns a deep copy of the internal assignment map."""
        return copy.deepcopy(self.assignment_map)

    def apply_functional_swap(self, phys_loc_A_id: str, phys_loc_B_id: str) -> 'FunctionalAssignment':
        """Creates a new FunctionalAssignment representing the state after swapping
        the functional roles currently hosted at phys_loc_A_id and phys_loc_B_id.

        For example, if A hosts 'Radiology' and B hosts 'Lab', the new assignment
        will have A hosting 'Lab' and B hosting 'Radiology'.
        If one location is "unassigned" (not in any list in assignment_map),
        the function from the other location moves to it, and the original location
        becomes unassigned for that function.

        Args:
            phys_loc_A_id: Name_ID of the first physical location.
            phys_loc_B_id: Name_ID of the second physical location.

        Returns:
            A new FunctionalAssignment object with the swapped roles.
        """
        new_map = self.get_map_copy()  # Start with a copy of the current assignment

        func_type_at_A = self.get_functional_type_at_physical_id(phys_loc_A_id)
        func_type_at_B = self.get_functional_type_at_physical_id(phys_loc_B_id)

        # Handle func_type_at_A (moving func_type_at_A from A to B, if B is involved)
        if func_type_at_A is not None:
            if phys_loc_A_id in new_map.get(func_type_at_A, []):
                new_map[func_type_at_A].remove(phys_loc_A_id)
            # If B is now supposed to host what A had
            if func_type_at_B != func_type_at_A:  # Avoid issues if A and B had same func type initially
                if func_type_at_A not in new_map:  # Should not be needed if remove was successful
                    new_map[func_type_at_A] = []
                # Add B to host A's original function
                if phys_loc_B_id not in new_map[func_type_at_A]:
                    new_map[func_type_at_A].append(phys_loc_B_id)

        # Handle func_type_at_B (moving func_type_at_B from B to A, if A is involved)
        if func_type_at_B is not None:
            if phys_loc_B_id in new_map.get(func_type_at_B, []):
                new_map[func_type_at_B].remove(phys_loc_B_id)
            # If A is now supposed to host what B had
            # if func_type_at_A != func_type_at_B: # Redundant due to first block, but helps clarity
            if func_type_at_B not in new_map:
                new_map[func_type_at_B] = []
            # Add A to host B's original function
            if phys_loc_A_id not in new_map[func_type_at_B]:
                new_map[func_type_at_B].append(phys_loc_A_id)

        # Clean up: remove empty lists and ensure lists are sorted and unique
        # Iterate over a copy of keys for safe deletion
        for func_type in list(new_map.keys()):
            if new_map[func_type]:
                new_map[func_type] = sorted(list(set(new_map[func_type])))
            else:
                # Remove function type if it no longer has any locations
                del new_map[func_type]

        return FunctionalAssignment(new_map)

    def __repr__(self) -> str:
        return f"FunctionalAssignment(map_size={len(self.assignment_map)})"

    def __eq__(self, other: object) -> bool:
        if not isinstance(other, FunctionalAssignment):
            return NotImplemented

        # Check if the keys (functional types) are the same
        if set(self.assignment_map.keys()) != set(other.assignment_map.keys()):
            return False

        # Check if the lists of physical IDs for each functional type are the same
        # Comparing sorted lists ensures that order of IDs within the list doesn't affect equality
        for func_type, phys_ids_self in self.assignment_map.items():
            # Already know func_type exists in other due to key check
            phys_ids_other = other.assignment_map.get(func_type)
            if sorted(phys_ids_self) != sorted(phys_ids_other):
                return False
        return True


class WorkflowDefinition:
    """Defines a patient/staff workflow to be evaluated.

    Attributes:
        workflow_id: A unique identifier for this workflow (e.g., 'OutpatientVisit').
        functional_sequence: An ordered list of functional type names representing
                             the steps in the workflow (e.g., ['Reception', 'ConsultRoom', 'Pharmacy']).
        weight: A float representing the importance or frequency of this workflow,
                used in the objective function.
    """

    def __init__(self, workflow_id: str, functional_sequence: List[str], weight: float = 1.0):
        self.workflow_id: str = workflow_id
        self.functional_sequence: List[str] = functional_sequence
        self.weight: float = weight

    def __repr__(self) -> str:
        return (f"WorkflowDefinition(id='{self.workflow_id}', "
                f"seq_len={len(self.functional_sequence)}, weight={self.weight})")


class EvaluatedWorkflowOutcome:
    """Stores the outcome of evaluating a WorkflowDefinition under a specific assignment.

    Attributes:
        workflow_definition: The WorkflowDefinition that was evaluated.
        average_time: The average travel time of all valid PeopleFlows for this workflow
                      under the given assignment. Can be float('inf') if any potential path
                      is unroutable or if no valid paths exist.
        shortest_flow: The actual PeopleFlow object representing the shortest path found
                       for this workflow under the given assignment. Can be None if no
                       valid path was found or if average_time is inf.
        num_paths_considered: The number of valid PeopleFlows used to calculate the average_time.
        all_path_times: A list of travel times for all valid PeopleFlows. Can be None.
    """

    def __init__(self,
                 workflow_definition: WorkflowDefinition,
                 average_time: Optional[float],
                 # Still store the shortest for reference
                 shortest_flow: Optional[PeopleFlow],
                 num_paths_considered: int,
                 all_path_times: Optional[List[float]]):
        self.workflow_definition: WorkflowDefinition = workflow_definition
        self.average_time: float = average_time if average_time is not None else float(
            'inf')
        self.shortest_flow: Optional[PeopleFlow] = shortest_flow
        self.num_paths_considered: int = num_paths_considered
        self.all_path_times: Optional[List[float]] = all_path_times

    def __repr__(self) -> str:
        path_str = "N/A"
        if self.shortest_flow and self.shortest_flow.actual_node_id_sequence:
            shortest_time_val = self.shortest_flow.total_time if self.shortest_flow.total_time is not None else float(
                'inf')
            path_str = (f"Shortest: {self.shortest_flow.actual_node_id_sequence[0]}..."
                        f"{self.shortest_flow.actual_node_id_sequence[-1]} (Time: {shortest_time_val:.2f})")

        return (f"EvaluatedWorkflow(id='{self.workflow_definition.workflow_id}', "
                f"avg_time={self.average_time:.2f}, num_paths={self.num_paths_considered}, {path_str})")

# Helper function for joblib to process a single flow
# This function will be called in parallel for each flow.
# It needs access to a PathFinder instance.


def _calculate_flow_time_joblib_task(flow_to_process: PeopleFlow, path_finder_instance: PathFinder) -> Tuple[Optional[float], Any, List[str]]:
    """
    Calculates total time for a flow and returns time, flow identifier, and sequence.
    The flow_to_process.total_time will be updated by calculate_flow_total_time.
    """
    time = path_finder_instance.calculate_flow_total_time(flow_to_process)
    # Return time and identifiers to re-associate with original flow object if needed,
    # and to allow the main thread to update the original flow objects.
    return time, flow_to_process.identify, flow_to_process.actual_node_id_sequence

def _calculate_batch_flow_times(flows_batch, path_finder):
    results = []
    for flow in flows_batch:
        time = path_finder.calculate_flow_total_time(flow)
        results.append((time, flow.identify, flow.actual_node_id_sequence))
    return results

class LayoutObjectiveCalculator:
    """
    Calculates the overall objective value for a given functional assignment.
    Supports incremental calculation.
    """

    def __init__(self,
                 workflow_definitions: Sequence[WorkflowDefinition],
                 path_finder: PathFinder,
                 n_jobs_for_flows: int = -1):
        self.workflow_definitions: Sequence[WorkflowDefinition] = workflow_definitions
        self.path_finder: PathFinder = path_finder

        if n_jobs_for_flows == 0:
            self.n_jobs_for_flows = 1
        elif n_jobs_for_flows == -1:
            cpu_count = os.cpu_count()
            self.n_jobs_for_flows = cpu_count if cpu_count is not None else 1
        else:
            self.n_jobs_for_flows = n_jobs_for_flows

        logger.info(
            f"LayoutObjectiveCalculator initialized with n_jobs_for_flows={self.n_jobs_for_flows}")

        self.cached_evaluated_outcomes: Optional[List[EvaluatedWorkflowOutcome]] = None
        self.cached_total_objective_value: Optional[float] = None
        # Stores the FunctionalAssignment object
        self.cached_assignment_obj: Optional[FunctionalAssignment] = None

    def _evaluate_single_workflow_flows(self,
                                        generated_flows: List[PeopleFlow],
                                        workflow_id_for_logging: str
                                        ) -> Tuple[float, Optional[PeopleFlow], int, Optional[List[float]]]:
        # ... (Implementation from the previous response, unchanged) ...
        workflow_average_time: float = float('inf')
        shortest_flow: Optional[PeopleFlow] = None
        shortest_time_val: float = float('inf')
        valid_flow_times: List[float] = []
        num_paths_considered: int = 0

        if not generated_flows:
            logger.debug(
                f"Single WF Eval: Workflow '{workflow_id_for_logging}' generated no flows. Avg Time is Inf.")
            return float('inf'), None, 0, None

        try:
            parallel_results_wf: List[Tuple[Optional[float], Any, List[str]]] = [
            ]
            backend_to_use = 'loky'
            if self.n_jobs_for_flows > 1 and len(generated_flows) > 1:
                batch_size = 600
                batch_results = []
                flow_batches = [generated_flows[i:i + batch_size] for i in range(0, len(generated_flows), batch_size)]
                # Ensure _calculate_batch_flow_times is globally defined or properly imported
                with Parallel(n_jobs=self.n_jobs_for_flows, backend=backend_to_use) as parallel:
                    results_from_parallel = parallel(
                        delayed(_calculate_batch_flow_times)(
                            flow_batch, self.path_finder)
                        for flow_batch in flow_batches
                    )
                for single_batch_result in results_from_parallel:
                    batch_results.extend(single_batch_result)
                parallel_results_wf = batch_results
            else:
                for flow in generated_flows:
                    parallel_results_wf.append(
                        _calculate_flow_time_joblib_task(flow, self.path_finder))
        except Exception as e:
            logger.error(
                f"Error during parallel flow calculation for single workflow '{workflow_id_for_logging}': {e}. Falling back to sequential.", exc_info=True)
            parallel_results_wf = [_calculate_flow_time_joblib_task(
                flow, self.path_finder) for flow in generated_flows]

        all_paths_routable = True
        temp_flow_times: List[float] = []
        flow_map_for_update: Dict[Tuple[Any, Tuple[str, ...]], PeopleFlow] = {
            (f.identify, tuple(f.actual_node_id_sequence)): f for f in generated_flows
        }

        for flow_time, flow_id, flow_seq_list in parallel_results_wf:
            flow_seq_tuple = tuple(flow_seq_list)
            original_flow = flow_map_for_update.get((flow_id, flow_seq_tuple))
            if original_flow:
                original_flow.update_total_time(
                    flow_time if flow_time is not None else float('inf'))

            if flow_time is None:
                all_paths_routable = False
                break
            temp_flow_times.append(flow_time)
            if original_flow and flow_time < shortest_time_val:
                shortest_time_val = flow_time
                shortest_flow = original_flow

        if all_paths_routable:
            if temp_flow_times:
                valid_flow_times = temp_flow_times
                workflow_average_time = sum(
                    valid_flow_times) / len(valid_flow_times)
                num_paths_considered = len(valid_flow_times)
        else:
            workflow_average_time = float('inf')
            shortest_flow = None
            valid_flow_times = []
            num_paths_considered = 0

        return workflow_average_time, shortest_flow, num_paths_considered, (valid_flow_times if workflow_average_time != float('inf') else None)

    def _perform_full_evaluation(self,
                                 current_assignment: FunctionalAssignment
                                 ) -> Tuple[float, List[EvaluatedWorkflowOutcome]]:
        # ... (Implementation from the previous response, unchanged) ...
        logger.debug(
            f"Performing full evaluation for assignment (map_size={len(current_assignment.assignment_map)}).")
        total_weighted_time: float = 0.0
        evaluated_outcomes: List[EvaluatedWorkflowOutcome] = []

        for wf_def in self.workflow_definitions:
            generated_flows = self.path_finder.generate_flows(
                workflow_names=wf_def.functional_sequence,
                workflow_identifier=wf_def.workflow_id,
                custom_assignment_map=current_assignment.get_map_copy()
            )

            wf_avg_time, wf_shortest_flow, wf_num_paths, wf_all_times = \
                self._evaluate_single_workflow_flows(
                    generated_flows, wf_def.workflow_id)

            outcome = EvaluatedWorkflowOutcome(
                workflow_definition=wf_def,
                average_time=wf_avg_time,
                shortest_flow=wf_shortest_flow,
                num_paths_considered=wf_num_paths,
                all_path_times=wf_all_times
            )
            evaluated_outcomes.append(outcome)

            if total_weighted_time != float('inf'):
                if wf_avg_time == float('inf'):
                    total_weighted_time = float('inf')
                else:
                    total_weighted_time += wf_avg_time * wf_def.weight

        return total_weighted_time, evaluated_outcomes

    def evaluate(self,
                 assignment_to_evaluate: FunctionalAssignment,
                 base_assignment_for_cache: Optional[FunctionalAssignment] = None,
                 changed_locations: Optional[Tuple[str, str]] = None
                 ) -> Tuple[float, List[EvaluatedWorkflowOutcome]]:
        """Evaluates the given FunctionalAssignment.

        If `base_assignment_for_cache` and `changed_locations` are provided, and
        `base_assignment_for_cache` matches the internally cached assignment object,
        an incremental update is attempted. Otherwise, a full evaluation is performed.
        """
        can_do_incremental = (
            base_assignment_for_cache is not None and
            changed_locations is not None and
            self.cached_assignment_obj is not None and
            self.cached_evaluated_outcomes is not None and
            # Use the __eq__ method of FunctionalAssignment for comparison
            self.cached_assignment_obj == base_assignment_for_cache
        )

        if not can_do_incremental:
            if base_assignment_for_cache is not None and changed_locations is not None and self.cached_assignment_obj is not None:
                # More specific logging if an attempt at incremental was made but failed due to cache mismatch
                logger.debug(
                    "Cache mismatch for incremental: `base_assignment_for_cache` does not match `cached_assignment_obj`. Performing full evaluation.")
            else:
                logger.debug(
                    "Performing full evaluation (no valid base for incremental or cache empty).")
            total_objective, outcomes = self._perform_full_evaluation(
                assignment_to_evaluate)
        else:
            # Incremental calculation
            logger.debug(
                f"Attempting incremental evaluation. Changed locations: {changed_locations}")
            loc_A_id, loc_B_id = changed_locations

            func_type_at_A_prev = base_assignment_for_cache.get_functional_type_at_physical_id(
                loc_A_id)
            func_type_at_B_prev = base_assignment_for_cache.get_functional_type_at_physical_id(
                loc_B_id)
            func_type_at_A_curr = assignment_to_evaluate.get_functional_type_at_physical_id(
                loc_A_id)
            func_type_at_B_curr = assignment_to_evaluate.get_functional_type_at_physical_id(
                loc_B_id)

            affected_functional_types: Set[str] = set()
            if func_type_at_A_prev:
                affected_functional_types.add(func_type_at_A_prev)
            if func_type_at_B_prev:
                affected_functional_types.add(func_type_at_B_prev)
            if func_type_at_A_curr:
                affected_functional_types.add(func_type_at_A_curr)
            if func_type_at_B_curr:
                affected_functional_types.add(func_type_at_B_curr)

            logger.debug(f"Swap details for incremental: LocA ({loc_A_id}): {func_type_at_A_prev} -> {func_type_at_A_curr}. "
                         f"LocB ({loc_B_id}): {func_type_at_B_prev} -> {func_type_at_B_curr}.")
            logger.debug(
                f"Functional types triggering re-evaluation: {affected_functional_types}")

            new_evaluated_outcomes: List[EvaluatedWorkflowOutcome] = []

            for i, wf_def in enumerate(self.workflow_definitions):
                workflow_is_potentially_affected = any(
                    func_step in affected_functional_types for func_step in wf_def.functional_sequence
                )

                if workflow_is_potentially_affected:
                    # logger.debug(f"Workflow '{wf_def.workflow_id}' is affected by swap. Re-evaluating.")
                    generated_flows_wf = self.path_finder.generate_flows(
                        workflow_names=wf_def.functional_sequence,
                        workflow_identifier=wf_def.workflow_id,
                        custom_assignment_map=assignment_to_evaluate.get_map_copy()
                    )
                    wf_avg_time, wf_shortest_flow, wf_num_paths, wf_all_times = \
                        self._evaluate_single_workflow_flows(
                            generated_flows_wf, wf_def.workflow_id)

                    current_outcome = EvaluatedWorkflowOutcome(
                        workflow_definition=wf_def, average_time=wf_avg_time,
                        shortest_flow=wf_shortest_flow, num_paths_considered=wf_num_paths,
                        all_path_times=wf_all_times)
                    new_evaluated_outcomes.append(current_outcome)
                else:
                    # Ensure self.cached_evaluated_outcomes is not None (already checked by can_do_incremental)
                    new_evaluated_outcomes.append(
                        self.cached_evaluated_outcomes[i])  # type: ignore

            current_total_weighted_time_calc = 0.0
            has_inf_path_incrementally = False
            for outcome in new_evaluated_outcomes:
                if outcome.average_time == float('inf'):
                    has_inf_path_incrementally = True
                    break
                current_total_weighted_time_calc += outcome.average_time * \
                    outcome.workflow_definition.weight

            if has_inf_path_incrementally:
                total_objective = float('inf')
            else:
                total_objective = current_total_weighted_time_calc

            outcomes = new_evaluated_outcomes
            # Log difference for significant changes to monitor incremental logic
            if self.cached_total_objective_value is not None and abs(total_objective - self.cached_total_objective_value) > 1e-5:
                logger.info(
                    f"Incremental eval result. Prev Obj: {self.cached_total_objective_value:.2f}, New Obj: {total_objective:.2f}. Diff: {total_objective - self.cached_total_objective_value:.2f}")
            elif self.cached_total_objective_value is None:
                logger.info(
                    f"Incremental eval result (no prev obj for diff). New Obj: {total_objective:.2f}")

        # Update cache with the results of *this* evaluation, for the *assignment_to_evaluate*
        self.cached_total_objective_value = total_objective
        self.cached_evaluated_outcomes = outcomes
        # Store a deepcopy to prevent external modifications to assignment_to_evaluate from affecting the cache
        self.cached_assignment_obj = copy.deepcopy(assignment_to_evaluate)

        return total_objective, outcomes

    def reset_cache(self):
        """Resets the cache. Call when the base state for comparison changes significantly."""
        logger.info("Resetting LayoutObjectiveCalculator cache.")
        self.cached_evaluated_outcomes = None
        self.cached_total_objective_value = None
        self.cached_assignment_obj = None


class LayoutOptimizer:
    """
    Optimizes facility layout by reassigning functional types to physical locations.
    Uses a greedy iterative approach (best-swap local search).
    """

    def __init__(self,
                 path_finder: PathFinder,
                 workflow_definitions: Sequence[WorkflowDefinition],
                 config: NetworkConfig,  # Used by _initialize_physical_locations
                 area_tolerance_ratio: float = 0.2):
        self.path_finder: PathFinder = path_finder
        # For BAN_TYPES, CONNECTION_TYPES etc.
        self.config: NetworkConfig = config
        self.objective_calculator: LayoutObjectiveCalculator = LayoutObjectiveCalculator(
            workflow_definitions, path_finder, n_jobs_for_flows=config.N_JOBS_FOR_OPTIMIZER_FLOWS if hasattr(
                config, 'N_JOBS_FOR_OPTIMIZER_FLOWS') else -1
        )  # Pass n_jobs from config if available
        self.area_tolerance_ratio: float = area_tolerance_ratio
        self.all_physical_locations: List[PhysicalLocation] = self._initialize_physical_locations(
        )
        self.swappable_locations: List[PhysicalLocation] = [
            loc for loc in self.all_physical_locations if loc.is_swappable
        ]
        logger.info(f"Optimizer initialized. Found {len(self.all_physical_locations)} total physical locations, "
                    f"{len(self.swappable_locations)} are swappable.")

    def _initialize_physical_locations(self) -> List[PhysicalLocation]:
        locations = []
        if self.path_finder.travel_times_df is None:
            logger.error(
                "PathFinder's travel_times_df is not loaded. Cannot initialize physical locations.")
            return []
        # Ensure '面积' key exists or handle gracefully
        has_area_info = '面积' in self.path_finder.travel_times_df.index

        for name_id_str in self.path_finder.all_name_ids:
            parts = name_id_str.split('_', 1)
            original_func_type = parts[0]
            area = 0.0  # Default area
            if has_area_info and name_id_str in self.path_finder.travel_times_df.columns:
                try:
                    area_val = self.path_finder.travel_times_df.loc['面积', name_id_str]
                    if pd.notna(area_val):  # Check for NaN
                        area = float(area_val)
                    else:
                        logger.debug(
                            f"Area for {name_id_str} is NaN. Defaulting to 0.")
                except ValueError:
                    logger.warning(
                        f"Could not parse area for {name_id_str}. Defaulting to 0.")
                except KeyError:  # Should be caught by `name_id_str in ...columns` but as a safeguard
                    logger.warning(
                        f"KeyError for area of {name_id_str}. Defaulting to 0.")

            # Determine swappability (e.g., '门' is a connection type, not swappable for a room function)
            # This uses self.config.CONNECTION_TYPES from your NetworkConfig
            is_swappable = original_func_type not in self.config.CONNECTION_TYPES and \
                original_func_type not in getattr(self.config, 'BAN_TYPES_FOR_SWAP', [
                ])  # Example of more specific ban types

            locations.append(PhysicalLocation(
                name_id_str, original_func_type, area, is_swappable))
        return locations

    def _get_valid_swap_pairs(self) -> List[Tuple[PhysicalLocation, PhysicalLocation]]:
        valid_pairs: List[Tuple[PhysicalLocation, PhysicalLocation]] = []
        num_swappable = len(self.swappable_locations)
        for i in range(num_swappable):
            for j in range(i + 1, num_swappable):
                loc_A = self.swappable_locations[i]
                loc_B = self.swappable_locations[j]

                # Ensure original functions are different, or allow swapping same if it makes sense
                # For now, let's assume swapping locations with the same original function is allowed
                # if their areas match, effectively just moving a function between two identical spots.
                # If they must have different functions to be considered a meaningful swap, add:
                # if loc_A.original_functional_type == loc_B.original_functional_type:
                #     continue

                area_A, area_B = loc_A.area, loc_B.area
                # Handle area_A or area_B being 0 or very small if that's possible
                if area_A > 1e-6 and area_B > 1e-6:  # Use a small epsilon for float comparison
                    if abs(area_A - area_B) / max(area_A, area_B) > self.area_tolerance_ratio:
                        continue
                elif (area_A <= 1e-6 and area_B > 1e-6) or \
                     (area_A > 1e-6 and area_B <= 1e-6):
                    # If one area is (near) zero and the other isn't,
                    # only allow swap if tolerance is very high (e.g., >= 1.0 means ignore area diff)
                    if self.area_tolerance_ratio < 1.0:  # A stricter interpretation
                        continue
                # If both areas are (near) zero, they are considered compatible in terms of area

                valid_pairs.append((loc_A, loc_B))
        logger.info(
            f"Generated {len(valid_pairs)} valid swap pairs based on area tolerance {self.area_tolerance_ratio}.")
        return valid_pairs

    def run_optimization(self,
                         initial_assignment: FunctionalAssignment,
                         max_iterations: int = 100
                         ) -> Tuple[FunctionalAssignment, float, List[EvaluatedWorkflowOutcome]]:
        logger.info("Starting layout optimization...")

        current_best_assignment = copy.deepcopy(
            initial_assignment)  # Start with a copy

        # Initial full evaluation and cache setup
        self.objective_calculator.reset_cache()  # Clear any old cache
        current_best_objective, current_best_outcomes = self.objective_calculator.evaluate(
            assignment_to_evaluate=current_best_assignment
            # No base_assignment or changed_locations, so it performs a full evaluation
            # and populates its internal cache with current_best_assignment
        )
        logger.info(f"Initial Objective Value: {current_best_objective:.2f}")

        if current_best_objective == float('inf'):
            logger.error(
                "Initial assignment results in unroutable workflows. Optimization aborted.")
            return current_best_assignment, current_best_objective, current_best_outcomes

        valid_swap_pairs = self._get_valid_swap_pairs()
        if not valid_swap_pairs:
            logger.warning(
                "No valid pairs of physical locations to swap. Optimization cannot proceed.")
            return current_best_assignment, current_best_objective, current_best_outcomes

        for iteration in range(max_iterations):
            logger.info(f"--- Iteration {iteration + 1}/{max_iterations} ---")

            best_swap_candidate_in_iteration: Optional[FunctionalAssignment] = None
            # Initialize with current best
            best_swap_objective_in_iteration = current_best_objective
            best_swap_outcomes_in_iteration = current_best_outcomes
            best_swapped_pair_info: Optional[Tuple[PhysicalLocation,
                                                   PhysicalLocation]] = None

            num_evaluated_swaps = 0
            for loc_A, loc_B in valid_swap_pairs:
                # Create a candidate assignment by swapping functions based on the *current_best_assignment*
                candidate_assignment = current_best_assignment.apply_functional_swap(
                    loc_A.name_id, loc_B.name_id
                )

                # Evaluate this candidate, providing the base for incremental calculation
                candidate_objective, candidate_outcomes = self.objective_calculator.evaluate(
                    assignment_to_evaluate=candidate_assignment,
                    base_assignment_for_cache=current_best_assignment,  # The state *before* this swap
                    # The specific swap made
                    changed_locations=(loc_A.name_id, loc_B.name_id)
                )
                num_evaluated_swaps += 1
                if num_evaluated_swaps % 200 == 0 and num_evaluated_swaps > 0:  # Log progress periodically
                    logger.debug(
                        f"  Evaluated {num_evaluated_swaps}/{len(valid_swap_pairs)} potential swaps in iteration {iteration+1}...")

                if candidate_objective < best_swap_objective_in_iteration:
                    best_swap_objective_in_iteration = candidate_objective
                    best_swap_candidate_in_iteration = candidate_assignment  # This is a new object
                    best_swap_outcomes_in_iteration = candidate_outcomes
                    best_swapped_pair_info = (loc_A, loc_B)

            if best_swap_candidate_in_iteration is not None and best_swap_objective_in_iteration < current_best_objective:
                # An improvement was found in this iteration
                improvement = current_best_objective - best_swap_objective_in_iteration
                # Should not be None if candidate is not None
                sw_loc_A, sw_loc_B = best_swapped_pair_info

                logger.info(
                    f"Improvement found! Swapping functions related to '{sw_loc_A.name_id}' "
                    f"(Orig Func: {sw_loc_A.original_functional_type}, Area: {sw_loc_A.area:.0f}) and "
                    f"'{sw_loc_B.name_id}' (Orig Func: {sw_loc_B.original_functional_type}, Area: {sw_loc_B.area:.0f})."
                )
                logger.info(
                    f"Objective improved from {current_best_objective:.2f} to {best_swap_objective_in_iteration:.2f} (Gain: {improvement:.2f}).")

                # Update the current best state for the next iteration
                current_best_assignment = copy.deepcopy(
                    best_swap_candidate_in_iteration)  # Store the new best assignment
                current_best_objective = best_swap_objective_in_iteration
                current_best_outcomes = best_swap_outcomes_in_iteration

                # The objective_calculator's cache was already updated to reflect best_swap_candidate_in_iteration
                # when it was evaluated and found to be the best in the inner loop, because evaluate() always
                # updates its cache with the assignment_to_evaluate.
                # So, when the next iteration starts, current_best_assignment will match the calculator's cache.

            else:
                logger.info(
                    "No further improvement found in this iteration. Optimization stopped.")
                break  # Stop if no improvement in this iteration

        if iteration == max_iterations - 1 and best_swap_candidate_in_iteration is not None and best_swap_objective_in_iteration < current_best_objective:
            logger.info(
                f"Reached maximum number of iterations ({max_iterations}), but was still improving.")
        elif iteration == max_iterations - 1:
            logger.info(
                f"Reached maximum number of iterations ({max_iterations}).")

        logger.info("Layout optimization finished.")
        logger.info(
            f"Final Best Objective Value: {current_best_objective:.2f}")
        # The current_best_outcomes corresponds to current_best_assignment
        return current_best_assignment, current_best_objective, current_best_outcomes
</file>

<file path="src/rl_optimizer/agent/ppo_agent.py">
# src/rl_optimizer/agent/ppo_agent.py

import torch
from pathlib import Path
from stable_baselines3.common.vec_env import VecNormalize
from stable_baselines3.common.env_util import make_vec_env
from sb3_contrib.common.maskable.callbacks import MaskableEvalCallback as EvalCallback
from sb3_contrib import MaskablePPO
from sb3_contrib.common.wrappers import ActionMasker
from sb3_contrib.common.maskable.policies import MaskableActorCriticPolicy
from typing import Dict, Any, List
import numpy as np
import time

from src.config import RLConfig
from src.rl_optimizer.data.cache_manager import CacheManager
from src.rl_optimizer.env.cost_calculator import CostCalculator
from src.rl_optimizer.env.layout_env import LayoutEnv
from src.rl_optimizer.model.policy_network import LayoutTransformer
from src.rl_optimizer.utils.setup import setup_logger, save_json
from src.rl_optimizer.utils.lr_scheduler import get_lr_scheduler
from src.rl_optimizer.utils.checkpoint_callback import CheckpointCallback

logger = setup_logger(__name__)

def get_action_mask_from_info(infos: List[Dict]) -> np.ndarray:
    """
    一个辅助函数，用于从矢量化环境的info字典列表中提取动作掩码。
    这是为了适配 `stable-baselines3` 的 `ActionMasker` 包装器。

    Args:
        infos (List[Dict]): 来自矢量化环境的info字典列表。

    Returns:
        np.ndarray: 合法的动作掩码堆叠成的数组。
    """
    return np.array([info.get("action_mask", []) for info in infos])

class PPOAgent:
    """
    PPO智能体，负责编排整个强化学习训练和评估流程。

    它使用stable-baselines3和sb3-contrib库来高效地实现MaskablePPO算法，
    该算法能够处理带有动作掩码的复杂决策问题。
    """

    def __init__(self, config: RLConfig, cache_manager: CacheManager, cost_calculator: CostCalculator):
        """
        初始化PPO智能体。

        Args:
            config (RLConfig): RL优化器的配置对象。
            cache_manager (CacheManager): 已初始化的数据缓存管理器。
            cost_calculator (CostCalculator): 已初始化的成本计算器。
        """
        self.config = config
        self.cm = cache_manager
        self.cc = cost_calculator

        # 将所有需要传递给环境的参数打包成一个字典
        self.env_kwargs = {
            "config": self.config,
            "cache_manager": self.cm,
            "cost_calculator": self.cc
        }

    def _check_for_resume(self) -> tuple[str, int]:
        """
        检查是否需要从checkpoint恢复训练。
        
        Returns:
            tuple: (模型路径, 已完成的训练步数)，如果不需要恢复则返回 (None, 0)
        """
        if not self.config.RESUME_TRAINING:
            return None, 0
            
        if self.config.PRETRAINED_MODEL_PATH:
            # 使用指定的预训练模型路径
            model_path = Path(self.config.PRETRAINED_MODEL_PATH)
            if not model_path.exists():
                logger.warning(f"指定的预训练模型不存在: {model_path}")
                return None, 0
        else:
            # 自动查找最新的checkpoint
            checkpoint_callback = CheckpointCallback(
                save_freq=1,  # 临时值，仅用于查找checkpoint
                save_path=self.config.LOG_PATH
            )
            model_path = checkpoint_callback.get_latest_checkpoint()
            if not model_path:
                logger.info("未找到可用的checkpoint，将开始全新训练")
                return None, 0
                
        # 尝试加载checkpoint元数据获取训练进度
        metadata = CheckpointCallback.load_checkpoint_metadata(model_path)
        completed_steps = 0
        if metadata:
            completed_steps = metadata.get("training_progress", {}).get("num_timesteps", 0)
            logger.info(f"从checkpoint恢复训练，已完成步数: {completed_steps}")
        else:
            logger.warning("无法加载checkpoint元数据，从步数0开始")
            
        return str(model_path), completed_steps
        
    def _load_pretrained_model(self, model_path: str, vec_env) -> MaskablePPO:
        """
        加载预训练模型并准备继续训练。
        
        Args:
            model_path (str): 模型文件路径
            vec_env: 矢量化环境
            
        Returns:
            MaskablePPO: 加载的模型实例
        """
        logger.info(f"正在加载预训练模型: {model_path}")
        
        try:
            # 加载模型
            model = MaskablePPO.load(
                model_path,
                env=vec_env,
                device='cuda' if torch.cuda.is_available() else 'cpu'
            )
            
            # 重新设置学习率调度器（如果需要）
            if hasattr(model, 'lr_schedule'):
                lr_scheduler = get_lr_scheduler(
                    schedule_type=self.config.LEARNING_RATE_SCHEDULE_TYPE,
                    initial_lr=self.config.LEARNING_RATE_INITIAL,
                    final_lr=self.config.LEARNING_RATE_FINAL
                )
                model.lr_schedule = lr_scheduler
                logger.info("学习率调度器已重新设置")
                
            logger.info("预训练模型加载成功")
            return model
            
        except Exception as e:
            logger.error(f"加载预训练模型失败: {e}")
            raise

    def train(self):
        """
        执行PPO智能体的完整训练流程，包括环境创建、模型初始化、训练过程、评估回调和模型保存。
        
        支持断点续训：如果配置了RESUME_TRAINING=True，将自动检查并加载最新的checkpoint继续训练。
        训练过程中会自动创建并行矢量化环境，配置自定义特征提取器和网络结构，定期评估并保存最佳模型，最终保存完整训练后的模型至指定目录。支持手动中断训练并安全保存当前模型进度。
        """
        logger.info("开始配置和启动PPO训练流程...")
        timestamp = time.strftime("%Y%m%d-%H%M%S")
        log_dir = self.config.LOG_PATH / f"ppo_layout_{timestamp}"
        result_dir = self.config.RESULT_PATH / f"ppo_layout_{timestamp}"

        # 检查是否需要从checkpoint恢复
        resume_model_path, completed_steps = self._check_for_resume()
        if resume_model_path:
            logger.info(f"将从checkpoint恢复训练: {resume_model_path}")
            # 如果是恢复训练，使用原有的日志目录结构
            checkpoint_path = Path(resume_model_path)
            if "ppo_layout_" in checkpoint_path.parent.name:
                log_dir = checkpoint_path.parent
                result_dir = self.config.RESULT_PATH / checkpoint_path.parent.name
        
        remaining_steps = max(0, self.config.TOTAL_TIMESTEPS - completed_steps)
        if remaining_steps == 0:
            logger.info("训练已完成，无需继续训练")
            return

        # --- 1. 创建并行化的、支持掩码的矢量化环境 ---
        logger.info(f"正在创建 {self.config.NUM_ENVS} 个并行环境...")

        # 构建单个带掩码的环境
        vec_env = make_vec_env(
            lambda: ActionMasker(LayoutEnv(**self.env_kwargs), LayoutEnv._action_mask_fn),
            n_envs=self.config.NUM_ENVS
        )

        logger.info("矢量化环境创建成功。")

        # --- 2. 配置PPO模型 ---
        model = None
        if resume_model_path:
            # 加载预训练模型
            model = self._load_pretrained_model(resume_model_path, vec_env)
            logger.info(f"剩余训练步数: {remaining_steps}")
        else:
            # 创建新模型
            logger.info("创建全新的PPO模型...")
            # 创建学习率调度器
            lr_scheduler = get_lr_scheduler(
                schedule_type=self.config.LEARNING_RATE_SCHEDULE_TYPE,
                initial_lr=self.config.LEARNING_RATE_INITIAL,
                final_lr=self.config.LEARNING_RATE_FINAL
            )
            logger.info(f"使用学习率调度器: {self.config.LEARNING_RATE_SCHEDULE_TYPE}")
            logger.info(f"初始学习率: {self.config.LEARNING_RATE_INITIAL}, 最终学习率: {self.config.LEARNING_RATE_FINAL}")
            
            # 定义策略网络的关键字参数，指定自定义的特征提取器
            policy_kwargs = {
                "features_extractor_class": LayoutTransformer,
                "features_extractor_kwargs": {
                    "config": self.config,
                    "features_dim": self.config.FEATURES_DIM
                },
                # 定义Actor和Critic网络的隐藏层结构
                "net_arch": [256, 256] 
            }

            logger.info("正在初始化MaskablePPO模型...")
            model = MaskablePPO(
                MaskableActorCriticPolicy,
                vec_env,
                policy_kwargs=policy_kwargs,
                learning_rate=lr_scheduler,  # 使用学习率调度器
                n_steps=self.config.NUM_STEPS,
                batch_size=self.config.BATCH_SIZE,
                n_epochs=self.config.NUM_EPOCHS,
                gamma=self.config.GAMMA,
                gae_lambda=self.config.GAE_LAMBDA,
                clip_range=self.config.CLIP_EPS,
                ent_coef=self.config.ENT_COEF,
                verbose=1,
                tensorboard_log=str(log_dir / 'tensorboard_logs'),
                device='cuda' if torch.cuda.is_available() else 'cpu'
            )
            logger.info(f"模型初始化成功，将在 {model.device.type} 设备上进行训练。")

        # --- 3. 配置回调函数 ---
        callbacks = []
        
        # 评估回调
        eval_env = ActionMasker(LayoutEnv(**self.env_kwargs), LayoutEnv._action_mask_fn)
        eval_callback = EvalCallback(
            eval_env,
            best_model_save_path=str(result_dir / 'best_model'),
            log_path=str(log_dir / 'eval_logs'),
            eval_freq=max(self.config.NUM_STEPS * 5, 2048),
            deterministic=True,
            render=False,
            n_eval_episodes=20 # 评估更多回合以获得更稳定的结果
        )
        callbacks.append(eval_callback)
        
        # Checkpoint回调
        if self.config.SAVE_TRAINING_STATE:
            checkpoint_callback = CheckpointCallback(
                save_freq=self.config.CHECKPOINT_FREQUENCY,
                save_path=str(log_dir / 'checkpoints'),
                name_prefix="checkpoint",
                verbose=1
            )
            callbacks.append(checkpoint_callback)
            logger.info(f"启用checkpoint保存，频率: 每{self.config.CHECKPOINT_FREQUENCY}步")

        # --- 4. 启动训练 ---
        logger.info(f"开始训练，剩余时间步数: {remaining_steps}...")
        logger.info(f"日志和模型将保存在: {log_dir}")
        try:
            model.learn(
                total_timesteps=remaining_steps,
                callback=callbacks,
                progress_bar=True,
                reset_num_timesteps=not bool(resume_model_path)  # 如果是恢复训练则不重置计数器
            )
        except KeyboardInterrupt:
            logger.warning("训练被手动中断。")
        finally:
            # --- 5. 保存最终模型 ---
            final_model_path = log_dir / 'final_model.zip'
            model.save(final_model_path)
            logger.info(f"训练结束，最终模型已保存至: {final_model_path}")
            vec_env.close()

    def evaluate(self, model_path: str) -> Dict[str, Any]:
        """
        加载一个已训练好的模型，进行确定性评估并返回最优布局方案。

        Args:
            model_path (str): 已保存模型的路径 (.zip 文件)。

        Returns:
            Dict[str, Any]: 包含最优布局、总成本和各流程成本的结果字典。
        """
        logger.info(f"正在加载模型进行评估: {model_path}...")
        try:
            model = MaskablePPO.load(model_path, device='cpu')
        except FileNotFoundError:
            logger.error(f"模型文件未找到: {model_path}")
            return {}
        
        eval_env = LayoutEnv(**self.env_kwargs)

        obs, info = eval_env.reset()
        terminated = False

        while not terminated:
            action_mask = eval_env.get_action_mask()
            action, _states = model.predict(obs, action_masks=action_mask, deterministic=True)
            obs, reward, terminated, _, info = eval_env.step(int(action))

        final_layout_str = eval_env._get_final_layout_str()
        total_cost = self.cc.calculate_total_cost(final_layout_str)
        per_process_cost = self.cc.calculate_per_process_cost(final_layout_str)

        # 构建最终的布局映射：槽位 -> 科室
        final_layout_map = {
            slot: dept for slot, dept in zip(eval_env.placeable_slots, final_layout_str)
            if dept is not None
        }

        results = {
            "best_layout": final_layout_map,
            "total_weighted_cost": total_cost,
            "per_process_unweighted_cost": per_process_cost,
            "final_reward_from_env": reward
        }

        result_path = self.config.LOG_PATH / 'best_layout_result.json'
        save_json(results, result_path)
        logger.info(f"评估完成！最优布局已保存至: {result_path}")
        logger.info(f"最优布局的总加权成本为: {total_cost:.2f}")

        return results
</file>

<file path="src/rl_optimizer/env/layout_env.py">
# src/rl_optimizer/env/layout_env.py

import gymnasium as gym
from gymnasium import spaces
import numpy as np
from typing import List, Dict, Any, Tuple, Optional
from collections import defaultdict

from src.config import RLConfig
from src.rl_optimizer.data.cache_manager import CacheManager
from src.rl_optimizer.env.cost_calculator import CostCalculator
from src.rl_optimizer.utils.setup import setup_logger

logger = setup_logger(__name__)  # 使用默认INFO级别

class LayoutEnv(gym.Env):
    """
    医院布局优化的强化学习环境 (范式A: 选科室，填槽位)。

    遵循Gymnasium接口，通过自回归方式构建布局。环境按照一个在每个回合
    开始时随机打乱的槽位顺序进行填充。在每一步，智能体从所有尚未被
    放置的科室中选择一个，放入当前待填充的槽位。

    - **状态 (Observation)**: 字典，包含当前部分布局、已放置科室的掩码、
                              以及当前待填充槽位的面积信息。
    - **动作 (Action)**: 离散值，代表在 `placeable_depts` 列表中的科室索引。
    - **奖励 (Reward)**: 仅在回合结束时给予的稀疏奖励，基于加权总通行时间
                      和软约束。
    - **约束 (Constraints)**: 硬约束（面积、强制相邻）通过动作掩码实现，
                          确保智能体只能选择合法的科室。
    """

    metadata = {'render_modes': ['human']}

    def __init__(self, config: RLConfig, cache_manager: CacheManager, cost_calculator: CostCalculator):
        """
        初始化布局优化环境。

        Args:
            config (RLConfig): RL优化器的配置对象。
            cache_manager (CacheManager): 已初始化的数据缓存管理器。
            cost_calculator (CostCalculator): 已初始化的成本计算器。
        """
        super().__init__()
        self.config = config
        self.cm = cache_manager
        self.cc = cost_calculator

        self._initialize_nodes_and_slots()
        self._define_spaces()
        self._initialize_state_variables()

        logger.info(f"环境初始化完成：{self.num_slots}个可用槽位，{self.num_depts}个待放置科室。")

    def _initialize_nodes_and_slots(self):
        """从CacheManager获取并设置节点和槽位信息。"""
        self.placeable_slots = self.cm.placeable_slots
        self.slot_areas = self.cm.placeable_nodes_df['area'].values
        self.num_slots = len(self.placeable_slots)

        self.placeable_depts = self.cm.placeable_departments
        self.dept_areas_map = dict(zip(self.cm.placeable_nodes_df['node_id'], self.cm.placeable_nodes_df['area']))
        self.num_depts = len(self.placeable_depts)
        
        if self.num_slots != self.num_depts:
            raise ValueError(f"槽位数 ({self.num_slots}) 与待布局科室数 ({self.num_depts}) 不匹配!")
        
        self.dept_to_idx = {dept: i for i, dept in enumerate(self.placeable_depts)}

    def _define_spaces(self):
        """定义观测空间和动作空间。"""
        # 动作空间：选择一个科室进行放置。动作是科室的索引。
        self.action_space = spaces.Discrete(self.num_depts)

        # 观测空间：使用Box空间来明确定义形状，避免SB3的意外转换
        self.observation_space = spaces.Dict({
            # layout[i] = k+1 表示槽位i放置了索引为k的科室。0表示空。
            "layout": spaces.Box(
                low=0, 
                high=self.num_depts, # 最大值为科室数 (num_depts-1)+1
                shape=(self.num_slots,), 
                dtype=np.int32
            ),
            # placed_mask[k] = 1 表示索引为k的科室已被放置。
            "placed_mask": spaces.MultiBinary(self.num_depts),
            # 当前待填充槽位的索引
            "current_slot_idx": spaces.Box(low=0, high=self.num_slots - 1, shape=(1,), dtype=np.int32)
        })

    def _initialize_state_variables(self):
        """初始化每个回合都会改变的状态变量。"""
        self.current_step = 0
        # layout的索引是物理槽位索引，值是科室索引+1
        self.layout = np.zeros(self.num_slots, dtype=np.int32)
        self.placed_mask = np.zeros(self.num_depts, dtype=bool)
        # 每个回合开始时需要被打乱的槽位处理顺序
        self.shuffled_slot_indices = np.arange(self.num_slots)

    def reset(self, *, seed: Optional[int] = None, options: Optional[dict] = None) -> Tuple[Dict, Dict]:
        """重置环境，随机化槽位顺序，并返回初始观测。"""
        super().reset(seed=seed)
        self._initialize_state_variables()
        self.np_random.shuffle(self.shuffled_slot_indices)
        return self._get_obs(), self._get_info(terminated=False)
    
    def step(self, action: int) -> Tuple[Dict, float, bool, bool, Dict]:
        """执行一个动作：在当前槽位放置选定的科室。"""
        # 更严格的输入验证
        if not (0 <= action < self.num_depts):
            logger.error(f"无效的动作索引: {action}，有效范围是 [0, {self.num_depts-1}]")
            return self._get_obs(), -100.0, True, False, self._get_info(terminated=False)
        
        if self.placed_mask[action]:
            # 这是一个安全检查。理论上动作掩码会阻止这种情况。
            # 如果发生，说明上游逻辑有误，应给予重罚并终止。
            logger.error(f"严重错误：智能体选择了已被放置的科室！动作={action}, 科室={self.placeable_depts[action]}")
            logger.error(f"当前步骤: {self.current_step}, placed_mask: {self.placed_mask}")
            logger.error(f"当前动作掩码: {self.get_action_mask()}")
            return self._get_obs(), -100.0, True, False, self._get_info(terminated=False)

        # 确定当前要填充的物理槽位索引
        slot_to_fill = self.shuffled_slot_indices[self.current_step]
        
        # 更新状态：在布局中记录放置，并标记科室为"已用"
        self.layout[slot_to_fill] = action + 1  # 动作是科室索引，存储时+1
        self.placed_mask[action] = True
        self.current_step += 1

        terminated = (self.current_step == self.num_slots)
        reward = self._calculate_reward() if terminated else 0.0
        info = self._get_info(terminated)
        
        # 调试日志：检查episode结束时的info内容
        if terminated and logger.isEnabledFor(20):  # INFO级别
            logger.debug(f"Episode结束，info内容: {info}")
            if 'episode' in info:
                logger.debug(f"Episode数据: {info['episode']}")

        return self._get_obs(), reward, terminated, False, info
    
    def _get_obs(self) -> Dict[str, Any]:
        """构建并返回当前观测字典。"""
        if self.current_step >= self.num_slots:
            current_slot_idx = 0
        else:
            current_slot_idx = self.shuffled_slot_indices[self.current_step]

        return {
            "layout": self.layout,
            "placed_mask": self.placed_mask,
            "current_slot_idx": np.array([current_slot_idx], dtype=np.int32)
        }
    
    def _get_info(self, terminated: bool = False) -> Dict[str, Any]:
        """返回包含动作掩码和episode信息的附加信息。"""
        info = {"action_mask": self.get_action_mask()}
        
        # 如果episode结束，添加训练指标信息
        if terminated:
            # 构造最终布局
            final_layout_depts = [None] * self.num_slots
            for slot_idx, dept_id in enumerate(self.layout):
                if dept_id > 0:
                    final_layout_depts[slot_idx] = self.placeable_depts[dept_id - 1]
            
            # 如果布局完整，计算时间成本
            if None not in final_layout_depts:
                # 计算原始的总时间成本（未缩放）
                raw_time_cost = self.cc.calculate_total_cost(final_layout_depts)
                
                # 计算用于训练的缩放reward（与_calculate_reward中的逻辑一致）
                scaled_time_reward = -raw_time_cost / 1e4
                
                # 调试日志：确认传递的是原始值
                if logger.isEnabledFor(20):  # INFO级别
                    logger.debug(f"Episode结束 - 原始时间成本: {raw_time_cost:.2f}, "
                               f"缩放后训练reward: {scaled_time_reward:.6f}")
                
                # Episode结束时添加详细信息
                info['episode'] = {
                    'time_cost': raw_time_cost,  # 明确传递原始值
                    'scaled_reward': scaled_time_reward,  # 同时提供缩放值用于对比
                    'layout': final_layout_depts.copy(),
                    'r': self._calculate_reward(),  # 完整的奖励值（包含时间+邻接）
                    'l': self.current_step  # episode长度
                }
                
                # 如果启用了详细日志，还可以添加更多信息
                if logger.isEnabledFor(20):  # INFO级别
                    info['episode']['per_process_costs'] = self.cc.calculate_per_process_cost(final_layout_depts)
        
        return info
    
    def get_action_mask(self) -> np.ndarray:
        """
        计算当前步骤下所有合法的科室动作掩码。
        
        该方法根据当前待填充槽位的面积约束，返回一个布尔数组，指示哪些未放置的科室可被合法选择。如果所有未放置科室均不满足面积约束，将逐步放宽容差，直至至少有一个合法动作；如仍无合法动作，则强制允许所有未放置科室。
        
        返回值:
            np.ndarray: 长度等于科室数的布尔数组，True 表示对应科室当前可被选择。
        """
        if self.current_step >= self.num_slots:
            return np.zeros(self.num_depts, dtype=bool)

        # 1. 初始掩码：所有未放置的科室都是潜在的合法动作
        action_mask = ~self.placed_mask

        # 2. 面积约束：科室面积必须符合当前槽位的容差
        current_slot_idx = self.shuffled_slot_indices[self.current_step]
        current_slot_area = self.slot_areas[current_slot_idx]
        min_area = current_slot_area * (1 - self.config.AREA_SCALING_FACTOR)
        max_area = current_slot_area * (1 + self.config.AREA_SCALING_FACTOR)

        for dept_idx in range(self.num_depts):
            if action_mask[dept_idx]:  # 只检查当前合法的动作
                dept_name = self.placeable_depts[dept_idx]
                dept_area = self.dept_areas_map[dept_name]
                if not (min_area <= dept_area <= max_area):
                    action_mask[dept_idx] = False
        
        # 容错机制：如果没有合法动作，逐步放松约束
        if np.sum(action_mask) == 0:
            logger.warning(f"在步骤 {self.current_step}，没有找到任何合法的动作！尝试放松约束...")
            
            # 重新计算，使用更宽松的面积约束
            relaxation_factors = [0.2, 0.3, 0.5, 0.7, 1.0]  # 逐步放松
            for factor in relaxation_factors:
                action_mask = ~self.placed_mask  # 重新开始，只考虑未放置的科室
                min_area = current_slot_area * (1 - factor)
                max_area = current_slot_area * (1 + factor)
                
                for dept_idx in range(self.num_depts):
                    if action_mask[dept_idx]:
                        dept_name = self.placeable_depts[dept_idx]
                        dept_area = self.dept_areas_map[dept_name]
                        if not (min_area <= dept_area <= max_area):
                            action_mask[dept_idx] = False
                
                if np.sum(action_mask) > 0:
                    logger.info(f"使用松弛因子 {factor} 找到了 {np.sum(action_mask)} 个合法动作")
                    break
            
            # 最后的安全检查：如果仍然没有合法动作，至少允许所有未放置的科室
            if np.sum(action_mask) == 0:
                current_slot_name = self.placeable_slots[current_slot_idx]
                current_slot_area = self.slot_areas[current_slot_idx]
                
                unplaced_indices = np.where(~self.placed_mask)[0]
                unplaced_depts_info = [
                    f"{self.placeable_depts[i]}({self.dept_areas_map[self.placeable_depts[i]]:.2f})"
                    for i in unplaced_indices
                ]
                
                logger.error(
                    f"即使放松所有约束也没有为槽位 '{current_slot_name}' (面积: {current_slot_area:.2f}) 找到合法科室！"
                    f"剩余待放置科室: [{', '.join(unplaced_depts_info)}]. "
                    f"将强制允许所有未放置的科室。"
                )
                action_mask = ~self.placed_mask
        
        return action_mask
    
    def _calculate_reward(self) -> float:
        """
        在回合结束时，根据最终布局计算并返回总奖励。
        
        如果布局未完成，则返回重罚。奖励由时间成本和邻接约束两部分加权组成，其中时间成本通过 `CostCalculator` 计算并缩放，邻接奖励由 `_calculate_adjacency_reward` 计算。
        """
        final_layout_depts = [None] * self.num_slots
        for slot_idx, dept_id in enumerate(self.layout):
            if dept_id > 0:
                final_layout_depts[slot_idx] = self.placeable_depts[dept_id - 1]
        
        if None in final_layout_depts:
            logger.error("布局计算奖励时发现未完成的布局，返回重罚。")
            return -500.0

        time_cost = self.cc.calculate_total_cost(final_layout_depts)
        time_reward = -time_cost / 1e4  # 缩放以稳定训练

        adjacency_reward = self._calculate_adjacency_reward(final_layout_depts)
        
        total_reward = (self.config.REWARD_TIME_WEIGHT * time_reward + 
                        self.config.REWARD_ADJACENCY_WEIGHT * adjacency_reward)
        
        return total_reward
    
    def _calculate_adjacency_reward(self, _final_layout: List[str]) -> float:
        """
        计算偏好相邻软约束的奖励。
        (TODO: 需要一个可靠的邻接关系数据源)
        """
        # 这是一个示例实现，实际部署时需要替换为真实邻接数据
        # 假设 travel_times 矩阵中时间小于某个阈值（例如 10）即为相邻
        reward = 0.0
        # ... 实现软约束奖励计算 ...
        return reward
    
    def render(self, mode="human"):
        """(可选) 渲染环境状态，用于调试。"""
        if mode == "human":
            print(f"--- Step: {self.current_step} ---")
            current_slot_idx = self.shuffled_slot_indices[self.current_step]
            current_slot = self.placeable_slots[current_slot_idx]
            print(f"Current Slot to Fill: {current_slot} (Area: {self.slot_areas[current_slot_idx]:.2f})")
            
            layout_str = []
            for i in range(self.num_slots):
                dept_id = self.layout[i]
                if dept_id > 0:
                    layout_str.append(f"{self.placeable_slots[i]}: {self.placeable_depts[dept_id-1]}")
                else:
                    layout_str.append(f"{self.placeable_slots[i]}: EMPTY")
            print("Current Layout:\n" + "\n".join(layout_str))

    @staticmethod
    def _action_mask_fn(env: 'LayoutEnv') -> np.ndarray:
        """
        ActionMasker 包装器所需的静态方法，用于提取动作掩码。
        
        Args:
            env (LayoutEnv): 环境实例。
            
        Returns:
            np.ndarray: 当前状态下的动作掩码。
        """
        return env.get_action_mask()

    def _get_final_layout_str(self) -> List[str]:
        """
        获取最终布局的科室名称列表。
        
        Returns:
            List[str]: 按槽位顺序排列的科室名称列表。
        """
        final_layout = [None] * self.num_slots
        for slot_idx, dept_id in enumerate(self.layout):
            if dept_id > 0:
                final_layout[slot_idx] = self.placeable_depts[dept_id - 1]
        return final_layout
</file>

<file path="src/network/network.py">
"""
Orchestrates the construction of a single-floor network graph.
"""

import numpy as np
import networkx as nx
import logging
from typing import Dict, Tuple, Any, List, Optional
from scipy.spatial import KDTree  # For connecting doors to mesh nodes

from src.config import NetworkConfig
from src.graph.graph_manager import GraphManager
from src.graph.node import Node
from src.image_processing.processor import ImageProcessor
from .node_creators import (  # Assuming node_creators.py is in the same directory
    BaseNodeCreator,
    RoomNodeCreator,
    VerticalNodeCreator,
    PedestrianNodeCreator,
    OutsideNodeCreator,
    ConnectionNodeCreator
)

logger = logging.getLogger(__name__)

class Network:
    """
    Manages the creation of a network graph for a single floor from an image.

    The process involves:
    1. Loading and preprocessing the image.
    2. Creating different types of nodes (rooms, doors, corridors, etc.) using
       specialized NodeCreator strategies.
    3. Establishing connections between nodes, including specific logic for
       connecting doors to pedestrian/outside mesh areas.
    """

    def __init__(self,
                 config: NetworkConfig,
                 color_map_data: Dict[Tuple[int, int, int], Dict[str, Any]],
                 id_generator_start_value: int):
        """
        Initializes the Network orchestrator.

        Args:
            config: The main configuration object.
            color_map_data: The RGB color to type mapping.
            id_generator_start_value: The starting ID for nodes in this network.
                                      Crucial for `SuperNetwork` to ensure global ID uniqueness
                                      when processing multiple floors in parallel.
        """
        self.config = config
        self.color_map_data = color_map_data  # Passed to creators

        self.image_processor = ImageProcessor(config, color_map_data)
        self.graph_manager = GraphManager(id_generator_start_value)

        # Initialize node creators
        self._node_creators: List[BaseNodeCreator] = [
            RoomNodeCreator(config, color_map_data,
                            self.image_processor, self.graph_manager),
            VerticalNodeCreator(config, color_map_data,
                                self.image_processor, self.graph_manager),
            # Pedestrian and Outside creators mark areas in id_map first, then create mesh.
            # Connection creator relies on these id_map markings.
            PedestrianNodeCreator(config, color_map_data,
                                  self.image_processor, self.graph_manager),
            # Outside creator might be conditional based on 'outside' flag in run()
            # Connection creator should run after rooms, vertical, pedestrian, outside areas are marked/created
            ConnectionNodeCreator(config, color_map_data,
                                  self.image_processor, self.graph_manager)
        ]

        # have an instance of OutsideNodeCreator for conditional use
        self._outside_node_creator = OutsideNodeCreator(
            config, color_map_data, self.image_processor, self.graph_manager)

        self._current_image_data: Optional[np.ndarray] = None
        self._id_map: Optional[np.ndarray] = None
        self._image_height: Optional[int] = None
        self._image_width: Optional[int] = None

    def _initialize_run(self, image_path: str) -> None:
        """Loads image, prepares internal data structures for a run."""
        # Load and preprocess image (quantize colors)
        raw_image_data = self.image_processor.load_and_prepare_image(
            image_path)
        self._current_image_data = self.image_processor.quantize_colors(
            raw_image_data)

        self._image_height, self._image_width = self.image_processor.get_image_dimensions()

        # id_map stores the ID of the node occupying each pixel, or special area IDs
        self._id_map = np.full((self._image_height, self._image_width),
                               self.config.BACKGROUND_ID_MAP_VALUE, dtype=np.int32)  # Use int32 for IDs

        # This ensures 'out' doors can be identified even if OutsideNodeCreator doesn't run
        # to create detailed outside mesh nodes.
        if self.config.OUTSIDE_TYPES:  # Check if there are any outside types defined
            for outside_type_name in self.config.OUTSIDE_TYPES:
                # Create a mask for this outside type from the quantized image
                # We don't need full morphology here, just the raw areas.
                # ConnectionNodeCreator will use its own dilation.
                outside_mask = self._outside_node_creator._create_mask_for_type(
                    self._current_image_data,
                    outside_type_name,
                    apply_morphology=True  # Apply basic morphology to clean up the mask
                )
                if outside_mask is not None:
                    self._id_map[outside_mask !=
                                 0] = self.config.OUTSIDE_ID_MAP_VALUE

    def _create_all_node_types(self, z_level: int, process_outside_nodes: bool) -> None:
        """Iterates through node creators to populate the graph."""
        if self._current_image_data is None or self._id_map is None:
            raise RuntimeError(
                "Network run not initialized properly. Call _initialize_run first.")

        # Specific order of creation can be important
        # 1. Rooms and Vertical transport (solid areas with own IDs)
        # 2. Pedestrian areas (mesh + special ID in id_map)
        # 3. Outside areas (mesh + special ID in id_map) - if requested
        # 4. Connections (doors - rely on previously set IDs in id_map)

        # Execute creators in the predefined order
        for creator in self._node_creators:
            logger.info(f"Running creator: {creator.__class__.__name__}")
            creator.create_nodes(self._current_image_data,
                                 self._id_map, z_level)

        # Conditionally run the OutsideNodeCreator to create actual mesh nodes for outside
        # FIXME: 不需要室外节点
        # if process_outside_nodes:
        #     logger.info(f"Running creator: {self._outside_node_creator.__class__.__name__} (for mesh)")
        #     # Note: _create_mesh_nodes_for_mask in OutsideNodeCreator also sets id_map,
        #     # but it's okay as it will set the same OUTSIDE_ID_MAP_VALUE before creating nodes.
        #     self._outside_node_creator.create_nodes(
        #         self._current_image_data, self._id_map, z_level)

    def _connect_doors_to_mesh_areas(self, z_level: int) -> None:
        """
        Connects door nodes (type 'in' or 'out') to the nearest pedestrian/outside
        mesh nodes respectively.
        """
        if not self.graph_manager.get_all_nodes():
            return  # Optimization

        connection_nodes = [
            node for node in self.graph_manager.get_all_nodes()
            if node.node_type in self.config.CONNECTION_TYPES and node.pos[2] == z_level
            # Only connect these
            and (node.door_type == 'in' or node.door_type == 'out')
        ]
        if not connection_nodes:
            return

        pedestrian_mesh_nodes = [
            node for node in self.graph_manager.get_all_nodes()
            if node.node_type in self.config.PEDESTRIAN_TYPES and node.pos[2] == z_level
        ]
        outside_mesh_nodes = [
            node for node in self.graph_manager.get_all_nodes()
            if node.node_type in self.config.OUTSIDE_TYPES and node.pos[2] == z_level
        ]

        ped_tree = None
        if pedestrian_mesh_nodes:
            ped_positions = np.array([p_node.pos[:2]
                                     for p_node in pedestrian_mesh_nodes])
            if ped_positions.size > 0:  # Ensure not empty before creating KDTree
                ped_tree = KDTree(ped_positions)

        out_tree = None
        if outside_mesh_nodes:
            out_positions = np.array([o_node.pos[:2]
                                     for o_node in outside_mesh_nodes])
            if out_positions.size > 0:
                out_tree = KDTree(out_positions)

        max_door_to_mesh_distance = self.config.GRID_SIZE * 3

        for conn_node in connection_nodes:
            door_pos_2d = conn_node.pos[:2]

            if conn_node.door_type == 'in' and ped_tree:
                dist, idx = ped_tree.query(door_pos_2d, k=1)
                # Check if idx is a valid index and not out of bounds (e.g. if ped_tree was empty for some reason)
                if idx < len(pedestrian_mesh_nodes) and dist <= max_door_to_mesh_distance:
                    nearest_ped_node = pedestrian_mesh_nodes[idx]
                    self.graph_manager.connect_nodes_by_ids(
                        conn_node.id, nearest_ped_node.id)

            elif conn_node.door_type == 'out':  # 'out' doors connect to outside AND potentially nearby pedestrian areas
                connected_to_main_outside = False
                if out_tree:
                    dist, idx = out_tree.query(door_pos_2d, k=1)
                    if idx < len(outside_mesh_nodes) and dist <= max_door_to_mesh_distance:
                        nearest_out_node = outside_mesh_nodes[idx]
                        self.graph_manager.connect_nodes_by_ids(
                            conn_node.id, nearest_out_node.id)
                        connected_to_main_outside = True

                # Also check for nearby pedestrian nodes if this 'out' door is on a path
                if ped_tree:
                    # Query for potentially multiple pedestrian nodes within a smaller radius
                    # This is for cases like an exit onto a patio (pedestrian) then to lawn (outside)
                    indices_in_ball = ped_tree.query_ball_point(
                        door_pos_2d, r=np.sqrt(2 * self.config.GRID_SIZE ** 2))
                    for ped_idx in indices_in_ball:
                        if ped_idx < len(pedestrian_mesh_nodes):
                            self.graph_manager.connect_nodes_by_ids(
                                conn_node.id, pedestrian_mesh_nodes[ped_idx].id)
                            # If it connects to outside mesh AND pedestrian mesh, that's fine.
                            # The pathfinding will choose the best route.

    def run(self, image_path: str, z_level: int = 0, process_outside_nodes: bool = False) \
            -> Tuple[nx.Graph, int, int, int]:
        """
        Executes the full network generation pipeline.
        Args:
            process_outside_nodes: If True, detailed mesh nodes for outside areas are created.
                                   If False, outside areas are only marked in id_map (for door typing)
                                   but no actual outside mesh nodes are generated by OutsideNodeCreator.
        """
        logger.info(f"--- Processing floor: {image_path} at z={z_level}, process_outside_nodes={process_outside_nodes} ---")
        
        # self.graph_manager.clear(...) # Only if reusing Network instance, typically not.

        self._initialize_run(image_path) # This now pre-marks OUTSIDE_ID_MAP_VALUE
        
        self._create_all_node_types(z_level, process_outside_nodes) # process_outside_nodes controls mesh creation
        
        self._connect_doors_to_mesh_areas(z_level)
        
        logger.info(f"--- Finished floor. Nodes: {len(self.graph_manager.get_all_nodes())} ---")
        
        if self._image_width is None or self._image_height is None:
            raise RuntimeError("Image dimensions not set.")

        return (
            self.graph_manager.get_graph_copy(),
            self._image_width,
            self._image_height,
            self.graph_manager.get_next_available_node_id_estimate()
        )
</file>

<file path="src/config.py">
"""Configuration module for the network generation project."""

import pathlib
from typing import Dict, Tuple, List, Any, Optional

# Global COLOR_MAP - Consider encapsulating or making it part of a config loader
COLOR_MAP: Dict[Tuple[int, int, int], Dict[str, Any]] = {
    (244, 67, 54): {'name': '内诊药房', 'time': 46},
    (0, 150, 136): {'name': '挂号收费', 'time': 79},
    (103, 58, 183): {'name': '急诊科', 'time': 9000},
    (145, 102, 86): {'name': '中心供应室', 'time': 1},
    (33, 150, 243): {'name': '全科', 'time': 435},
    (3, 169, 244): {'name': '放射科', 'time': 250},
    (0, 188, 212): {'name': '儿科', 'time': 640},
    (207, 216, 220): {'name': '走廊', 'time': 1},
    (117, 117, 117): {'name': '楼梯', 'time': 1},
    (189, 189, 189): {'name': '电梯', 'time': 1},
    (158, 158, 158): {'name': '扶梯', 'time': 1},
    (76, 175, 80): {'name': '绿化', 'time': 1},
    (255, 235, 59): {'name': '墙', 'time': 1},
    (121, 85, 72): {'name': '门', 'time': 1},
    (156, 39, 176): {'name': '室外', 'time': 1},
    (139, 195, 74): {'name': '内镜中心', 'time': 960},
    (205, 220, 57): {'name': '检验中心', 'time': 180},
    (255, 193, 7): {'name': '消化内科', 'time': 245},
    (255, 152, 0): {'name': '甲状腺外科', 'time': 256},
    (254, 87, 34): {'name': '肾内科', 'time': 315},
    (169, 238, 90): {'name': '心血管内科', 'time': 886},
    (88, 67, 60): {'name': '采血处', 'time': 184},
    (239, 199, 78): {'name': '眼科', 'time': 564},
    (253, 186, 87): {'name': '中医科', 'time': 1486},
    (250, 133, 96): {'name': '耳鼻喉科', 'time': 737},
    (197, 254, 130): {'name': '口腔一区', 'time': 1004},
    (173, 133, 11): {'name': '超声科', 'time': 670},
    (119, 90, 10): {'name': '病理科', 'time': 1},
    (250, 146, 138): {'name': '骨科', 'time': 223},
    (255, 128, 171): {'name': '泌尿外科', 'time': 337},
    (33, 250, 230): {'name': '肝胆胰外科', 'time': 397},
    (82, 108, 255): {'name': '皮肤科', 'time': 462},
    (226, 58, 255): {'name': '妇科', 'time': 442},
    (100, 139, 55): {'name': '产科', 'time': 404},
    (188, 246, 126): {'name': '产房', 'time': 1},
    (113, 134, 91): {'name': '手术室', 'time': 1},
    (175, 207, 142): {'name': '门诊手术室', 'time': 1},
    (179, 116, 190): {'name': '中庭', 'time': 1},
    (232, 137, 248): {'name': '口腔科二区', 'time': 1004},
    (63, 100, 23): {'name': '神经内科', 'time': 428},
    (240, 222, 165): {'name': '呼吸内科', 'time': 359},
    (187, 24, 80): {'name': '综合激光科', 'time': 462},
    (150, 133, 179): {'name': '透析中心', 'time': 14400},
    (112, 40, 236): {'name': '肿瘤科', 'time': 2320},
    (241, 190, 186): {'name': '产前诊断门诊', 'time': 442},
    (186, 146, 160): {'name': '体检科', 'time': 1260},
    (71, 195, 180): {'name': '生殖医学科', 'time': 425},
    (187, 152, 247): {'name': '烧伤整形科', 'time': 256},
    (254, 210, 145): {'name': '介入科', 'time': 3240},
    (251, 242, 159): {'name': '栏杆', 'time': 1},
    (240, 61, 123): {'name': 'NICU', 'time': 1},
    (250, 162, 193): {'name': 'ICU', 'time': 1},
    (252, 201, 126): {'name': '静配中心', 'time': 1},
    (255, 255, 255): {'name': '空房间', 'time': 1}
}

class NetworkConfig:
    """Stores configuration parameters for network generation and plotting."""

    def __init__(self, color_map_data: Dict[Tuple[int, int, int], Dict[str, Any]] = COLOR_MAP):
        self.RESULT_PATH: pathlib.Path = pathlib.Path(__file__).parent.parent / 'results' / 'network'
        self.DEBUG_PATH: pathlib.Path = pathlib.Path(__file__).parent.parent / 'debug'
        self.IMAGE_ROTATE: int = 180
        self.AREA_THRESHOLD: int = 60  # Minimum area for a component to be considered a node

        # Node Type Definitions (derived from COLOR_MAP)
        self.ALL_TYPES: List[str] = [v['name'] for v in color_map_data.values()]

        self.CONNECTION_TYPES: List[str] = ['门']
        _ban_type_base: List[str] = ['墙', '栏杆', '室外', '走廊', '电梯', '扶梯', '楼梯', '空房间', '绿化', '中庭']
        self.BAN_TYPES: List[str] = [name for name in _ban_type_base if name in self.ALL_TYPES]

        self.ROOM_TYPES: List[str] = [
            v['name'] for v in color_map_data.values()
            if v['name'] not in self.BAN_TYPES and v['name'] not in self.CONNECTION_TYPES
        ]
        self.VERTICAL_TYPES: List[str] = [name for name in ['电梯', '扶梯', '楼梯'] if name in self.ALL_TYPES]
        self.PEDESTRIAN_TYPES: List[str] = [name for name in ['走廊'] if name in self.ALL_TYPES]
        self.OUTSIDE_TYPES: List[str] = [name for name in ['室外'] if name in self.ALL_TYPES]

        # Grid and Special IDs for pixel-level identification in id_map
        self.GRID_SIZE: int = 40  # Base grid size for mesh node generation
        self.OUTSIDE_ID_MAP_VALUE: int = -1  # Special ID for 'outside' areas in the id_map
        self.BACKGROUND_ID_MAP_VALUE: int = -2 # Special ID for 'background' in the id_map
        self.PEDESTRIAN_ID_MAP_VALUE: int = -3 # Special ID for 'pedestrian' areas in the id_map

        # Node Property Times
        self.OUTSIDE_MESH_TIMES_FACTOR: int = 2  # Multiplier for grid size and time for outside nodes
        self.PEDESTRIAN_TIME: float = 1.75  # Default time for pedestrian nodes
        self.CONNECTION_TIME: float = 3.0  # Default time for connection nodes (e.g., doors)

        # Plotting and Visualization
        self.IMAGE_MIRROR: bool = True  # Whether to mirror the image horizontally in plots
        self.NODE_COLOR_FROM_MAP: bool = True  # Use colors from COLOR_MAP for nodes in plots

        self.NODE_SIZE_DEFAULT: int = 10
        self.NODE_SIZE_PEDESTRIAN: int = 5
        self.NODE_SIZE_CONNECTION: int = 8
        self.NODE_SIZE_VERTICAL: int = 10
        self.NODE_SIZE_ROOM: int = 7
        self.NODE_SIZE_OUTSIDE: int = 4
        self.NODE_OPACITY: float = 0.8
        self.SHOW_PEDESTRIAN_LABELS: bool = False

        self.HORIZONTAL_EDGE_COLOR: str = "#1f77b4"
        self.VERTICAL_EDGE_COLOR: str = "#ff7f0e"
        self.EDGE_WIDTH: float = 0.5

        # SuperNetwork Specific
        self.DEFAULT_FLOOR_HEIGHT: float = 10.0
        self.DEFAULT_VERTICAL_CONNECTION_TOLERANCE: int = 0 # Default pixel distance for connecting vertical nodes across floors
        # Estimated max nodes per floor. Used for pre-allocating ID ranges in multi-processing.
        # Should be an overestimate to avoid ID collisions.
        self.ESTIMATED_MAX_NODES_PER_FLOOR: int = 10000
        self.DEFAULT_OUTSIDE_PROCESSING_IN_SUPERNETWORK: bool = False # Default for processing outside nodes per floor in SuperNetwork
        self.GROUND_FLOOR_NUMBER_FOR_OUTSIDE: Optional[int] = None # Or 0, or None to rely on auto-detection

        # Morphology Kernel
        self.MORPHOLOGY_KERNEL_SIZE: Tuple[int, int] = (5, 5)
        self.CONNECTION_DILATION_KERNEL_SIZE: Tuple[int, int] = (3,3)

        # KDTree query parameters
        self.MESH_NODE_CONNECTIVITY_K: int = 9 # k-nearest neighbors for mesh node connection

        # Ensure paths exist
        self.RESULT_PATH.mkdir(parents=True, exist_ok=True)
        self.DEBUG_PATH.mkdir(parents=True, exist_ok=True)

class RLConfig:
    """存储用于基于强化学习的布局优化的所有配置参数。

    Attributes:
        ROOT_PATH (pathlib.Path): 项目的根目录路径。
        RL_OPTIMIZER_PATH (pathlib.Path): RL优化器模块的根目录。
        DATA_PATH (pathlib.Path): RL优化器的数据目录。
        CACHE_PATH (pathlib.Path): 用于存放所有自动生成的中间文件的缓存目录。
        LOG_PATH (pathlib.Path): 用于存放训练日志和模型的目录。
        TRAVEL_TIMES_CSV (pathlib.Path): 原始通行时间矩阵的CSV文件路径。
        PROCESS_TEMPLATES_JSON (pathlib.Path): 用户定义的就医流程模板文件路径。
        NODE_VARIANTS_JSON (pathlib.Path): 自动生成的节点变体缓存文件路径。
        TRAFFIC_DISTRIBUTION_JSON (pathlib.Path): 自动生成的流量分布缓存文件路径。
        RESOLVED_PATHWAYS_PKL (pathlib.Path): 最终解析出的流线数据缓存文件路径。
        COST_MATRIX_CACHE (pathlib.Path): 预计算成本矩阵的缓存文件路径。
        AREA_SCALING_FACTOR (float): 科室面积允许的缩放容差。
        MANDATORY_ADJACENCY (List[List[str]]): 强制相邻的科室对列表。
        PREFERRED_ADJACENCY (Dict[str, List[List[str]]]): 偏好相邻（软约束）的科室对字典。
        FIXED_NODE_TYPES (List[str]): 在布局中位置固定、不参与优化的节点类型列表。
        EMBEDDING_DIM (int): 节点嵌入向量的维度。
        TRANSFORMER_HEADS (int): Transformer编码器中的多头注意力头数。
        TRANSFORMER_LAYERS (int): Transformer编码器的层数。
        FEATURES_DIM (int): 特征提取器输出的特征维度。
        LEARNING_RATE (float): 优化器的学习率。
        NUM_ENVS (int): 用于训练的并行环境数量。
        NUM_STEPS (int): 每个环境在每次更新前收集的数据步数。
        TOTAL_TIMESTEPS (int): 训练的总时间步数。
        GAMMA (float): 奖励的折扣因子。
        GAE_LAMBDA (float): 通用优势估计(GAE)的lambda参数。
        CLIP_EPS (float): PPO中的裁剪范围。
        ENT_COEF (float): 熵损失的系数，用于鼓励探索。
        BATCH_SIZE (int): 每个优化轮次中使用的批大小。
        NUM_EPOCHS (int): 每次收集数据后，对数据进行优化的轮次。
        REWARD_TIME_WEIGHT (float): 奖励函数中通行时间成本的权重。
        REWARD_ADJACENCY_WEIGHT (float): 奖励函数中相邻性偏好的权重。
        RESUME_TRAINING (bool): 是否启用断点续训功能。
        PRETRAINED_MODEL_PATH (str): 预训练模型路径，用于断点续训。
        CHECKPOINT_FREQUENCY (int): checkpoint保存频率（按训练步数计算）。
        SAVE_TRAINING_STATE (bool): 是否保存完整训练状态（包括优化器、学习率调度器状态）。
    """

    def __init__(self):
        # --- 路径配置 (使用Pathlib) ---
        """
        初始化 RLConfig 类，设置强化学习布局优化所需的所有路径、输入文件、缓存文件、约束参数及模型训练超参数。
        
        该方法会自动创建缓存目录和日志目录（如不存在）。
        """
        self.ROOT_PATH: pathlib.Path = pathlib.Path(__file__).parent.parent
        self.RL_OPTIMIZER_PATH: pathlib.Path = self.ROOT_PATH / 'src' / 'rl_optimizer'
        self.DATA_PATH: pathlib.Path = self.RL_OPTIMIZER_PATH / 'data'
        self.CACHE_PATH: pathlib.Path = self.DATA_PATH / 'cache'
        self.LOG_PATH: pathlib.Path = self.ROOT_PATH / 'logs'
        self.RESULT_PATH: pathlib.Path = self.ROOT_PATH / 'results'

        # --- 输入文件 ---
        self.TRAVEL_TIMES_CSV: pathlib.Path = self.RESULT_PATH / 'network' / 'hospital_travel_times.csv'
        self.PROCESS_TEMPLATES_JSON: pathlib.Path = self.DATA_PATH / 'process_templates.json'

        # --- 自动生成/缓存的中间文件 ---
        self.NODE_VARIANTS_JSON: pathlib.Path = self.CACHE_PATH / 'node_variants.json'
        self.TRAFFIC_DISTRIBUTION_JSON: pathlib.Path = self.CACHE_PATH / 'traffic_distribution.json'
        self.RESOLVED_PATHWAYS_PKL: pathlib.Path = self.CACHE_PATH / 'resolved_pathways.pkl'
        self.COST_MATRIX_CACHE: pathlib.Path = self.CACHE_PATH / 'cost_precomputation.npz'

        # --- 约束配置 ---
        self.AREA_SCALING_FACTOR: float = 0.1
        self.MANDATORY_ADJACENCY: List[List[str]] = []  # 例如: [['手术室_30007', '中心供应室_10003']]
        self.PREFERRED_ADJACENCY: Dict[str, List[List[str]]] = {
            'positive': [], # 例如: [['检验中心_10007', '采血处_20007']]
            'negative': []  # 例如: [['儿科_10006', '急诊科_1']]
        }
        self.FIXED_NODE_TYPES: List[str] = [
            '门', '楼梯', '电梯', '扶梯', '走廊', '墙', '栏杆', 
            '室外', '绿化', '中庭', '空房间'
        ]

        # --- Transformer模型配置 ---
        self.EMBEDDING_DIM: int = 128  # 嵌入维度
        self.TRANSFORMER_HEADS: int = 4  # 多头注意力头数
        self.TRANSFORMER_LAYERS: int = 4  # Transformer层数
        self.TRANSFORMER_DROPOUT: float = 0.1  # Dropout比例
        
        # --- 策略网络配置 ---
        self.POLICY_NET_ARCH: int = 128
        self.POLICY_NET_LAYERS: int = 2
        self.VALUE_NET_ARCH: int = 128
        self.VALUE_NET_LAYERS: int = 2
        
        # --- 学习率调度器配置 ---
        self.LEARNING_RATE_SCHEDULE_TYPE: str = "linear"  # "linear", "constant"
        self.LEARNING_RATE_INITIAL: float = 3e-4  # 初始学习率
        self.LEARNING_RATE_FINAL: float = 1e-5   # 最终学习率（线性衰减的目标值）
        self.LEARNING_RATE: float = 3e-4  # 保持向后兼容性

        # --- PPO 训练超参数 ---
        self.NUM_ENVS: int = 8
        self.N_STEPS: int = 512  # 修正为N_STEPS以匹配PPO参数
        self.TOTAL_TIMESTEPS: int = 5_000_000
        self.GAMMA: float = 0.99
        self.GAE_LAMBDA: float = 0.95
        self.CLIP_RANGE: float = 0.2  # 修正为CLIP_RANGE以匹配PPO参数
        self.ENT_COEF: float = 0.01
        self.VF_COEF: float = 0.5  # 添加值函数损失系数
        self.MAX_GRAD_NORM: float = 0.5  # 添加梯度裁剪参数
        self.BATCH_SIZE: int = 64
        self.N_EPOCHS: int = 10  # 修正为N_EPOCHS以匹配PPO参数
        
        # --- 评估和检查点配置 ---
        self.EVAL_FREQUENCY: int = 10000  # 添加评估频率

        # --- 软约束奖励权重 ---
        self.REWARD_TIME_WEIGHT: float = 1.0
        self.REWARD_ADJACENCY_WEIGHT: float = 0.1

        # --- 断点续训配置 ---
        self.RESUME_TRAINING: bool = False  # 是否启用断点续训
        self.PRETRAINED_MODEL_PATH: str = "data/model"  # 预训练模型路径（用于断点续训）
        self.CHECKPOINT_FREQUENCY: int = 50000  # checkpoint保存频率（训练步数）
        self.SAVE_TRAINING_STATE: bool = True  # 是否保存完整训练状态（优化器、调度器等）

        # 确保关键路径存在
        self.CACHE_PATH.mkdir(parents=True, exist_ok=True)
        self.LOG_PATH.mkdir(parents=True, exist_ok=True)
</file>

<file path="main.py">
"""
医院布局优化系统主入口

统一的命令行接口，支持网络生成、算法优化和结果对比分析。
整合了所有功能模块，提供完整的医院布局优化解决方案。
"""

import argparse
import logging
import sys
import pathlib
from typing import List, Optional, Dict, Any
import pandas as pd

# 导入核心模块
from src.config import NetworkConfig, RLConfig, COLOR_MAP
from src.core.network_generator import NetworkGenerator
from src.core.algorithm_manager import AlgorithmManager
from src.comparison.results_comparator import ResultsComparator

# 导入优化组件
from src.rl_optimizer.data.cache_manager import CacheManager
from src.rl_optimizer.env.cost_calculator import CostCalculator
from src.algorithms.constraint_manager import ConstraintManager

logger = logging.getLogger(__name__)


def setup_logging(level=logging.INFO, log_file: Optional[pathlib.Path] = None):
    """配置日志系统"""
    root_logger = logging.getLogger()
    
    if root_logger.hasHandlers():
        return
    
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    
    # 控制台处理器
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setFormatter(formatter)
    console_handler.setLevel(level)
    root_logger.addHandler(console_handler)
    
    # 文件处理器
    if log_file:
        log_file.parent.mkdir(parents=True, exist_ok=True)
        file_handler = logging.FileHandler(log_file, mode='a', encoding='utf-8')
        file_handler.setFormatter(formatter)
        file_handler.setLevel(level)
        root_logger.addHandler(file_handler)
    
    root_logger.setLevel(level)


class HospitalLayoutOptimizer:
    """医院布局优化系统主类"""
    
    def __init__(self):
        """初始化系统"""
        self.network_config = NetworkConfig(color_map_data=COLOR_MAP)
        self.rl_config = RLConfig()
        self.network_generator = None
        self.algorithm_manager = None
        
        logger.info("医院布局优化系统初始化完成")
    
    def run_network_generation(self, 
                             image_dir: str = "./data/label/",
                             visualization_filename: str = "hospital_network_3d.html",
                             travel_times_filename: str = "hospital_travel_times.csv") -> bool:
        """
        运行网络生成
        
        Args:
            image_dir: 楼层标注图像目录
            visualization_filename: 可视化输出文件名
            travel_times_filename: 行程时间输出文件名
            
        Returns:
            bool: 是否成功
        """
        logger.info("=== 开始网络生成阶段 ===")
        
        self.network_generator = NetworkGenerator(self.network_config)
        
        success = self.network_generator.run_complete_generation(
            image_dir=image_dir,
            visualization_filename=visualization_filename,
            travel_times_filename=travel_times_filename
        )
        
        if success:
            network_info = self.network_generator.get_network_info()
            logger.info("网络生成完成，统计信息:")
            for key, value in network_info.items():
                logger.info(f"  {key}: {value}")
        
        return success
    
    def run_single_algorithm(self, 
                           algorithm_name: str,
                           travel_times_file: str = None,
                           **kwargs) -> bool:
        """
        运行单个优化算法
        
        Args:
            algorithm_name: 算法名称
            travel_times_file: 行程时间文件路径
            **kwargs: 算法特定参数
            
        Returns:
            bool: 是否成功
        """
        logger.info(f"=== 开始运行算法: {algorithm_name} ===")
        
        if travel_times_file is None:
            travel_times_file = self.rl_config.TRAVEL_TIMES_CSV
        
        # 初始化算法管理器
        if not self._initialize_algorithm_manager(travel_times_file):
            return False
        
        try:
            result = self.algorithm_manager.run_single_algorithm(
                algorithm_name=algorithm_name,
                custom_params=kwargs
            )
            
            logger.info(f"算法 {algorithm_name} 执行成功:")
            logger.info(f"  最优成本: {result.best_cost:.2f}")
            logger.info(f"  执行时间: {result.execution_time:.2f}秒")
            logger.info(f"  迭代次数: {result.iterations}")
            
            # 保存结果
            self.algorithm_manager.save_results()
            
            return True
            
        except Exception as e:
            logger.error(f"算法执行失败: {e}", exc_info=True)
            return False
    
    def run_algorithm_comparison(self, 
                               algorithm_names: List[str],
                               travel_times_file: str = None,
                               parallel: bool = False,
                               generate_plots: bool = True,
                               generate_report: bool = True) -> bool:
        """
        运行算法对比分析
        
        Args:
            algorithm_names: 算法名称列表
            travel_times_file: 行程时间文件路径
            parallel: 是否并行执行
            generate_plots: 是否生成图表
            generate_report: 是否生成报告
            
        Returns:
            bool: 是否成功
        """
        logger.info(f"=== 开始算法对比分析: {algorithm_names} ===")
        
        if travel_times_file is None:
            travel_times_file = self.rl_config.TRAVEL_TIMES_CSV
        
        # 初始化算法管理器
        if not self._initialize_algorithm_manager(travel_times_file):
            return False
        
        try:
            # 运行多个算法
            results = self.algorithm_manager.run_multiple_algorithms(
                algorithm_names=algorithm_names,
                parallel=parallel
            )
            
            if not results:
                logger.error("没有算法成功执行")
                return False
            
            logger.info(f"成功执行 {len(results)} 个算法")
            
            # 生成对比表格
            comparison_df = self.algorithm_manager.get_algorithm_comparison()
            logger.info("算法对比结果:")
            logger.info(f"\n{comparison_df.to_string(index=False)}")
            
            # 创建结果对比分析器
            comparator = ResultsComparator(results)
            
            # 生成详细对比表格
            detailed_df = comparator.generate_comparison_table()
            
            # 生成图表
            if generate_plots:
                comparator.create_comparison_plots()
                logger.info("对比图表已生成")
            
            # 生成报告
            if generate_report:
                report_path = comparator.generate_detailed_report()
                logger.info(f"详细报告已生成: {report_path}")
            
            # 导出布局对比
            layouts_path = comparator.export_layouts_comparison()
            logger.info(f"最优布局对比已导出: {layouts_path}")
            
            # 保存结果
            self.algorithm_manager.save_results()
            
            # 输出最佳结果
            best_result = self.algorithm_manager.get_best_result()
            if best_result:
                logger.info(f"整体最佳结果来自: {best_result.algorithm_name}")
                logger.info(f"最优成本: {best_result.best_cost:.2f}")
            
            return True
            
        except Exception as e:
            logger.error(f"算法对比分析失败: {e}", exc_info=True)
            return False
    
    def _initialize_algorithm_manager(self, travel_times_file: str) -> bool:
        """初始化算法管理器"""
        try:
            # 检查行程时间文件
            travel_times_path = pathlib.Path(travel_times_file)
            if not travel_times_path.exists():
                logger.error(f"行程时间文件不存在: {travel_times_file}")
                logger.error("请先运行网络生成阶段：python main.py --mode network")
                return False
            
            logger.info("正在初始化优化组件...")
            
            # 初始化缓存管理器
            cache_manager = CacheManager(self.rl_config)
            logger.info("缓存管理器初始化完成")
            
            # 初始化成本计算器
            cost_calculator = CostCalculator(
                config=self.rl_config,
                resolved_pathways=cache_manager.resolved_pathways,
                travel_times=cache_manager.travel_times_matrix,
                placeable_slots=cache_manager.placeable_slots,
                placeable_departments=cache_manager.placeable_departments
            )
            logger.info("成本计算器初始化完成")
            
            # 初始化约束管理器
            constraint_manager = ConstraintManager(
                placeable_slots=cache_manager.placeable_slots,
                placeable_departments=cache_manager.placeable_departments,
                travel_times=cache_manager.travel_times_matrix
            )
            logger.info("约束管理器初始化完成")
            
            # 初始化算法管理器
            self.algorithm_manager = AlgorithmManager(
                cost_calculator=cost_calculator,
                constraint_manager=constraint_manager,
                config=self.rl_config,
                cache_manager=cache_manager
            )
            logger.info("算法管理器初始化完成")
            
            return True
            
        except Exception as e:
            logger.error(f"算法管理器初始化失败: {e}", exc_info=True)
            return False
    
    def visualize_results(self, results_file: str):
        """可视化算法结果"""
        logger.info(f"=== 开始结果可视化: {results_file} ===")
        # 这里可以添加结果可视化逻辑
        logger.info("结果可视化功能待实现")


def create_argument_parser() -> argparse.ArgumentParser:
    """创建命令行参数解析器"""
    parser = argparse.ArgumentParser(
        description="医院布局优化系统 - 整合网络生成、算法优化和结果对比分析",
        formatter_class=argparse.RawTextHelpFormatter
    )
    
    parser.add_argument(
        '--mode',
        type=str,
        choices=['network', 'optimize', 'compare', 'visualize'],
        required=True,
        help="运行模式:\n"
             "  network    - 生成医院网络和行程时间矩阵\n"
             "  optimize   - 运行单个优化算法\n"
             "  compare    - 运行多个算法进行对比分析\n"
             "  visualize  - 可视化算法结果"
    )
    
    parser.add_argument(
        '--algorithm',
        type=str,
        choices=['ppo', 'simulated_annealing', 'genetic_algorithm'],
        help="优化算法名称 (用于 optimize 模式)"
    )
    
    parser.add_argument(
        '--algorithms',
        type=str,
        help="算法列表，用逗号分隔 (用于 compare 模式)\n"
             "例如: ppo,simulated_annealing,genetic_algorithm"
    )
    
    parser.add_argument(
        '--image-dir',
        type=str,
        default="./data/label/",
        help="楼层标注图像目录 (默认: ./data/label/)"
    )
    
    parser.add_argument(
        '--travel-times-file',
        type=str,
        default=None,
        help="行程时间文件路径 (默认: 由config.py中的RLConfig.TRAVEL_TIMES_CSV指定)"
    )
    
    parser.add_argument(
        '--parallel',
        action='store_true',
        help="并行执行多个算法 (用于 compare 模式)"
    )
    
    parser.add_argument(
        '--no-plots',
        action='store_true',
        help="不生成对比图表"
    )
    
    parser.add_argument(
        '--no-report',
        action='store_true',
        help="不生成详细报告"
    )
    
    parser.add_argument(
        '--results-file',
        type=str,
        help="结果文件路径 (用于 visualize 模式)"
    )
    
    # 算法特定参数
    parser.add_argument(
        '--max-iterations',
        type=int,
        help="最大迭代次数"
    )
    
    parser.add_argument(
        '--population-size',
        type=int,
        help="遗传算法种群大小"
    )
    
    parser.add_argument(
        '--initial-temperature',
        type=float,
        help="模拟退火初始温度"
    )
    
    parser.add_argument(
        '--total-timesteps',
        type=int,
        help="PPO总训练步数"
    )
    
    parser.add_argument(
        '--verbose',
        action='store_true',
        help="详细输出"
    )
    
    return parser


def main():
    """主函数"""
    # 创建参数解析器
    parser = create_argument_parser()
    args = parser.parse_args()
    
    # 设置日志
    log_level = logging.DEBUG if args.verbose else logging.INFO
    log_dir = pathlib.Path("./logs")
    log_dir.mkdir(parents=True, exist_ok=True)
    setup_logging(level=log_level, log_file=log_dir / "hospital_optimizer.log")
    
    logger.info("=== 医院布局优化系统启动 ===")
    logger.info(f"运行模式: {args.mode}")
    
    # 创建系统实例
    optimizer = HospitalLayoutOptimizer()
    
    success = False
    
    try:
        if args.mode == 'network':
            # 网络生成模式
            success = optimizer.run_network_generation(
                image_dir=args.image_dir
            )
            
        elif args.mode == 'optimize':
            # 单算法优化模式
            if not args.algorithm:
                logger.error("optimize 模式需要指定 --algorithm 参数")
                sys.exit(1)
            
            # 构建算法参数
            algorithm_params = {}
            if args.max_iterations:
                algorithm_params['max_iterations'] = args.max_iterations
            if args.population_size:
                algorithm_params['population_size'] = args.population_size
            if args.initial_temperature:
                algorithm_params['initial_temperature'] = args.initial_temperature
            if args.total_timesteps:
                algorithm_params['total_timesteps'] = args.total_timesteps
            
            success = optimizer.run_single_algorithm(
                algorithm_name=args.algorithm,
                travel_times_file=args.travel_times_file,
                **algorithm_params
            )
            
        elif args.mode == 'compare':
            # 算法对比模式
            if not args.algorithms:
                logger.error("compare 模式需要指定 --algorithms 参数")
                sys.exit(1)
            
            algorithm_names = [name.strip() for name in args.algorithms.split(',')]
            
            success = optimizer.run_algorithm_comparison(
                algorithm_names=algorithm_names,
                travel_times_file=args.travel_times_file,
                parallel=args.parallel,
                generate_plots=not args.no_plots,
                generate_report=not args.no_report
            )
            
        elif args.mode == 'visualize':
            # 结果可视化模式
            if not args.results_file:
                logger.error("visualize 模式需要指定 --results-file 参数")
                sys.exit(1)
            
            optimizer.visualize_results(args.results_file)
            success = True
        
        if success:
            logger.info("=== 系统执行成功完成 ===")
        else:
            logger.error("=== 系统执行失败 ===")
            sys.exit(1)
            
    except KeyboardInterrupt:
        logger.warning("用户中断执行")
        sys.exit(1)
    except Exception as e:
        logger.error(f"系统执行异常: {e}", exc_info=True)
        sys.exit(1)


if __name__ == "__main__":
    main()
</file>

</files>
