This file is a merged representation of a subset of the codebase, containing specifically included files, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Only files matching these patterns are included: src/, main.py, result/super_network_travel_times.csv
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
main.py
result/super_network_travel_times.csv
src/analysis/__init__.py
src/analysis/process_flow.py
src/analysis/travel_time.py
src/analysis/word_detect.py
src/config.py
src/graph/graph_manager.py
src/graph/node.py
src/image_processing/processor.py
src/network/floor_manager.py
src/network/network.py
src/network/node_creators.py
src/network/super_network.py
src/optimization/optimizer.py
src/plotting/__init__.py
src/plotting/plotter.py
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="src/analysis/__init__.py">
# src/analysis/__init__.py
"""Analysis module for calculating travel times and other graph metrics."""
from .travel_time import calculate_room_travel_times

__all__ = ["calculate_room_travel_times"]
</file>

<file path="src/graph/graph_manager.py">
"""Manages the graph structure, node storage, and ID generation."""

import itertools
import networkx as nx
import logging
from typing import Dict, Optional, Iterator

from .node import Node

logger = logging.getLogger(__name__)

class GraphManager:
    """
    Manages a networkx graph, stores nodes by ID, and generates unique node IDs.

    Attributes:
        graph (nx.Graph): The underlying networkx graph object.
    """

    def __init__(self, id_generator_start_value: int = 1):
        """
        Initializes the GraphManager.

        Args:
            id_generator_start_value: The starting value for the node ID generator.
                                      This is crucial for multi-processing to ensure
                                      unique IDs across different Network instances.
        """
        self.graph: nx.Graph = nx.Graph()
        self._id_to_node_map: Dict[int, Node] = {}
        # Using itertools.count for a thread-safe (in CPython due to GIL)
        # and efficient ID generator. For true multiprocessing, the start
        # value must be managed externally to ensure uniqueness.
        self._node_id_generator: Iterator[int] = itertools.count(
            id_generator_start_value)
        self._last_generated_id: int = id_generator_start_value - 1

    def generate_node_id(self) -> int:
        """
        Generates and returns a new unique node ID.

        Returns:
            A unique integer ID for a new node.
        """
        new_id = next(self._node_id_generator)
        self._last_generated_id = new_id
        return new_id

    def add_node(self, node: Node) -> None:
        """
        Adds a node to the graph and the ID-to-node map.

        Args:
            node: The Node object to add.

        Raises:
            ValueError: If a node with the same ID already exists.
        """
        if node.id in self._id_to_node_map:
            logger.error(
                f"Node with ID {node.id} already exists in this graph manager.")
        if self.graph.has_node(node):  # Should be redundant if ID is unique
            logger.error(
                f"Node object {node} (ID: {node.id}) already exists in the graph.")

        self.graph.add_node(node, type=node.node_type, pos=node.pos,
                            time=node.time, door_type=node.door_type)
        self._id_to_node_map[node.id] = node

    def get_node_by_id(self, node_id: int) -> Optional[Node]:
        """
        Retrieves a node by its ID.

        Args:
            node_id: The ID of the node to retrieve.

        Returns:
            The Node object if found, otherwise None.
        """
        return self._id_to_node_map.get(node_id)

    def connect_nodes_by_ids(self, node_id1: int, node_id2: int, **edge_attributes) -> bool:
        """
        Connects two nodes in the graph using their IDs.

        Args:
            node_id1: The ID of the first node.
            node_id2: The ID of the second node.
            **edge_attributes: Additional attributes for the edge.

        Returns:
            True if the connection was successful, False if one or both nodes
            were not found, or if they are the same node.
        """
        if node_id1 == node_id2:
            logger.warning(
                f"Warning: Attempted to connect node ID {node_id1} to itself. Skipping.")
            return False

        node1 = self.get_node_by_id(node_id1)
        node2 = self.get_node_by_id(node_id2)

        if node1 and node2:
            if not self.graph.has_edge(node1, node2):
                self.graph.add_edge(node1, node2, **edge_attributes)
            return True
        else:
            missing_ids = []
            if not node1:
                missing_ids.append(node_id1)
            if not node2:
                missing_ids.append(node_id2)
            logger.warning(
                f"Warning: Could not connect nodes. Missing node IDs: {missing_ids}")
            return False
        
    def get_all_nodes(self) -> list[Node]:
        """Returns a list of all Node objects in the graph."""
        return list(self._id_to_node_map.values())
    
    def get_graph_copy(self) -> nx.Graph:
        """
        Returns a deep copy of the internal networkx graph.
        This is important if the graph is to be modified externally
        without affecting the manager's internal state, or for passing
        to other processes.
        """
        return self.graph.copy() # networkx.Graph.copy() is a deep copy by default for node/edge attributes
    
    def get_next_available_node_id_estimate(self) -> int:
        """
        Returns an estimate of the next node ID that would be generated.
        This is primarily for `SuperNetwork` to estimate ID blocks for workers.
        It's `_last_generated_id + 1`.
        """
        return self._last_generated_id + 1
    
    def clear(self, id_generator_start_value: int = 1):
        """
        Clears the graph and resets the ID generator.
        Useful for reusing the manager instance.
        """
        self.graph.clear()
        self._id_to_node_map.clear()
        self._node_id_generator = itertools.count(id_generator_start_value)
        self._last_generated_id = id_generator_start_value - 1
</file>

<file path="src/graph/node.py">
"""Defines the Node class for the network graph."""

from typing import Tuple, Optional

class Node:
    """
    Represents a node in the network graph.

    Attributes:
        id (int): A unique identifier for the node.
        node_type (str): The type of the node (e.g., 'Room', 'Door', 'Corridor').
        pos (Tuple[int, int, int]): The (x, y, z) coordinates of the node.
        time (float): The time cost associated with traversing this node.
        door_type (Optional[str]): Specifies the type of door connection
            (e.g., 'room', 'in', 'out'), if the node is a door.
            Defaults to None.
        area (float): The area occupied by the node in pixel units.
                      For point-like nodes (e.g., mesh centroids), this might be
                      an estimated representative area or a standard small value.
    """
    def __init__(self, node_id: int, node_type: str, pos: Tuple[int, int, int],
                 default_time: float, area: float = 1.0): # Default area to 1 pixel if not specified
        """
        Initializes a Node object.

        Args:
            node_id: The unique identifier for the node.
            node_type: The type of the node.
            pos: The (x, y, z) coordinates of the node.
            default_time: The default time cost for this node type.
            area: The area of the node in pixel units.
        """
        self.id: int = node_id
        self.node_type: str = node_type
        self.pos: Tuple[int, int, int] = pos
        self.time: float = default_time
        self.door_type: Optional[str] = None
        self.area: float = area

    def __repr__(self) -> str:
        """Returns a string representation of the Node."""
        return (f"Node(id={self.id}, type='{self.node_type}', pos={self.pos}, "
                f"time={self.time:.2f}, area={self.area:.2f}, door_type='{self.door_type}')")
    
    def __hash__(self) -> int:
        """Returns the hash of the node based on its ID."""
        return hash(self.id)
    
    def __eq__(self, other: object) -> bool:
        """Checks equality with another Node based on ID."""
        if isinstance(other, Node):
            return self.id == other.id
        return False
</file>

<file path="src/image_processing/processor.py">
"""Handles image loading, preprocessing, and basic morphological operations."""

import cv2
import logging
import numpy as np
from PIL import Image
from scipy.spatial import KDTree
from typing import Tuple, Dict, Any, Optional

from src.config import NetworkConfig

logger = logging.getLogger(__name__)


class ImageProcessor:
    """
    Provides functionalities for image loading, color quantization,
    and morphological operations.
    """

    def __init__(self, config: NetworkConfig, color_map_data: Dict[Tuple[int, int, int], Dict[str, Any]]):
        """
        Initializes the ImageProcessor.

        Args:
            config: The NetworkConfig object.
            color_map_data: The color map dictionary.
        """
        self.config = config
        self.color_map_data = color_map_data
        self._current_image_data: Optional[np.ndarray] = None
        self._image_height: Optional[int] = None
        self._image_width: Optional[int] = None

    def load_and_prepare_image(self, image_path: str) -> np.ndarray:
        """
        Loads an image, rotates it, and stores its dimensions.

        The processed image is stored internally and also returned.

        Args:
            image_path: Path to the image file.

        Returns:
            A NumPy array representing the processed image (RGB).

        Raises:
            FileNotFoundError: If the image_path does not exist.
            IOError: If the image cannot be opened or read.
        """
        try:
            img = Image.open(image_path).convert('RGB')  # Ensure RGB
            if self.config.IMAGE_ROTATE != 0:
                img = img.rotate(self.config.IMAGE_ROTATE)
        except FileNotFoundError:
            raise FileNotFoundError(f"Image file not found: {image_path}")
        except IOError:
            raise IOError(f"Could not open or read image file: {image_path}")

        self._current_image_data = np.asarray(img, dtype=np.uint8)
        if self._current_image_data is None:
            raise ValueError(
                f"Failed to convert image to numpy array: {image_path}")

        self._image_height, self._image_width = self._current_image_data.shape[:2]
        return self._current_image_data.copy()  # Return a copy

    def get_image_dimensions(self) -> Tuple[int, int]:
        """
        Returns the dimensions of the last loaded image.

        Returns:
            A tuple (height, width).

        Raises:
            ValueError: If no image has been loaded yet.
        """
        if self._image_height is None or self._image_width is None:
            raise ValueError(
                "Image not loaded. Call load_and_prepare_image() first.")
        return self._image_height, self._image_width

    def quantize_colors(self, image_data: np.ndarray) -> np.ndarray:
        """
        Replaces each pixel's color in the image with the nearest color
        from the provided color_map using a KDTree for efficiency.

        Args:
            image_data: A NumPy array representing the image (H, W, 3) in RGB.

        Returns:
            A NumPy array of the same shape with colors replaced.
        """
        if not self.color_map_data:
            # If color map is empty, return original image to avoid errors
            logger.warning(
                "Warning: Color map is empty. Returning original image from quantize_colors.")
            return image_data.copy()

        pixels = image_data.reshape(-1, 3)
        map_colors_rgb = list(self.color_map_data.keys())

        if not map_colors_rgb:  # Should not happen if self.color_map_data is not empty
            logger.warning(
                "Warning: No colors in color_map_data keys. Returning original image.")
            return image_data.copy()

        kdtree = KDTree(map_colors_rgb)
        _, closest_indices = kdtree.query(pixels)

        new_pixels = np.array(map_colors_rgb, dtype=np.uint8)[closest_indices]
        new_image = new_pixels.reshape(image_data.shape).astype(np.uint8)
        return new_image

    def apply_morphology(self, mask: np.ndarray, operation: str = 'close_open',
                         kernel_size: Optional[Tuple[int, int]] = None) -> np.ndarray:
        """
        Applies morphological operations to a binary mask.

        Args:
            mask: The input binary mask (NumPy array, dtype=uint8).
            operation: The type of operation.
                       'close_open': MORPH_CLOSE then MORPH_OPEN (default)
                       'open': MORPH_OPEN
                       'close': MORPH_CLOSE
                       'dilate': MORPH_DILATE
                       'erode': MORPH_ERODE
            kernel_size: Tuple (k_height, k_width) for the morphological kernel.
                         Defaults to `config.MORPHOLOGY_KERNEL_SIZE`.

        Returns:
            The processed binary mask.
        """
        if kernel_size is None:
            k_size = self.config.MORPHOLOGY_KERNEL_SIZE
        else:
            k_size = kernel_size

        kernel = np.ones(k_size, np.uint8)
        processed_mask = mask.copy()

        if operation == 'close_open':
            processed_mask = cv2.morphologyEx(
                processed_mask, cv2.MORPH_CLOSE, kernel)
            processed_mask = cv2.morphologyEx(
                processed_mask, cv2.MORPH_OPEN, kernel)
        elif operation == 'open':
            processed_mask = cv2.morphologyEx(
                processed_mask, cv2.MORPH_OPEN, kernel)
        elif operation == 'close':
            processed_mask = cv2.morphologyEx(
                processed_mask, cv2.MORPH_CLOSE, kernel)
        elif operation == 'dilate':
            processed_mask = cv2.dilate(processed_mask, kernel, iterations=1)
        elif operation == 'erode':
            processed_mask = cv2.erode(processed_mask, kernel, iterations=1)
        else:
            raise ValueError(
                f"Unsupported morphological operation: {operation}")

        return processed_mask
    

class DebugImage:
    """Helper class for saving and displaying debug images."""
    count = 0

    def __init__(self, image_data: np.ndarray, save: bool = False,
                 show_napari: bool = False, suffix: str = '',
                 config: NetworkConfig = NetworkConfig()):
        """
        Initializes DebugImage.

        Args:
            image_data: NumPy array of the image to debug.
            save: If True, saves the image.
            show_napari: If True, shows the image using napari (requires napari installed).
            suffix: Suffix for the saved filename.
            debug_path_base: Base directory for saving debug images.
        """
        self.image_to_debug = image_data.copy() # Work with a copy
        self.debug_path = config.DEBUG_PATH
        self.debug_path.mkdir(parents=True, exist_ok=True)

        if save:
            self._save_image(suffix)
        if show_napari:
            self._show_with_napari()

    def _save_image(self, suffix: str = ''):
        """Saves the debug image."""
        filename = f'debug_{DebugImage.count}_{suffix}.png'
        save_path = self.debug_path / filename
        try:
            # Convert to BGR if it's RGB for Pillow saving, or handle grayscale
            if self.image_to_debug.ndim == 3 and self.image_to_debug.shape[2] == 3:
                # Assume RGB from PIL, convert to BGR for OpenCV-style saving or save as is with PIL
                img_to_save_pil = Image.fromarray(self.image_to_debug)
            elif self.image_to_debug.ndim == 2: # Grayscale
                 img_to_save_pil = Image.fromarray(self.image_to_debug, mode='L')
            else:
                logger.warning(f"Warning: Unsupported image format for saving: {self.image_to_debug.shape}")
                return

            img_to_save_pil.save(save_path)
            DebugImage.count += 1
            logger.info(f"Debug image saved to {save_path}")
        except Exception as e:
            logger.error(f"Error saving debug image {save_path}: {e}")


    def _show_with_napari(self):
        """Shows the image using napari."""
        try:
            import napari
            viewer = napari.Viewer()
            viewer.add_image(self.image_to_debug)
            napari.run()
        except ImportError:
            logger.warning("Napari is not installed. Skipping napari display.")
        except Exception as e:
            logger.error(f"Error showing image with napari: {e}")
</file>

<file path="src/network/floor_manager.py">
"""
Manages floor detection from filenames and Z-level calculations for SuperNetwork.
"""

import os
import re
import pathlib
from typing import List, Dict, Tuple, Optional
import logging

logger = logging.getLogger(__name__)

class FloorManager:
    """
    Handles detection of floor numbers from image filenames and calculation
    of their corresponding Z-coordinate levels.
    """

    def __init__(self, base_floor_default: int = 0, default_floor_height: float = 10.0):
        """
        Initializes the FloorManager.

        Args:
            base_floor_default: The default starting floor number if none can be detected.
            default_floor_height: The default height difference between adjacent floors.
        """
        self.base_floor_default = base_floor_default
        self.default_floor_height = default_floor_height
        self._floor_patterns = {
            # Order matters: more specific patterns first
            # B-1, B1, b-1, b1 -> negative
            r'([Bb])-?(\d+)': lambda m: -int(m.group(2)),
            # -1F, -2f -> negative
            r'-(\d+)[Ff]': lambda m: -int(m.group(1)),
            r'([Ll])(\d+)': lambda m: int(m.group(2)),    # L1, l2 -> positive
            r'(\d+)[Ff]': lambda m: int(m.group(1)),      # 1F, 2f -> positive
            # Add more general patterns if needed, like just a number if no prefix/suffix
            # r'_(\d+)_': lambda m: int(m.group(1)) # Example: floor_1_plan.png
        }

    def detect_floor_from_filename(self, file_path: pathlib.Path) -> Optional[int]:
        """
        Detects the floor number from a filename.

        Args:
            file_path: Path object of the image file.

        Returns:
            The detected floor number (integer) or None if not detected.
        """
        filename = file_path.stem  # Get filename without extension
        for pattern, converter in self._floor_patterns.items():
            match = re.search(pattern, filename)
            if match:
                try:
                    return converter(match)
                except ValueError:
                    continue  # Conversion failed, try next pattern
        return None

    def auto_assign_floors(self, image_paths: List[pathlib.Path]) \
            -> Tuple[Dict[pathlib.Path, int], Dict[int, pathlib.Path]]:
        """
        Assigns floor numbers to a list of image paths.

        Attempts to detect from filenames first. If unsuccessful for some or all,
        assigns sequentially based on sort order or a defined starting floor.

        Args:
            image_paths: A list of Path objects for the images.

        Returns:
            A tuple containing:
                - path_to_floor_map (Dict[pathlib.Path, int]): Maps image path to floor number.
                - floor_to_path_map (Dict[int, pathlib.Path]): Maps floor number to image path.
                                                               (Assumes one image per floor for this map)
        """
        path_to_floor_map: Dict[pathlib.Path, int] = {}
        detected_floors: Dict[pathlib.Path, int] = {}
        undetected_paths: List[pathlib.Path] = []

        for p_path in image_paths:
            floor = self.detect_floor_from_filename(p_path)
            if floor is not None:
                if floor in path_to_floor_map.values():
                    # Handle duplicate floor detection if necessary (e.g., error or rename)
                    # For now, we might overwrite or just take the first one.
                    # Let's assume for now floor numbers detected are unique or take the first.
                    # A more robust solution would collect all paths per floor number.
                    logger.warning(
                        f"Warning: Duplicate floor number {floor} detected. Check filenames.")
                detected_floors[p_path] = floor
            else:
                undetected_paths.append(p_path)

        path_to_floor_map.update(detected_floors)

        # Assign floors to undetected paths sequentially
        if undetected_paths:
            # Sort undetected paths to ensure consistent assignment order
            # (e.g., alphabetically or by modification time if relevant)
            undetected_paths.sort()

            # Determine starting floor for sequential assignment
            if detected_floors:
                # Start from one above the highest detected floor, or one below the lowest if all are negative
                all_detected_nos = list(detected_floors.values())
                if all(f < 0 for f in all_detected_nos):  # if all are basement floors
                    start_floor = min(all_detected_nos) - 1 if min(all_detected_nos) - \
                        1 not in all_detected_nos else max(all_detected_nos) + 1
                else:
                    start_floor = max(all_detected_nos) + 1

                # Ensure start_floor is not already taken
                while start_floor in path_to_floor_map.values():
                    start_floor += 1  # simple increment, could be smarter
            else:
                start_floor = self.base_floor_default

            for i, p_path in enumerate(undetected_paths):
                current_assigned_floor = start_floor + i
                while current_assigned_floor in path_to_floor_map.values():  # Avoid collision
                    current_assigned_floor += 1
                path_to_floor_map[p_path] = current_assigned_floor

        # Create the reverse map (floor_to_path_map)
        # This assumes one unique image per floor for this specific map.
        # If multiple images could correspond to the same floor, this needs adjustment.
        floor_to_path_map: Dict[int, pathlib.Path] = {
            v: k for k, v in path_to_floor_map.items()}

        # Verify uniqueness for floor_to_path_map
        if len(floor_to_path_map) != len(path_to_floor_map):
            logger.warning("Non-unique floor numbers assigned or detected, "
                  "floor_to_path_map may not represent all images.")
            # Potentially rebuild floor_to_path_map to store List[pathlib.Path] per floor
            # For now, this structure is kept simple as per original design.

        return path_to_floor_map, floor_to_path_map

    def calculate_z_levels(self, floor_to_path_map: Dict[int, pathlib.Path]) \
            -> Dict[int, float]:
        """
        Calculates the Z-coordinate for each floor.

        Args:
            floor_to_path_map: A map from floor number to image path (used to get sorted floors).

        Returns:
            A dictionary mapping floor number to its Z-coordinate.
        """
        if not floor_to_path_map:
            return {}

        sorted_floor_numbers = sorted(floor_to_path_map.keys())

        # Simple Z level calculation: floor_number * default_floor_height
        # This assumes a consistent floor height and that floor numbers represent relative positions.
        # For example, Floor 0 is at Z=0, Floor 1 at Z=10, Floor -1 at Z=-10.
        z_levels: Dict[int, float] = {
            floor_num: float(floor_num * self.default_floor_height)
            for floor_num in sorted_floor_numbers
        }
        return z_levels
</file>

<file path="src/network/node_creators.py">
"""
Defines strategies for creating different types of nodes in the network.

This module uses the Strategy design pattern where each node type (Room,
Connection, Pedestrian, etc.) has its own creator class derived from a
base class.
"""

import abc
import cv2
import numpy as np
import logging
from scipy.spatial import KDTree
from typing import Dict, Tuple, List, Any, Optional

from src.config import NetworkConfig
from src.graph.node import Node
from src.graph.graph_manager import GraphManager
from src.image_processing.processor import ImageProcessor

logger = logging.getLogger(__name__)


class BaseNodeCreator(abc.ABC):
    """
    Abstract base class for node creators.
    """

    def __init__(self,
                 config: NetworkConfig,
                 color_map_data: Dict[Tuple[int, int, int], Dict[str, Any]],
                 image_processor: ImageProcessor,
                 graph_manager: GraphManager):
        self.config = config
        self.color_map_data = color_map_data
        self.image_processor = image_processor
        self.graph_manager = graph_manager
        self.types_map_name_to_rgb: Dict[str, Tuple[int, int, int]] = \
            {details['name']: rgb for rgb, details in color_map_data.items()}
        self.types_map_name_to_time: Dict[str, float] = \
            {details['name']: details.get('time', 1.0)
             for rgb, details in color_map_data.items()}

    def _get_color_rgb_by_name(self, type_name: str) -> Optional[Tuple[int, int, int]]:
        return self.types_map_name_to_rgb.get(type_name)

    def _get_time_by_name(self, type_name: str) -> float:
        return self.types_map_name_to_time.get(type_name, self.config.PEDESTRIAN_TIME)

    def _create_mask_for_type(self,
                              image_data: np.ndarray,
                              target_type_name: str,
                              apply_morphology: bool = True,
                              morphology_operation: str = 'close_open',
                              morphology_kernel_size: Optional[Tuple[int, int]] = None
                              ) -> Optional[np.ndarray]:
        """Creates a binary mask for a single specified node type."""
        color_rgb = self._get_color_rgb_by_name(target_type_name)
        if color_rgb is None:
            logger.warning(f"Warning: Color for type '{target_type_name}' not found. Cannot create mask.")
            return None

        mask = np.all(image_data == np.array(
            color_rgb, dtype=np.uint8).reshape(1, 1, 3), axis=2)
        mask = mask.astype(np.uint8) * 255

        if apply_morphology:
            kernel_size = morphology_kernel_size or self.config.MORPHOLOGY_KERNEL_SIZE
            mask = self.image_processor.apply_morphology(
                mask,
                operation=morphology_operation,
                kernel_size=kernel_size
            )
        return mask

    def _find_connected_components(self, mask: np.ndarray, connectivity: int = 4) \
            -> Tuple[int, np.ndarray, np.ndarray, np.ndarray]:
        return cv2.connectedComponentsWithStats(mask, connectivity=connectivity)

    @abc.abstractmethod
    def create_nodes(self,
                     processed_image_data: np.ndarray,
                     id_map: np.ndarray,
                     z_level: int):
        pass

class RoomNodeCreator(BaseNodeCreator):
    """Creates nodes for room-type areas."""
    def create_nodes(self, processed_image_data: np.ndarray, id_map: np.ndarray, z_level: int):
        target_room_types = self.config.ROOM_TYPES
        if not target_room_types: return

        for room_type_name in target_room_types:
            mask = self._create_mask_for_type(processed_image_data, room_type_name)
            if mask is None: continue

            retval, labels, stats, centroids = self._find_connected_components(mask)
            if retval <= 1: continue

            node_time = self._get_time_by_name(room_type_name)

            for i in range(1, retval):
                area = float(stats[i, cv2.CC_STAT_AREA])
                if area < self.config.AREA_THRESHOLD: continue

                centroid_x, centroid_y = centroids[i]
                position = (int(centroid_x), int(centroid_y), z_level)
                node_id = self.graph_manager.generate_node_id()
                room_node = Node(node_id=node_id, node_type=room_type_name, pos=position,
                                 default_time=node_time, area=area)
                self.graph_manager.add_node(room_node)
                id_map[labels == i] = room_node.id

class VerticalNodeCreator(BaseNodeCreator):
    """Creates nodes for vertical transport areas (stairs, elevators, escalators)."""
    def create_nodes(self, processed_image_data: np.ndarray, id_map: np.ndarray, z_level: int):
        target_vertical_types = self.config.VERTICAL_TYPES
        if not target_vertical_types: return

        for vertical_type_name in target_vertical_types:
            mask = self._create_mask_for_type(processed_image_data, vertical_type_name)
            if mask is None: continue

            retval, labels, stats, centroids = self._find_connected_components(mask)
            if retval <= 1: continue

            node_time = self._get_time_by_name(vertical_type_name)

            for i in range(1, retval):
                area = float(stats[i, cv2.CC_STAT_AREA])
                if area < self.config.AREA_THRESHOLD: continue

                centroid_x, centroid_y = centroids[i]
                position = (int(centroid_x), int(centroid_y), z_level)
                node_id = self.graph_manager.generate_node_id()
                v_node = Node(node_id=node_id, node_type=vertical_type_name, pos=position,
                              default_time=node_time, area=area)
                self.graph_manager.add_node(v_node)
                id_map[labels == i] = v_node.id

class MeshBasedNodeCreator(BaseNodeCreator): # New base for Pedestrian and Outside
    """Base class for creators that generate a mesh of nodes within areas."""

    def _create_mesh_nodes_for_mask(self,
                                    mask: np.ndarray,
                                    region_type_name: str,
                                    id_map: np.ndarray, # Pass id_map to update
                                    id_map_value_for_area: int, # Value to mark the area in id_map
                                    z_level: int,
                                    node_time: float,
                                    grid_size_multiplier: int):
        """Helper to create mesh nodes within a given mask and connect them."""
        # First, mark the entire area in id_map with the special area identifier
        id_map[mask != 0] = id_map_value_for_area

        retval, labels, stats, _ = self._find_connected_components(mask, connectivity=8)
        grid_size = self.config.GRID_SIZE * grid_size_multiplier
        # Estimated area for a single mesh node
        mesh_node_area = float(1)


        for i in range(1, retval): # For each connected component
            component_area = stats[i, cv2.CC_STAT_AREA]
            if component_area < self.config.AREA_THRESHOLD: # Ensure component itself is large enough
                continue

            x_stat, y_stat, w_stat, h_stat, _ = stats[i]

            gx = np.arange(x_stat + grid_size / 2, x_stat + w_stat, grid_size) # Center points in grid cells
            gy = np.arange(y_stat + grid_size / 2, y_stat + h_stat, grid_size)
            if len(gx) == 0 or len(gy) == 0: continue # Avoid empty grid

            grid_points_x, grid_points_y = np.meshgrid(gx, gy)

            grid_points_y_int = grid_points_y.astype(int).clip(0, mask.shape[0] - 1)
            grid_points_x_int = grid_points_x.astype(int).clip(0, mask.shape[1] - 1)
            
            valid_mask_indices = labels[grid_points_y_int, grid_points_x_int] == i
            
            valid_x_coords = grid_points_x[valid_mask_indices]
            valid_y_coords = grid_points_y[valid_mask_indices]

            component_nodes: List[Node] = []
            for vx, vy in zip(valid_x_coords, valid_y_coords):
                pos = (int(vx), int(vy), z_level)
                node_id = self.graph_manager.generate_node_id()
                mesh_node = Node(node_id, region_type_name, pos, node_time, area=mesh_node_area)
                self.graph_manager.add_node(mesh_node)
                component_nodes.append(mesh_node)
                # Optionally, mark the exact grid cell in id_map with the mesh_node.id
                # id_map[int(vy-grid_size/2):int(vy+grid_size/2), int(vx-grid_size/2):int(vx+grid_size/2)] = mesh_node.id
                # For now, the broader area is already marked.

            if not component_nodes or len(component_nodes) < 2:
                continue

            node_positions_2d = np.array([node.pos[:2] for node in component_nodes])
            kdtree = KDTree(node_positions_2d)
            # Max distance to connect (diagonal of a grid cell, plus a small tolerance)
            max_distance_connect = np.sqrt(2) * grid_size * 1.05

            for j, current_node in enumerate(component_nodes):
                # Query for k-nearest, then filter by distance
                # k=9 includes self + 8 neighbors in a square grid
                distances, indices_k_nearest = kdtree.query(
                    current_node.pos[:2],
                    k=min(len(component_nodes), self.config.MESH_NODE_CONNECTIVITY_K), # Ensure k is not > num_points
                    distance_upper_bound=max_distance_connect
                )

                for dist_val, neighbor_idx in zip(distances, indices_k_nearest):
                    if neighbor_idx >= len(component_nodes) or dist_val > max_distance_connect :
                        continue # Out of bounds or too far

                    neighbor_node = component_nodes[neighbor_idx]
                    if current_node.id == neighbor_node.id:
                        continue
                    
                    self.graph_manager.connect_nodes_by_ids(current_node.id, neighbor_node.id)

class PedestrianNodeCreator(MeshBasedNodeCreator):
    """Creates mesh nodes for pedestrian areas (e.g., corridors)."""
    def create_nodes(self, processed_image_data: np.ndarray, id_map: np.ndarray, z_level: int):
        target_pedestrian_types = self.config.PEDESTRIAN_TYPES
        if not target_pedestrian_types: return

        for ped_type_name in target_pedestrian_types:
            mask = self._create_mask_for_type(processed_image_data, ped_type_name)
            if mask is None: continue
            
            self._create_mesh_nodes_for_mask(
                mask=mask,
                region_type_name=ped_type_name,
                id_map=id_map,
                id_map_value_for_area=self.config.PEDESTRIAN_ID_MAP_VALUE,
                z_level=z_level,
                node_time=self.config.PEDESTRIAN_TIME,
                grid_size_multiplier=1
            )

class OutsideNodeCreator(MeshBasedNodeCreator):
    """Creates mesh nodes for outside areas."""
    def create_nodes(self, processed_image_data: np.ndarray, id_map: np.ndarray, z_level: int):
        target_outside_types = self.config.OUTSIDE_TYPES
        if not target_outside_types: return

        for outside_type_name in target_outside_types: # Should typically be just one '室外'
            mask = self._create_mask_for_type(processed_image_data, outside_type_name)
            if mask is None: continue
            
            self._create_mesh_nodes_for_mask(
                mask=mask,
                region_type_name=outside_type_name,
                id_map=id_map,
                id_map_value_for_area=self.config.OUTSIDE_ID_MAP_VALUE,
                z_level=z_level,
                node_time=self._get_time_by_name(outside_type_name) * self.config.OUTSIDE_MESH_TIMES_FACTOR,
                grid_size_multiplier=self.config.OUTSIDE_MESH_TIMES_FACTOR
            )

class ConnectionNodeCreator(BaseNodeCreator):
    """Creates nodes for connections (e.g., doors) and links them to adjacent areas."""
    def create_nodes(self, processed_image_data: np.ndarray, id_map: np.ndarray, z_level: int):
        target_connection_types = self.config.CONNECTION_TYPES # Typically '门'
        if not target_connection_types: return

        pass_through_ids_in_id_map = [self.config.BACKGROUND_ID_MAP_VALUE]
        dilation_kernel_np = np.ones(self.config.CONNECTION_DILATION_KERNEL_SIZE, np.uint8)

        for conn_type_name in target_connection_types:
            mask = self._create_mask_for_type(processed_image_data, conn_type_name)
            if mask is None: continue

            retval, labels, stats, centroids = self._find_connected_components(mask)
            if retval <= 1: continue

            for i in range(1, retval): # For each door component
                area = float(stats[i, cv2.CC_STAT_AREA])
                # Doors can be smaller, adjust threshold if needed, e.g., AREA_THRESHOLD / 4
                if area < self.config.AREA_THRESHOLD / 10 and area < 5: # Allow very small doors
                    continue

                centroid_x, centroid_y = centroids[i]
                position = (int(centroid_x), int(centroid_y), z_level)

                node_id = self.graph_manager.generate_node_id()
                conn_node = Node(node_id, conn_type_name, position,
                                 self.config.CONNECTION_TIME, area=area)
                self.graph_manager.add_node(conn_node)

                component_mask_pixels = (labels == i)
                id_map[component_mask_pixels] = conn_node.id # Mark door pixels with its own ID

                # Dilate the door component mask to find neighboring regions/nodes in id_map
                # Need to convert boolean mask `component_mask_pixels` to uint8 for dilate
                uint8_component_mask = component_mask_pixels.astype(np.uint8) * 255
                dilated_component_mask = cv2.dilate(uint8_component_mask, dilation_kernel_np, iterations=1)
                
                neighbor_ids_in_map = np.unique(id_map[dilated_component_mask != 0])

                # Determine door type
                is_connected_to_outside = self.config.OUTSIDE_ID_MAP_VALUE in neighbor_ids_in_map
                is_connected_to_pedestrian = self.config.PEDESTRIAN_ID_MAP_VALUE in neighbor_ids_in_map

                if is_connected_to_outside:
                    conn_node.door_type = 'out'
                elif is_connected_to_pedestrian:
                    # If it connects to pedestrian and NOT to outside, it's an 'in' door (e.g. from corridor to room)
                    # Or if it connects pedestrian to room.
                    # If a door connects pedestrian area to an outside area, it's more like an 'out' door.
                    # This needs careful definition based on your use case.
                    # For now: if it sees pedestrian and not outside, consider it 'in' (towards rooms/internal).
                    # If it sees pedestrian AND outside, the 'out' takes precedence.
                    conn_node.door_type = 'in'
                else: # Connects only to actual nodes (rooms, vertical, other doors)
                    conn_node.door_type = 'room' # Default for internal doors

                # Connect the door node to the identified neighboring ACTUAL nodes
                for neighbor_id_val in neighbor_ids_in_map:
                    if neighbor_id_val == conn_node.id or neighbor_id_val in pass_through_ids_in_id_map:
                        continue
                    
                    # Check if it's an actual node ID (positive)
                    # Special area IDs (OUTSIDE_ID_MAP_VALUE, PEDESTRIAN_ID_MAP_VALUE) are negative or large positive.
                    if neighbor_id_val > 0: # Assuming actual node IDs are positive and start from 1
                        target_node = self.graph_manager.get_node_by_id(neighbor_id_val)
                        if target_node and target_node.id != conn_node.id:
                            self.graph_manager.connect_nodes_by_ids(conn_node.id, target_node.id)
                
                # Special handling: if a door is 'out' and also touches a pedestrian area,
                # it might still need a direct link to that pedestrian area's mesh nodes later.
                # Similarly for 'in' doors. This will be handled in a later connection phase.
</file>

<file path="src/optimization/optimizer.py">
"""
Module for facility layout optimization by reassigning functional types
to physical locations to minimize total travel times for defined workflows.
"""

import copy
import logging
from typing import List, Dict, Optional, Tuple, Set, Any, Sequence
import os
from joblib import Parallel, delayed

from src.analysis.process_flow import PeopleFlow, PathFinder
from src.config import NetworkConfig

logger = logging.getLogger(__name__)


class PhysicalLocation:
    """Represents a physical space/node in the facility.

    Attributes:
        name_id: The unique identifier of the physical location (e.g., 'RoomType_123').
        original_functional_type: The functional type initially associated with this
            physical location based on the input data (e.g., ' Radiology').
        area: The area of this physical location.
        is_swappable: Boolean indicating if this location can have its function reassigned.
                      Typically, connection types like 'Door' are not swappable.
    """

    def __init__(self, name_id: str, original_functional_type: str, area: float, is_swappable: bool = True):
        self.name_id: str = name_id
        self.original_functional_type: str = original_functional_type
        self.area: float = area
        self.is_swappable: bool = is_swappable

    def __repr__(self) -> str:
        return (f"PhysicalLocation(name_id='{self.name_id}', "
                f"original_type='{self.original_functional_type}', area={self.area:.2f}, "
                f"swappable={self.is_swappable})")

    def __eq__(self, other: object) -> bool:
        if not isinstance(other, PhysicalLocation):
            return NotImplemented
        return self.name_id == other.name_id

    def __hash__(self) -> int:
        return hash(self.name_id)


class FunctionalAssignment:
    """Manages the assignment of functional types to lists of physical locations.

    This class represents the current "layout" by defining which physical
    locations (Name_IDs) are currently serving each functional type.
    It supports creating new assignment states by swapping the functional roles
    of two physical locations.

    Attributes:
        assignment_map: A dictionary where keys are functional type strings (e.g., "Radiology")
                        and values are lists of Name_ID strings of PhysicalLocations
                        currently assigned to that function.
    """

    def __init__(self, initial_assignment_map: Dict[str, List[str]]):
        # Deepcopy to ensure independence from the source map
        self.assignment_map: Dict[str, List[str]
                                  ] = copy.deepcopy(initial_assignment_map)

    def get_physical_ids_for_function(self, functional_type: str) -> List[str]:
        """Returns the list of physical Name_IDs assigned to the given functional type."""
        return self.assignment_map.get(functional_type, [])

    def get_functional_type_at_physical_id(self, physical_name_id: str) -> Optional[str]:
        """Finds which functional type, if any, the given physical_name_id is currently assigned to."""
        for func_type, id_list in self.assignment_map.items():
            if physical_name_id in id_list:
                return func_type
        return None

    def get_map_copy(self) -> Dict[str, List[str]]:
        """Returns a deep copy of the internal assignment map."""
        return copy.deepcopy(self.assignment_map)

    def apply_functional_swap(self, phys_loc_A_id: str, phys_loc_B_id: str) -> 'FunctionalAssignment':
        """Creates a new FunctionalAssignment representing the state after swapping
        the functional roles currently hosted at phys_loc_A_id and phys_loc_B_id.

        For example, if A hosts 'Radiology' and B hosts 'Lab', the new assignment
        will have A hosting 'Lab' and B hosting 'Radiology'.
        If one location is "unassigned" (not in any list in assignment_map),
        the function from the other location moves to it, and the original location
        becomes unassigned for that function.

        Args:
            phys_loc_A_id: Name_ID of the first physical location.
            phys_loc_B_id: Name_ID of the second physical location.

        Returns:
            A new FunctionalAssignment object with the swapped roles.
        """
        new_map = self.get_map_copy()

        func_type_at_A = self.get_functional_type_at_physical_id(phys_loc_A_id)
        func_type_at_B = self.get_functional_type_at_physical_id(phys_loc_B_id)

        # Remove A from its current function's list (if any) and add B instead.
        if func_type_at_A:
            if phys_loc_A_id in new_map.get(func_type_at_A, []):
                new_map[func_type_at_A].remove(phys_loc_A_id)
            if func_type_at_A not in new_map:
                new_map[func_type_at_A] = []  # Should not happen if found
            new_map[func_type_at_A].append(phys_loc_B_id)
            if not new_map[func_type_at_A]:  # If list became empty
                del new_map[func_type_at_A]

        # Remove B from its current function's list (if any) and add A instead.
        if func_type_at_B:
            # Check if B was already moved (e.g. A and B had same func type)
            if phys_loc_B_id in new_map.get(func_type_at_B, []):
                if func_type_at_A == func_type_at_B:
                    pass
                else:
                    new_map[func_type_at_B].remove(phys_loc_B_id)

            if func_type_at_B not in new_map:
                new_map[func_type_at_B] = []
            new_map[func_type_at_B].append(phys_loc_A_id)
            if not new_map[func_type_at_B]:
                del new_map[func_type_at_B]

        for func_type in list(new_map.keys()):
            if new_map[func_type]:
                new_map[func_type] = sorted(list(set(new_map[func_type])))
            else:
                del new_map[func_type]

        return FunctionalAssignment(new_map)

    def __repr__(self) -> str:
        return f"FunctionalAssignment(map_size={len(self.assignment_map)})"


class WorkflowDefinition:
    """Defines a patient/staff workflow to be evaluated.

    Attributes:
        workflow_id: A unique identifier for this workflow (e.g., 'OutpatientVisit').
        functional_sequence: An ordered list of functional type names representing
                             the steps in the workflow (e.g., ['Reception', 'ConsultRoom', 'Pharmacy']).
        weight: A float representing the importance or frequency of this workflow,
                used in the objective function.
    """

    def __init__(self, workflow_id: str, functional_sequence: List[str], weight: float = 1.0):
        self.workflow_id: str = workflow_id
        self.functional_sequence: List[str] = functional_sequence
        self.weight: float = weight

    def __repr__(self) -> str:
        return (f"WorkflowDefinition(id='{self.workflow_id}', "
                f"seq_len={len(self.functional_sequence)}, weight={self.weight})")


class EvaluatedWorkflowOutcome:
    """Stores the outcome of evaluating a WorkflowDefinition under a specific assignment.

    Attributes:
        workflow_definition: The WorkflowDefinition that was evaluated.
        average_time: The average travel time of all valid PeopleFlows for this workflow
                      under the given assignment. Can be float('inf') if any potential path
                      is unroutable or if no valid paths exist.
        shortest_flow: The actual PeopleFlow object representing the shortest path found
                       for this workflow under the given assignment. Can be None if no
                       valid path was found or if average_time is inf.
        num_paths_considered: The number of valid PeopleFlows used to calculate the average_time.
        all_path_times: A list of travel times for all valid PeopleFlows. Can be None.
    """

    def __init__(self,
                 workflow_definition: WorkflowDefinition,
                 average_time: Optional[float],
                 # Still store the shortest for reference
                 shortest_flow: Optional[PeopleFlow],
                 num_paths_considered: int,
                 all_path_times: Optional[List[float]]):
        self.workflow_definition: WorkflowDefinition = workflow_definition
        self.average_time: float = average_time if average_time is not None else float(
            'inf')
        self.shortest_flow: Optional[PeopleFlow] = shortest_flow
        self.num_paths_considered: int = num_paths_considered
        self.all_path_times: Optional[List[float]] = all_path_times

    def __repr__(self) -> str:
        path_str = "N/A"
        if self.shortest_flow and self.shortest_flow.actual_node_id_sequence:
            shortest_time_val = self.shortest_flow.total_time if self.shortest_flow.total_time is not None else float(
                'inf')
            path_str = (f"Shortest: {self.shortest_flow.actual_node_id_sequence[0]}..."
                        f"{self.shortest_flow.actual_node_id_sequence[-1]} (Time: {shortest_time_val:.2f})")

        return (f"EvaluatedWorkflow(id='{self.workflow_definition.workflow_id}', "
                f"avg_time={self.average_time:.2f}, num_paths={self.num_paths_considered}, {path_str})")

# Helper function for joblib to process a single flow
# This function will be called in parallel for each flow.
# It needs access to a PathFinder instance.


def _calculate_flow_time_joblib_task(flow_to_process: PeopleFlow, path_finder_instance: PathFinder) -> Tuple[Optional[float], Any, List[str]]:
    """
    Calculates total time for a flow and returns time, flow identifier, and sequence.
    The flow_to_process.total_time will be updated by calculate_flow_total_time.
    """
    time = path_finder_instance.calculate_flow_total_time(flow_to_process)
    # Return time and identifiers to re-associate with original flow object if needed,
    # and to allow the main thread to update the original flow objects.
    return time, flow_to_process.identify, flow_to_process.actual_node_id_sequence


class LayoutObjectiveCalculator:
    """Calculates the overall objective value for a given functional assignment.

    The objective is typically the sum of weighted average travel times for all defined workflows.
    """

    def __init__(self,
                 workflow_definitions: Sequence[WorkflowDefinition],
                 path_finder: PathFinder,
                 n_jobs_for_flows: int = -1):  # Default to all cores for flow calculation
        self.workflow_definitions: Sequence[WorkflowDefinition] = workflow_definitions
        self.path_finder: PathFinder = path_finder
        self.n_jobs_for_flows: int = n_jobs_for_flows if n_jobs_for_flows != 0 else 1
        if self.n_jobs_for_flows == -1:  # Check if os.cpu_count() is available
            cpu_count = os.cpu_count()
            self.n_jobs_for_flows = cpu_count if cpu_count is not None else 1
        elif self.n_jobs_for_flows == 0:  # Explicitly set to 1 if 0 is passed.
            self.n_jobs_for_flows = 1

    def evaluate(self, assignment: FunctionalAssignment) -> Tuple[float, List[EvaluatedWorkflowOutcome]]:
        """Evaluates the given FunctionalAssignment.

        Args:
            assignment: The FunctionalAssignment to evaluate.

        Returns:
            A tuple containing:
                - total_objective_value (float): The sum of weighted average travel times.
                  Can be float('inf') if any critical workflow is unroutable.
                - evaluated_outcomes (List[EvaluatedWorkflowOutcome]): A list of outcomes
                  for each workflow definition.
        """
        total_weighted_time: float = 0.0
        evaluated_outcomes: List[EvaluatedWorkflowOutcome] = []

        for wf_def in self.workflow_definitions:
            generated_flows = self.path_finder.generate_flows(
                workflow_names=wf_def.functional_sequence,
                workflow_identifier=wf_def.workflow_id,
                custom_assignment_map=assignment.get_map_copy()
            )

            workflow_average_time: float = float('inf')
            shortest_flow_for_wf: Optional[PeopleFlow] = None
            shortest_time_val_for_wf: float = float('inf')
            valid_flow_times_for_wf: List[float] = []
            num_paths_considered_for_wf: int = 0

            if not generated_flows:
                logger.debug(
                    f"Workflow '{wf_def.workflow_id}' generated no flows for current assignment. Avg Time is Inf.")
            else:
                # Use joblib to parallelize the calculation of total_time for each flow
                # The _calculate_flow_time_joblib_task helper will be used.
                # PathFinder instance needs to be passed to the helper.
                try:
                    # Using 'loky' backend which is robust for multiprocessing
                    # prefer='threads' could be used if tasks are I/O bound and GIL is an issue,
                    # but here it's likely CPU bound due to DataFrame lookups.
                    # 'threading' backend is limited by GIL for CPU-bound tasks in CPython.
                    # 'loky' or 'multiprocessing' are better for CPU-bound.
                    with Parallel(n_jobs=self.n_jobs_for_flows, backend='loky') as parallel:
                        results = parallel(
                            delayed(_calculate_flow_time_joblib_task)(
                                flow, self.path_finder)
                            for flow in generated_flows
                        )
                    # results is a list of (time, flow_identify, flow_sequence)

                except Exception as e:  # Catch potential joblib/pickling errors
                    logger.error(
                        f"Error during parallel flow calculation for workflow '{wf_def.workflow_id}': {e}. Falling back to sequential.")
                    results = [_calculate_flow_time_joblib_task(
                        flow, self.path_finder) for flow in generated_flows]

                all_paths_routable = True
                temp_flow_times: List[float] = []

                # Create a mapping from (identify, tuple(sequence)) to original flow object for quick lookup
                flow_map_for_update: Dict[Tuple[Any, Tuple[str, ...]], PeopleFlow] = {
                    (f.identify, tuple(f.actual_node_id_sequence)): f for f in generated_flows
                }

                for current_flow_time, flow_identify, flow_sequence_list in results:
                    # Ensure hashable for dict key
                    flow_sequence_tuple = tuple(flow_sequence_list)
                    original_flow_obj = flow_map_for_update.get(
                        (flow_identify, flow_sequence_tuple))

                    if original_flow_obj:
                        # Update the total_time on the original PeopleFlow object
                        # Note: _calculate_flow_time_joblib_task already calls calculate_flow_total_time
                        # which updates the flow object passed to it (which is a copy in multiprocessing).
                        # So, we need to update the original object in the main process's list.
                        original_flow_obj.update_total_time(
                            current_flow_time if current_flow_time is not None else float('inf'))
                    else:
                        logger.warning(
                            f"Could not find original flow for id {flow_identify} and sequence {flow_sequence_list} after parallel processing.")
                        # This should ideally not happen if identifiers are unique and sequences match.

                    if current_flow_time is None:
                        logger.debug(f"Flow identified by '{flow_identify}' (sequence: {'->'.join(flow_sequence_list[:2])}... ) "
                                     f"for workflow '{wf_def.workflow_id}' is unroutable. "
                                     f"Workflow average time will be Inf (strict mode).")
                        all_paths_routable = False
                        break  # Strict mode

                    temp_flow_times.append(current_flow_time)
                    if original_flow_obj and current_flow_time < shortest_time_val_for_wf:
                        shortest_time_val_for_wf = current_flow_time
                        shortest_flow_for_wf = original_flow_obj

                if all_paths_routable:
                    if temp_flow_times:
                        valid_flow_times_for_wf = temp_flow_times
                        workflow_average_time = sum(
                            valid_flow_times_for_wf) / len(valid_flow_times_for_wf)
                        num_paths_considered_for_wf = len(
                            valid_flow_times_for_wf)
                    else:
                        logger.warning(
                            f"Workflow '{wf_def.workflow_id}': all_paths_routable is True, but temp_flow_times is empty.")
                        workflow_average_time = float('inf')
                else:
                    workflow_average_time = float('inf')
                    shortest_flow_for_wf = None
                    valid_flow_times_for_wf = []
                    num_paths_considered_for_wf = 0

            outcome = EvaluatedWorkflowOutcome(
                workflow_definition=wf_def,
                average_time=workflow_average_time,
                shortest_flow=shortest_flow_for_wf,
                num_paths_considered=num_paths_considered_for_wf,
                all_path_times=valid_flow_times_for_wf if workflow_average_time != float(
                    'inf') else None
            )
            evaluated_outcomes.append(outcome)

            if workflow_average_time == float('inf'):
                total_weighted_time = float('inf')

            if total_weighted_time != float('inf'):
                total_weighted_time += workflow_average_time * wf_def.weight

        return total_weighted_time, evaluated_outcomes


class LayoutOptimizer:
    """
    Optimizes facility layout by reassigning functional types to physical locations.

    Uses a greedy iterative approach (best-swap local search) to minimize the
    total weighted travel time of predefined workflows.
    """

    def __init__(self,
                 path_finder: PathFinder,
                 workflow_definitions: Sequence[WorkflowDefinition],
                 config: NetworkConfig,
                 area_tolerance_ratio: float = 0.2):
        """
        Initializes the LayoutOptimizer.

        Args:
            path_finder: An initialized PathFinder instance, used for path generation
                         and time calculation. It must have its travel_times_df loaded.
            workflow_definitions: A list of WorkflowDefinition objects that define
                                  the paths and their importance.
            config: The NetworkConfig object, used to identify non-swappable
                    connection types.
            area_tolerance_ratio: The maximum allowed relative area difference between
                                  two physical locations for them to be considered swappable.
                                  E.g., 0.2 means areas can differ by at most 20%.
        """
        self.path_finder: PathFinder = path_finder
        self.config: NetworkConfig = config
        self.objective_calculator: LayoutObjectiveCalculator = LayoutObjectiveCalculator(
            workflow_definitions, path_finder
        )
        self.area_tolerance_ratio: float = area_tolerance_ratio
        self.all_physical_locations: List[PhysicalLocation] = self._initialize_physical_locations(
        )
        self.swappable_locations: List[PhysicalLocation] = [
            loc for loc in self.all_physical_locations if loc.is_swappable
        ]

        logger.info(f"Optimizer initialized. Found {len(self.all_physical_locations)} total physical locations, "
                    f"{len(self.swappable_locations)} are swappable.")

    def _initialize_physical_locations(self) -> List[PhysicalLocation]:
        """
        Creates PhysicalLocation objects from the PathFinder's data.
        """
        locations = []
        if self.path_finder.travel_times_df is None:
            logger.error(
                "PathFinder's travel_times_df is not loaded. Cannot initialize physical locations.")
            return []

        if '面积' not in self.path_finder.travel_times_df.index:
            logger.warning(
                "Optimizer: '面积' row not found in travel_times_df. Areas will default to 0 for locations.")

        for name_id_str in self.path_finder.all_name_ids:
            parts = name_id_str.split('_', 1)
            original_func_type = parts[0]

            area = 0.0
            if '面积' in self.path_finder.travel_times_df.index and \
               name_id_str in self.path_finder.travel_times_df.columns:
                try:
                    area = float(
                        self.path_finder.travel_times_df.loc['面积', name_id_str])
                except ValueError:
                    logger.warning(
                        f"Could not parse area for {name_id_str}. Defaulting to 0.")

            is_swappable = original_func_type not in self.config.CONNECTION_TYPES
            locations.append(PhysicalLocation(
                name_id_str, original_func_type, area, is_swappable))
        return locations

    def _get_valid_swap_pairs(self) -> List[Tuple[PhysicalLocation, PhysicalLocation]]:
        """Generates pairs of swappable physical locations that satisfy area constraints."""
        valid_pairs: List[Tuple[PhysicalLocation, PhysicalLocation]] = []
        num_swappable = len(self.swappable_locations)
        for i in range(num_swappable):
            for j in range(i + 1, num_swappable):
                loc_A = self.swappable_locations[i]
                loc_B = self.swappable_locations[j]

                area_A, area_B = loc_A.area, loc_B.area
                if area_A > 0 and area_B > 0:
                    if abs(area_A - area_B) / max(area_A, area_B) > self.area_tolerance_ratio:
                        continue
                elif (area_A == 0 and area_B > 0) or (area_A > 0 and area_B == 0):
                    if self.area_tolerance_ratio < 1.0:
                        continue

                valid_pairs.append((loc_A, loc_B))
        return valid_pairs

    def run_optimization(self,
                         initial_assignment: FunctionalAssignment,
                         max_iterations: int = 100
                         ) -> Tuple[FunctionalAssignment, float, List[EvaluatedWorkflowOutcome]]:
        """
        Runs the iterative layout optimization process.

        Args:
            initial_assignment: The starting FunctionalAssignment.
            max_iterations: The maximum number of iterations to perform.

        Returns:
            A tuple containing:
                - best_assignment (FunctionalAssignment): The optimized assignment.
                - best_objective_value (float): The objective value of the best_assignment.
                - best_outcomes (List[EvaluatedWorkflowOutcome]): Detailed outcomes for the best_assignment.
        """
        logger.info("Starting layout optimization...")

        current_assignment = initial_assignment
        current_best_objective, current_best_outcomes = self.objective_calculator.evaluate(
            current_assignment)

        logger.info(f"Initial Objective Value: {current_best_objective:.2f}")
        if current_best_objective == float('inf'):
            logger.error(
                "Initial assignment results in unroutable workflows. Optimization aborted.")
            return current_assignment, current_best_objective, current_best_outcomes

        valid_swap_pairs = self._get_valid_swap_pairs()
        if not valid_swap_pairs:
            logger.warning(
                "No valid pairs of physical locations to swap. Optimization cannot proceed.")
            return current_assignment, current_best_objective, current_best_outcomes

        for iteration in range(max_iterations):
            logger.info(f"--- Iteration {iteration + 1}/{max_iterations} ---")

            best_swap_this_iteration: Optional[Tuple[PhysicalLocation,
                                                     PhysicalLocation]] = None
            best_candidate_assignment_this_iteration: Optional[FunctionalAssignment] = None
            objective_after_best_swap_this_iteration = current_best_objective
            outcomes_for_best_swap_this_iteration = current_best_outcomes

            num_evaluated_swaps = 0
            for loc_A, loc_B in valid_swap_pairs:
                candidate_assignment = current_assignment.apply_functional_swap(
                    loc_A.name_id, loc_B.name_id)

                candidate_objective, candidate_outcomes = self.objective_calculator.evaluate(
                    candidate_assignment)
                num_evaluated_swaps += 1
                if num_evaluated_swaps % 100 == 0:
                    logger.debug(
                        f"  Evaluated {num_evaluated_swaps}/{len(valid_swap_pairs)} potential swaps in iteration {iteration+1}...")

                if candidate_objective < objective_after_best_swap_this_iteration:
                    objective_after_best_swap_this_iteration = candidate_objective
                    best_swap_this_iteration = (loc_A, loc_B)
                    best_candidate_assignment_this_iteration = candidate_assignment
                    outcomes_for_best_swap_this_iteration = candidate_outcomes

            if best_swap_this_iteration and best_candidate_assignment_this_iteration is not None:
                swapped_loc_A, swapped_loc_B = best_swap_this_iteration
                improvement = current_best_objective - objective_after_best_swap_this_iteration

                logger.info(
                    f"Improvement found! Swapping functions at '{swapped_loc_A.name_id}' "
                    f"(original: {swapped_loc_A.original_functional_type}, area: {swapped_loc_A.area:.0f}) and "
                    f"'{swapped_loc_B.name_id}' (original: {swapped_loc_B.original_functional_type}, area: {swapped_loc_B.area:.0f})."
                )
                logger.info(
                    f"Objective improved from {current_best_objective:.2f} to {objective_after_best_swap_this_iteration:.2f} (Gain: {improvement:.2f}).")

                current_assignment = best_candidate_assignment_this_iteration
                current_best_objective = objective_after_best_swap_this_iteration
                current_best_outcomes = outcomes_for_best_swap_this_iteration
            else:
                logger.info(
                    "No further improvement found in this iteration. Optimization stopped.")
                break

        if iteration == max_iterations - 1 and best_swap_this_iteration:
            logger.info(
                f"Reached maximum number of iterations ({max_iterations}).")

        logger.info("Layout optimization finished.")
        logger.info(
            f"Final Best Objective Value: {current_best_objective:.2f}")
        return current_assignment, current_best_objective, current_best_outcomes
</file>

<file path="src/plotting/__init__.py">
# src/plotting/__init__.py
"""Plotting module for network visualization."""

from .plotter import BasePlotter, PlotlyPlotter # MatplotlibPlotter can be added later

__all__ = ["BasePlotter", "PlotlyPlotter"]
</file>

<file path="src/analysis/process_flow.py">
import pandas as pd
import logging
from typing import List, Dict, Optional, Tuple, Set, Any, Mapping
from itertools import product

from src.analysis.word_detect import WordDetect
from src.config import NetworkConfig

logger = logging.getLogger(__name__)


class PeopleFlow:
    """
    Represents a specific, resolved flow of an entity through a sequence of nodes.

    Each node in the flow is represented by its unique "Name_ID" string.
    This class primarily stores a fully determined path.

    Attributes:
        identify (Any): An identifier for this specific flow instance or the
            workflow it belongs to.
        actual_node_id_sequence (List[str]): The complete, ordered list of
            "Name_ID" strings representing the flow.
        _cached_hash (Optional[int]): Cached hash value for performance.
    """

    def __init__(self, identify: Any, actual_node_id_sequence: List[str]):
        """
        Initializes a PeopleFlow instance with a specific node ID sequence.

        Args:
            identify (Any): An identifier for this flow.
            actual_node_id_sequence (List[str]): The fully resolved sequence
                of "Name_ID" strings for this flow.
        """
        self.identify: Any = identify
        self.actual_node_id_sequence: List[str] = actual_node_id_sequence
        self.total_time: Optional[float] = None
        self._cached_hash: Optional[int] = None

    def update_total_time(self, total_time: float) -> None:
        """
        Updates the total travel time for this flow.
        """
        self.total_time = total_time

    @property
    def start_node_id(self) -> Optional[str]:
        """Optional[str]: The 'Name_ID' of the first node in the sequence, if any."""
        return self.actual_node_id_sequence[0] if self.actual_node_id_sequence else None

    @property
    def end_node_id(self) -> Optional[str]:
        """Optional[str]: The 'Name_ID' of the last node in the sequence, if any."""
        if len(self.actual_node_id_sequence) > 0:
            return self.actual_node_id_sequence[-1]
        return None

    @property
    def intermediate_node_ids(self) -> List[str]:
        """List[str]: A list of 'Name_ID's for intermediate nodes."""
        if len(self.actual_node_id_sequence) > 2:
            return self.actual_node_id_sequence[1:-1]
        return []

    def __eq__(self, other: object) -> bool:
        """
        Checks equality based on the `identify` and `actual_node_id_sequence`.
        """
        if not isinstance(other, PeopleFlow):
            return NotImplemented
        return (self.identify == other.identify and
                self.actual_node_id_sequence == other.actual_node_id_sequence)

    def __hash__(self) -> int:
        """
        Computes hash based on `identify` and the tuple of `actual_node_id_sequence`.
        """
        if self._cached_hash is None:
            # Making sequence a tuple makes it hashable
            self._cached_hash = hash(
                (self.identify, tuple(self.actual_node_id_sequence)))
        return self._cached_hash

    def __repr__(self) -> str:
        """
        Returns a string representation of the PeopleFlow instance.
        """
        flow_str = " -> ".join(
            self.actual_node_id_sequence) if self.actual_node_id_sequence else "Empty"
        return (
            f"PeopleFlow(identify={self.identify}, "
            f"path=[{flow_str}], "
            f"total_time={self.total_time})"
        )


class PathFinder:
    """
    Finds all possible PeopleFlow paths based on a sequence of node names
    and a CSV file defining available "Name_ID"s and their connections/travel times.
    """

    def __init__(self, csv_filepath: str = None, config: NetworkConfig = None):
        """
        Initializes the PathFinder by loading and processing the CSV file.

        Args:
            csv_filepath (str): Path to the CSV file. The CSV should have
                "Name_ID" formatted strings as column headers and row index.
        """
        if config:
            self.config: NetworkConfig = config
        else:
            self.config: NetworkConfig = NetworkConfig()

        if csv_filepath:
            self.csv_filepath: str = csv_filepath
        else:
            self.csv_filepath: str = self.config.RESULT_PATH / 'super_network_travel_times.csv'

        self.name_to_ids_map: Dict[str, List[str]] = {}
        self.all_name_ids: Set[str] = set()
        self.travel_times_df: Optional[pd.DataFrame] = None
        self._load_and_process_csv()

    def _parse_name_id(self, name_id_str: str) -> Tuple[str, str]:
        """
        Parses a "Name_ID" string into its name and ID components.

        Args:
            name_id_str (str): The string in "Name_ID" format (e.g., "门_11072").

        Returns:
            Tuple[str, str]: (name, id)
        """
        parts = name_id_str.split('_', 1)
        name = parts[0]
        # if no '_', id is same as name
        node_id = parts[1] if len(parts) > 1 else name
        return name, node_id

    def _load_and_process_csv(self) -> None:
        """
        Loads the CSV, extracts "Name_ID"s, and populates the name_to_ids_map.
        """
        try:
            # Assuming the first column is the index and also contains Name_ID
            df = pd.read_csv(self.csv_filepath, index_col=0)
            self.travel_times_df = df
        except FileNotFoundError:
            logger.error(f"Error: CSV file not found at {self.csv_filepath}")
            return
        except Exception as e:
            logger.error(f"Error loading CSV {self.csv_filepath}: {e}")
            return

        # Process column headers (assuming they are the primary source of Name_IDs)
        # and also the index if it's different or more comprehensive
        all_headers = list(df.columns)
        if df.index.name is not None or df.index.dtype == 'object':  # Check if index is meaningful
            all_headers.extend(list(df.index))

        unique_name_ids = sorted(list(set(all_headers)))  # Get unique Name_IDs

        for name_id_str in unique_name_ids:
            if not isinstance(name_id_str, str) or '_' not in name_id_str:
                # Skip non-string headers or headers not in expected format, like "面积"
                # print(f"Skipping header/index: {name_id_str} as it's not in Name_ID format.")
                continue

            self.all_name_ids.add(name_id_str)
            name, _ = self._parse_name_id(name_id_str)
            if name not in self.name_to_ids_map:
                self.name_to_ids_map[name] = []
            # Ensure no duplicates if a Name_ID appears in both columns and index
            if name_id_str not in self.name_to_ids_map[name]:
                self.name_to_ids_map[name].append(name_id_str)

    def transform_workflow_name(self, workflow_names: List[str]) -> str:
        """
        Transforms a workflow name by detecting the nearest word in the CSV.
        """
        word_detect = WordDetect(config=self.config)
        all_type_names = self.config.ALL_TYPES
        return word_detect.detect_nearest_word(workflow_names, all_type_names)

    def generate_flows(self,
                       workflow_names: List[str],
                       workflow_identifier: Any = 0,
                       # Changed Dict to Mapping for broader type hint
                       custom_assignment_map: Optional[Mapping[str,
                                                               List[str]]] = None
                       ) -> List[PeopleFlow]:
        """Generates all possible PeopleFlow objects for a given sequence of node names.

        Uses custom_assignment_map if provided for resolving functional names to
        physical Name_IDs, otherwise defaults to the instance's self.name_to_ids_map
        which is loaded from the CSV during initialization.

        Args:
            workflow_names: An ordered list of functional node names.
            workflow_identifier: An identifier for the generated PeopleFlow objects.
            custom_assignment_map: An optional mapping where keys are functional type
                names (e.g., '妇科') and values are lists of physical Name_ID strings
                (e.g., ['妇科_101', '妇科_102']) that currently fulfill that function.

        Returns:
            A list of PeopleFlow objects.
        """
        if not workflow_names:
            return []

        # Use the provided custom_assignment_map if available, otherwise use the default one
        assignment_to_use = custom_assignment_map if custom_assignment_map is not None else self.name_to_ids_map

        # Convert node names in `workflow_names` to the closest node types (e.g., from config.ALL_TYPES)
        # This step helps normalize user input if workflow_names might not exactly match official types.

        possible_ids_per_step: List[List[str]] = []
        for name in workflow_names:
            # Use the determined assignment map
            ids_for_name = assignment_to_use.get(name)
            if not ids_for_name:
                logger.warning(
                    f"Functional type '{name}' (from original '{workflow_names[workflow_names.index(name)] if name in workflow_names else 'N/A'}') "
                    f"not found in the current assignment map. Workflow '{workflow_identifier}' cannot be fully resolved."
                )
                return []  # Cannot generate flows if a step is unresolvable
            possible_ids_per_step.append(ids_for_name)

        # Use itertools.product to get all combinations of Name_IDs
        all_possible_id_sequences = product(*possible_ids_per_step)

        generated_flows: List[PeopleFlow] = []
        # Use a local counter for unique flow IDs within this specific generation call,
        # prefixed by the workflow_identifier.
        flow_counter_for_identifier = 0
        for id_sequence_tuple in all_possible_id_sequences:
            # Create a more unique identifier for the flow if multiple flows are generated for one workflow_identifier
            current_flow_id = f"{workflow_identifier}_{flow_counter_for_identifier}" \
                if isinstance(workflow_identifier, str) else (workflow_identifier, flow_counter_for_identifier)

            flow = PeopleFlow(identify=current_flow_id,
                              actual_node_id_sequence=list(id_sequence_tuple))
            flow_counter_for_identifier += 1
            generated_flows.append(flow)

        return generated_flows

    def get_travel_time(self, from_name_id: str, to_name_id: str) -> Optional[float]:
        """
        Gets the travel time between two specific "Name_ID" nodes.

        Args:
            from_name_id (str): The "Name_ID" of the source node.
            to_name_id (str): The "Name_ID" of the target node.

        Returns:
            Optional[float]: The travel time if connection exists, else None.
                           Returns None also if dataframe isn't loaded.
        """
        if self.travel_times_df is None:
            logger.warning("Warning: Travel times DataFrame not loaded.")
            return None
        try:
            # Ensure both from_name_id and to_name_id are in the DataFrame's
            # index and columns to avoid KeyError
            if from_name_id in self.travel_times_df.index and \
               to_name_id in self.travel_times_df.columns:
                time = self.travel_times_df.loc[from_name_id, to_name_id]
                return float(time)  # Ensure it's a float
            else:
                logger.warning(
                    f"Warning: Node {from_name_id} or {to_name_id} not in travel matrix.")
                return None  # Or handle as an error, e.g., raise ValueError
        except KeyError:
            logger.warning(
                f"KeyError: Node {from_name_id} or {to_name_id} not found in travel matrix.")
            return None
        except ValueError:  # If conversion to float fails for some reason
            logger.warning(
                f"ValueError: Travel time for {from_name_id} to {to_name_id} is not a valid number.")
            return None

    def calculate_flow_total_time(self, flow: PeopleFlow) -> Optional[float]:
        """
        Calculates the total travel time for a given PeopleFlow.

        Args:
            flow (PeopleFlow): The PeopleFlow object.

        Returns:
            Optional[float]: The total travel time for the flow.
                             Returns None if any segment has no travel time.
                             Returns 0.0 for single-node flows.
        """
        if not flow.actual_node_id_sequence:
            return 0.0
        if len(flow.actual_node_id_sequence) == 1:
            return 0.0  # No travel for a single point

        total_time = 0.0
        for i in range(len(flow.actual_node_id_sequence) - 1):
            from_node = flow.actual_node_id_sequence[i]
            to_node = flow.actual_node_id_sequence[i+1]
            segment_time = self.get_travel_time(from_node, to_node)
            if segment_time is None:
                logger.warning(
                    f"Warning: No travel time for segment {from_node} -> {to_node} in flow {flow.identify}.")
                return None  # Or handle this case differently, e.g. infinite time
            total_time += segment_time
        flow.update_total_time(total_time)
        return total_time
</file>

<file path="src/analysis/word_detect.py">
import torch
import logging
from sentence_transformers import SentenceTransformer, util
from src.config import NetworkConfig

logger = logging.getLogger(__name__)

class WordDetect:
    def __init__(self, model: SentenceTransformer = None, config: NetworkConfig = None):
        self.model = model
        self.config = config
        self._initialize_model()

    def _initialize_model(self):
        if not self.model:
            model_name = 'paraphrase-multilingual-MiniLM-L12-v2'
            self.model = SentenceTransformer(model_name)
            logger.info(f'Loaded model: {model_name}')

    def _detect_nearest_word(self, query_word: str, word_list: list[str] = None):
        if word_list is None:
            word_list = self.config.ALL_TYPES
        query_embedding = self.model.encode(query_word, convert_to_tensor=True)
        list_embedding = self.model.encode(word_list, convert_to_tensor=True)
        cosine_scores = util.pytorch_cos_sim(query_embedding, list_embedding)
        max_score_index = torch.argmax(cosine_scores)
        return word_list[max_score_index]

    def detect_nearest_word(self, query_word: str | list[str], word_list: list[str] = None):
        if isinstance(query_word, str):
            return self._detect_nearest_word(query_word, word_list)
        elif isinstance(query_word, list):
            return [self._detect_nearest_word(word, word_list) for word in query_word]
        else:
            raise ValueError(f"Invalid query_word type: {type(query_word)}")
</file>

<file path="src/network/super_network.py">
"""
Manages the construction of a multi-floor network by orchestrating
individual Network instances, potentially in parallel.
"""

import multiprocessing
import os  # For os.cpu_count()
import pathlib
import networkx as nx
import numpy as np
import logging
from typing import List, Dict, Tuple, Optional, Any
from scipy.spatial import KDTree

from src.config import NetworkConfig
from src.graph.node import Node
from .network import Network  # The single-floor network builder
from .floor_manager import FloorManager

logger = logging.getLogger(__name__)

# Worker function for multiprocessing - must be defined at the top-level or picklable


def _process_floor_worker(task_args: Tuple[pathlib.Path, float, int, Dict[str, Any], Dict[Tuple[int, int, int], Dict[str, Any]], bool]) \
        -> Tuple[Optional[nx.Graph], Optional[int], Optional[int], int, pathlib.Path, float]:
    """
    Worker function to process a single floor's network generation.

    Args:
        task_args: A tuple containing:
            - image_path (pathlib.Path): Path to the floor image.
            - z_level (float): Z-coordinate for this floor.
            - id_start_value (int): Starting node ID for this floor.
            - config_dict (Dict): Dictionary representation of NetworkConfig.
            - color_map_data (Dict): The color map.
            - process_outside_nodes (bool): Flag to process outside nodes.

    Returns:
        A tuple containing:
            - graph (Optional[nx.Graph]): Generated graph for the floor, or None on error.
            - width (Optional[int]): Image width, or None on error.
            - height (Optional[int]): Image height, or None on error.
            - next_id_val_from_worker (int): The next available ID from this worker's GraphManager.
            - image_path (pathlib.Path): Original image path (for result matching).
            - z_level (float): Original z_level (for result matching).
    """
    image_path, z_level, id_start_value, config_dict, color_map_data, process_outside_nodes = task_args
    try:
        # Reconstruct config from dict for the worker process
        # Note: This assumes NetworkConfig can be reconstructed from its __dict__
        # and COLOR_MAP is passed directly.
        # A more robust way might be to pass necessary primitive types or use a dedicated
        # config serialization if NetworkConfig becomes very complex.
        # Initialize with color_map
        worker_config = NetworkConfig(color_map_data=color_map_data)
        # Update other attributes from the passed dictionary
        for key, value in config_dict.items():
            # Avoid re-assigning COLOR_MAP
            if key != "COLOR_MAP" and hasattr(worker_config, key):
                setattr(worker_config, key, value)

        network_builder = Network(
            config=worker_config,
            color_map_data=color_map_data,
            id_generator_start_value=id_start_value
        )
        graph, width, height, next_id = network_builder.run(
            image_path=str(image_path),  # network.run expects str path
            z_level=z_level,
            process_outside_nodes=process_outside_nodes
        )
        return graph, width, height, next_id, image_path, z_level
    except Exception as e:
        logger.error(
            f"Error processing floor {image_path.name} in worker: {e}")
        # Return next_id_val as id_start_value + config.ESTIMATED_MAX_NODES_PER_FLOOR
        # to ensure main process ID allocation remains consistent even on worker failure.
        # A more sophisticated error handling might be needed.
        est_next_id = id_start_value + \
            config_dict.get("ESTIMATED_MAX_NODES_PER_FLOOR", 10000)
        return None, None, None, est_next_id, image_path, z_level


class SuperNetwork:
    """
    Orchestrates the creation of a multi-floor network graph.

    It manages multiple Network instances, one for each floor, and combines
    their graphs. It supports parallel processing of floors.
    """

    def __init__(self,
                 config: NetworkConfig,
                 color_map_data: Dict[Tuple[int, int, int], Dict[str, Any]],
                 num_processes: Optional[int] = None,
                 base_floor: int = 0,
                 default_floor_height: Optional[float] = None,
                 vertical_connection_tolerance: Optional[int] = None):
        """
        Initializes the SuperNetwork.

        Args:
            config: The main configuration object.
            color_map_data: The RGB color to type mapping.
            num_processes: Number of processes to use for parallel floor processing.
                           Defaults to os.cpu_count().
            base_floor: Default base floor number if not detected from filename.
            default_floor_height: Default height between floors. Uses config value if None.
            vertical_connection_tolerance: Pixel distance tolerance for connecting
                                           vertical nodes between floors. Uses config if None.
        """
        self.config = config
        self.color_map_data = color_map_data
        self.super_graph: nx.Graph = nx.Graph()
        self.designated_ground_floor_number: Optional[int] = None
        self.designated_ground_floor_z: Optional[float] = None

        self.num_processes: int = num_processes if num_processes is not None else (
            os.cpu_count() or 1)

        _floor_height = default_floor_height if default_floor_height is not None else config.DEFAULT_FLOOR_HEIGHT
        self.floor_manager = FloorManager(
            base_floor_default=base_floor, default_floor_height=_floor_height)

        self.vertical_connection_tolerance: int = vertical_connection_tolerance \
            if vertical_connection_tolerance is not None else config.DEFAULT_VERTICAL_CONNECTION_TOLERANCE

        self.floor_z_map: Dict[int, float] = {}  # floor_number -> z_coordinate
        self.path_to_floor_map: Dict[pathlib.Path,
                                     int] = {}  # image_path -> floor_number

        self.width: Optional[int] = None
        self.height: Optional[int] = None

    def _prepare_floor_data(self, image_file_paths: List[pathlib.Path],
                           z_levels_override: Optional[List[float]] = None) \
            -> List[Tuple[pathlib.Path, float, bool]]:
        """
        Determines floor numbers and Z-levels for each image path.
        Also determines if outside nodes should be processed for that floor.
        Outside nodes are processed ONLY for the designated ground/first floor.

        Returns:
            A list of tuples: (image_path, z_level, process_outside_nodes_flag)
        """
        image_paths_as_pathlib = [pathlib.Path(p) for p in image_file_paths]
        
        self.path_to_floor_map, floor_to_path_map = self.floor_manager.auto_assign_floors(image_paths_as_pathlib)
        
        if z_levels_override and len(z_levels_override) == len(image_paths_as_pathlib):
            # ... (z_levels_override 逻辑保持不变) ...
            sorted_paths_by_floor = sorted(self.path_to_floor_map.keys(), key=lambda p: self.path_to_floor_map[p])
            temp_path_to_z = {path: z for path, z in zip(sorted_paths_by_floor, z_levels_override)} # Make sure this aligns correctly
            self.floor_z_map = {self.path_to_floor_map[p]: temp_path_to_z.get(p) for p in self.path_to_floor_map.keys() if temp_path_to_z.get(p) is not None}

        else:
            self.floor_z_map = self.floor_manager.calculate_z_levels(floor_to_path_map)

        if not self.floor_z_map:
            # logger.error("Could not determine Z-levels for floors.") # Ensure logger is available
            raise ValueError("Could not determine Z-levels for floors.")

        floor_tasks_data = []
        all_floor_nums = list(self.floor_z_map.keys()) # These are the actual floor numbers (e.g., -1, 0, 1, 2)

        if not all_floor_nums:
            return []

        # Determine the ground floor number for processing outside nodes.
        # Strategy: Lowest non-negative floor number. If all are negative, no ground floor with outside.
        # If you have a specific config for ground floor, use that.
        # Example: self.config.GROUND_FLOOR_NUMBER (e.g., 1 or 0)
        
        # Let's find the designated ground floor number.
        # Assuming '1' is the typical first/ground floor if positive floors exist.
        # If '0' exists and is the lowest non-negative, it's the ground floor.
        # Otherwise, if only positive floors, the smallest positive is ground.
        # If only negative floors, then no outside nodes unless specifically handled.
        
        designated_ground_floor_num: Optional[int] = self.config.GROUND_FLOOR_NUMBER_FOR_OUTSIDE
        
        if designated_ground_floor_num is None: # If not set in config, try auto-detection
            positive_or_zero_floors = sorted([fn for fn in all_floor_nums if fn >= 0])
            if 0 in all_floor_nums:
                designated_ground_floor_num = 0
            elif 1 in all_floor_nums and not positive_or_zero_floors : # Check if 1 is the only positive candidate
                 if not any(0 <= fn < 1 for fn in all_floor_nums): # Ensure no 0.x floors if 1 is chosen
                    designated_ground_floor_num = 1
            elif positive_or_zero_floors:
                designated_ground_floor_num = positive_or_zero_floors[0]
        
        # logger.info(f"Designated ground floor for outside nodes: {designated_ground_floor_num}")

        for p_path, floor_num in self.path_to_floor_map.items():
            z_level = self.floor_z_map.get(floor_num)
            if z_level is None:
                # logger.warning(f"Z-level for floor {floor_num} (path {p_path}) not found. Skipping task.")
                continue

            # Process outside nodes ONLY if the current floor is the designated ground floor
            # AND if there's at least one "OUTSIDE_TYPE" defined in config.
            process_outside = False
            if designated_ground_floor_num is not None and floor_num == designated_ground_floor_num \
               and self.config.OUTSIDE_TYPES:
                process_outside = True
            
            # Override via config if a global "always process outside" is set (though less likely now)
            if self.config.DEFAULT_OUTSIDE_PROCESSING_IN_SUPERNETWORK: # This flag might be re-purposed or removed
                # If this flag is true, it might override the ground-floor-only logic.
                # For "only on ground floor", this should typically be false.
                # Let's assume the ground_floor_num logic is primary.
                # So, if DEFAULT_OUTSIDE_PROCESSING_IN_SUPERNETWORK is true, it processes for all.
                # If false (typical for this new requirement), then only for designated_ground_floor_num.
                if self.config.DEFAULT_OUTSIDE_PROCESSING_IN_SUPERNETWORK:
                    process_outside = True 
                # else: it remains as determined by ground_floor_num logic

            # logger.debug(f"Floor task: Path={p_path.name}, FloorNum={floor_num}, Z={z_level:.2f}, ProcessOutside={process_outside}")
            floor_tasks_data.append((p_path, z_level, process_outside))
        
        floor_tasks_data.sort(key=lambda item: item[1]) # Sort by Z-level
        
        self.designated_ground_floor_number = designated_ground_floor_num
        if self.designated_ground_floor_number is not None:
            self.designated_ground_floor_z = self.floor_z_map.get(self.designated_ground_floor_number)

        return floor_tasks_data

    def run(self,
            image_file_paths: List[str],  # List of string paths from main
            z_levels_override: Optional[List[float]] = None,
            force_vertical_tolerance: Optional[int] = None) -> nx.Graph:
        """
        Builds the multi-floor network.

        Args:
            image_file_paths: List of string paths to floor images.
            z_levels_override: Optional list to manually set Z-levels for each image.
                               Order should correspond to sorted floor order or be a path-to-z map.
            force_vertical_tolerance: Optionally override the vertical connection tolerance.

        Returns:
            The combined multi-floor NetworkX graph.
        """
        self.super_graph.clear()  # Clear previous graph if any
        image_paths_pl = [pathlib.Path(p) for p in image_file_paths]

        floor_run_data = self._prepare_floor_data(
            image_paths_pl, z_levels_override)
        if not floor_run_data:
            logger.warning("Warning: No floor data to process.")
            return self.super_graph

        tasks_for_pool = []
        current_id_start = 1
        config_dict_serializable = self.config.__dict__.copy()
        # COLOR_MAP is already part of config_dict_serializable if NetworkConfig init stores it.
        # If COLOR_MAP is global, it's fine for multiprocessing on systems where memory is copied (fork).
        # For spawn, it needs to be picklable or passed. Here, color_map_data is passed.

        for p_path, z_level, process_outside_flag in floor_run_data:
            tasks_for_pool.append((
                p_path, z_level, current_id_start,
                config_dict_serializable, self.color_map_data, process_outside_flag
            ))
            current_id_start += self.config.ESTIMATED_MAX_NODES_PER_FLOOR

        logging.info(
            f"Starting parallel processing of {len(tasks_for_pool)} floors using {self.num_processes} processes...")

        results = []
        # Use with statement for Pool to ensure proper cleanup
        # Only use pool if multiple tasks and processes
        if self.num_processes > 1 and len(tasks_for_pool) > 1:
            with multiprocessing.Pool(processes=self.num_processes) as pool:
                results = pool.map(_process_floor_worker, tasks_for_pool)
        else:  # Run sequentially for single process or single task
            logging.info("Running floor processing sequentially...")
            for task in tasks_for_pool:
                results.append(_process_floor_worker(task))

        first_floor_processed = True
        # To store (graph, width, height) for valid results
        processed_graphs_data = []

        for graph_result, width_res, height_res, _next_id, res_path, res_z in results:
            if graph_result is None or width_res is None or height_res is None:
                logger.warning(
                    f"Warning: Failed to process floor image {res_path.name} (z={res_z}). Skipping.")
                continue

            if first_floor_processed:
                self.width = width_res
                self.height = height_res
                first_floor_processed = False
            elif self.width != width_res or self.height != height_res:
                raise ValueError(
                    f"Image dimensions mismatch for {res_path.name}. "
                    f"Expected ({self.width},{self.height}), got ({width_res},{height_res}). "
                    "All floor images must have the same dimensions."
                )

            processed_graphs_data.append(
                graph_result)  # Store the graph itself

        # Combine graphs
        for floor_graph in processed_graphs_data:
            # Nodes in floor_graph should already have all attributes from Node class
            # and GraphManager.add_node should have added them to nx.Graph.
            # Make sure node objects themselves are added, not just IDs.
            self.super_graph.add_nodes_from(floor_graph.nodes(data=True))
            self.super_graph.add_edges_from(floor_graph.edges(data=True))

        if force_vertical_tolerance is not None:
            self.vertical_connection_tolerance = force_vertical_tolerance
        elif self.config.DEFAULT_VERTICAL_CONNECTION_TOLERANCE == 0:  # 0 might mean auto-calculate
            self.vertical_connection_tolerance = self._auto_calculate_vertical_tolerance()

        self._connect_floors()

        logger.info(
            f"SuperNetwork construction complete. Total nodes: {self.super_graph.number_of_nodes()}")
        return self.super_graph

    def _auto_calculate_vertical_tolerance(self) -> int:
        """
        Automatically calculates a tolerance for connecting vertical nodes
        based on their typical proximity (if not specified).
        """
        vertical_nodes = [
            node for node in self.super_graph.nodes()  # Get node objects
            if isinstance(node, Node) and node.node_type in self.config.VERTICAL_TYPES
        ]
        if not vertical_nodes or len(vertical_nodes) < 2:
            return self.config.DEFAULT_VERTICAL_CONNECTION_TOLERANCE  # Fallback

        # Consider only XY positions for tolerance calculation
        positions_xy = np.array([node.pos[:2] for node in vertical_nodes])
        if len(positions_xy) < 2:
            return self.config.DEFAULT_VERTICAL_CONNECTION_TOLERANCE

        try:
            tree = KDTree(positions_xy)
            # Find distance to the nearest neighbor for each vertical node (excluding itself)
            # k=2 includes self and nearest
            distances, _ = tree.query(positions_xy, k=2)

            # Use distances to the actual nearest neighbor (second column)
            # Filter out zero distances if k=1 was used or if duplicates exist
            # Avoid self-match if k=1
            nearest_distances = distances[:, 1][distances[:, 1] > 1e-6]

            if nearest_distances.size == 0:
                return self.config.DEFAULT_VERTICAL_CONNECTION_TOLERANCE

            avg_min_distance = np.mean(nearest_distances)
            # Tolerance could be a factor of this average minimum distance
            # Example: 50% of avg min distance
            calculated_tolerance = int(avg_min_distance * 0.5)
            logger.info(
                f"Auto-calculated vertical tolerance: {calculated_tolerance} (based on avg_min_dist: {avg_min_distance:.2f})")
            return max(10, calculated_tolerance)  # Ensure a minimum tolerance
        except Exception as e:
            logger.error(
                f"Error in auto-calculating tolerance: {e}. Using default.")
            return self.config.DEFAULT_VERTICAL_CONNECTION_TOLERANCE

    def _connect_floors(self) -> None:
        """
        Connects vertical transport nodes (e.g., stairs, elevators) between
        different floors if they are of the same type and spatially close in XY.
        """
        all_vertical_nodes_in_graph = [
            node for node in self.super_graph.nodes()  # Iterating actual Node objects
            if isinstance(node, Node) and node.node_type in self.config.VERTICAL_TYPES
        ]

        if not all_vertical_nodes_in_graph:
            logger.info("No vertical nodes found to connect between floors.")
            return

        # Group vertical nodes by their specific type (e.g., 'Stairs', 'Elevator')
        nodes_by_type: Dict[str, List[Node]] = {}
        for node in all_vertical_nodes_in_graph:
            nodes_by_type.setdefault(node.node_type, []).append(node)

        logger.info(
            f"Attempting to connect floors. Tolerance: {self.vertical_connection_tolerance} pixels.")
        connected_pairs_count = 0

        for node_type, nodes_of_this_type in nodes_by_type.items():
            if len(nodes_of_this_type) < 2:
                continue  # Not enough nodes of this type to form a connection

            # Sort nodes by Z-level, then by Y, then by X for potentially more stable pairing
            # Though KDTree approach doesn't strictly need pre-sorting.
            nodes_of_this_type.sort(
                key=lambda n: (n.pos[2], n.pos[1], n.pos[0]))

            # Build KDTree for XY positions of nodes of this specific type
            positions_xy = np.array([node.pos[:2]
                                    for node in nodes_of_this_type])
            if positions_xy.shape[0] < 2:
                continue  # Need at least 2 points for KDTree sensible query

            try:
                kdtree = KDTree(positions_xy)
            except Exception as e:
                logger.error(
                    f"Could not build KDTree for vertical node type {node_type}: {e}")
                continue

            processed_nodes_indices = set()  # To avoid redundant checks

            for i, current_node in enumerate(nodes_of_this_type):
                if i in processed_nodes_indices:
                    continue

                # Query for other nodes of the SAME TYPE within the XY tolerance
                # query_ball_point returns indices into the `positions_xy` array
                indices_in_ball = kdtree.query_ball_point(
                    current_node.pos[:2], r=self.vertical_connection_tolerance)

                for neighbor_idx in indices_in_ball:
                    if neighbor_idx == i:  # Don't connect to self
                        continue

                    neighbor_node = nodes_of_this_type[neighbor_idx]

                    # Crucial check: Ensure they are on different floors (Z-levels differ significantly)
                    # Z-levels are too close (same floor)
                    if abs(current_node.pos[2] - neighbor_node.pos[2]) < 1.0:
                        continue

                    # Connect if not already connected
                    if not self.super_graph.has_edge(current_node, neighbor_node):
                        self.super_graph.add_edge(
                            current_node, neighbor_node, type='vertical_connection')
                        connected_pairs_count += 1
                        # Mark both as processed for this type of pairing to avoid re-pairing B with A if A-B done
                        # This might be too aggressive if a node can connect to multiple above/below.
                        # A simpler approach is to just let KDTree find pairs.
                        # The has_edge check prevents duplicate edges.

                processed_nodes_indices.add(i)

        logger.info(
            f"Inter-floor connections made for {connected_pairs_count} pairs of vertical nodes.")
</file>

<file path="src/plotting/plotter.py">
"""
Defines plotter classes for visualizing network graphs using Matplotlib and Plotly.
"""
import abc
import pathlib
import logging
import networkx as nx
import numpy as np
import plotly.graph_objects as go
from typing import Dict, Tuple, Any, List, Optional

from src.config import NetworkConfig  # 依赖配置类
from src.graph.node import Node     # 依赖节点类

logger = logging.getLogger(__name__)


class BasePlotter(abc.ABC):
    """
    Abstract base class for graph plotters.
    """

    def __init__(self,
                 config: NetworkConfig,
                 color_map_data: Dict[Tuple[int, int, int], Dict[str, Any]]):
        """
        Initializes the BasePlotter.

        Args:
            config: The network configuration object.
            color_map_data: The global color map dictionary.
        """
        self.config = config
        self.color_map_data = color_map_data
        self.type_to_plot_color_cache: Dict[str, str] = {}  # 缓存节点类型到绘图颜色的映射

    def _get_node_color(self, node_type: str) -> str:
        """
        Determines the plotting color for a given node type.

        Uses colors from `color_map_data` if `NODE_COLOR_FROM_MAP` is True in config,
        otherwise uses a default Plotly color. Caches results.

        Args:
            node_type: The type of the node (e.g., 'Room', 'Door').

        Returns:
            A string representing the color (e.g., 'rgb(R,G,B)' or a named Plotly color).
        """
        if node_type in self.type_to_plot_color_cache:
            return self.type_to_plot_color_cache[node_type]

        default_plotly_color = '#1f77b4'  # Plotly's default blue

        if self.config.NODE_COLOR_FROM_MAP and self.color_map_data:
            for rgb_tuple, details in self.color_map_data.items():
                if details.get('name') == node_type:
                    color_str = f'rgb{rgb_tuple}'
                    self.type_to_plot_color_cache[node_type] = color_str
                    return color_str

        self.type_to_plot_color_cache[node_type] = default_plotly_color
        return default_plotly_color

    @abc.abstractmethod
    def plot(self,
             graph: nx.Graph,
             output_path: Optional[pathlib.Path] = None,
             title: str = "Network Graph",
             # For Plotly layout, original image width
             graph_width: Optional[int] = None,
             # For Plotly layout, original image height
             graph_height: Optional[int] = None,
             # For SuperNetwork floor labels
             floor_z_map: Optional[Dict[int, float]] = None
             ):
        """
        Abstract method to plot the graph.

        Args:
            graph: The NetworkX graph to plot.
            output_path: Optional path to save the plot. If None, displays the plot.
            title: The title for the plot.
            graph_width: Original width of the (floor plan) image space. Used by Plotly.
            graph_height: Original height of the (floor plan) image space. Used by Plotly.
            floor_z_map: Mapping from floor number to Z-coordinate, for floor slider labels.
        """
        pass


class PlotlyPlotter(BasePlotter):
    """
    Generates interactive 3D network graph visualizations using Plotly.
    """

    def _create_floor_selection_controls(self,
                                         all_z_levels: List[float],
                                         min_z: float, max_z: float,
                                         floor_z_map_for_labels: Optional[Dict[int,
                                                                               float]] = None,
                                         base_floor_for_labels: int = 0
                                         ) -> Dict[str, Any]:
        """
        Creates slider controls for selecting and viewing individual floors or all floors.
        Args:
            all_z_levels: Sorted list of unique Z-coordinates present in the graph.
            min_z: Minimum Z-coordinate.
            max_z: Maximum Z-coordinate.
            floor_z_map_for_labels: Mapping from actual floor number to Z-coordinate.
            base_floor_for_labels: The base floor number for labeling (e.g. 0 for ground, 1 for first).
        """
        if not all_z_levels:
            return {"sliders": []}

        # Create floor labels. Try to map Z-levels back to "human-readable" floor numbers.
        z_to_floor_label_map: Dict[float, str] = {}
        if floor_z_map_for_labels:
            # Invert floor_z_map_for_labels to map z -> floor_num for easier lookup
            # Handle potential multiple floors at the same Z (unlikely with good input)
            z_to_floor_num: Dict[float, List[int]] = {}
            for fn, z_val in floor_z_map_for_labels.items():
                z_to_floor_num.setdefault(z_val, []).append(fn)

            for z_level in all_z_levels:
                floor_nums_at_z = z_to_floor_num.get(z_level)
                if floor_nums_at_z:
                    # If multiple floor numbers map to the same z_level, list them or take first
                    f_num_str = "/".join(map(str, sorted(floor_nums_at_z)))
                    # e.g., F1, F-1/B1
                    z_to_floor_label_map[z_level] = f"F{f_num_str}"
                # Fallback if z_level not in map (should not happen if map is complete)
                else:
                    z_to_floor_label_map[z_level] = f"Z={z_level:.1f}"
        else:  # Fallback if no floor_z_map is provided
            for i, z_level in enumerate(all_z_levels):
                # Attempt simple labeling if base_floor is known
                floor_num_guess = base_floor_for_labels + i  # This is a rough guess
                z_to_floor_label_map[z_level] = f"F{floor_num_guess} (Z={z_level:.1f})"

        slider_steps = []
        for z_level in all_z_levels:
            label = z_to_floor_label_map.get(z_level, f"Z={z_level:.1f}")
            slider_steps.append(dict(
                label=label,
                method="relayout",
                args=[{"scene.zaxis.range": [z_level - self.config.DEFAULT_FLOOR_HEIGHT / 2 + 0.1,
                                             z_level + self.config.DEFAULT_FLOOR_HEIGHT / 2 - 0.1]}]  # View single floor
            ))

        # Add a step to show all floors
        slider_steps.append(dict(
            label="所有楼层",
            method="relayout",
            args=[{"scene.zaxis.range": [min_z - self.config.DEFAULT_FLOOR_HEIGHT * 0.5,
                                         max_z + self.config.DEFAULT_FLOOR_HEIGHT * 0.5]}]  # View all
        ))

        sliders = [dict(
            active=len(all_z_levels),  # Default to "All Floors"
            currentvalue={"prefix": "当前显示: "},
            pad={"t": 50},
            steps=slider_steps,
            name="楼层选择"
        )]
        return {"sliders": sliders}

    def plot(self,
             graph: nx.Graph,
             output_path: Optional[pathlib.Path] = None,
             title: str = "3D Network Graph",
             graph_width: Optional[int] = None,
             graph_height: Optional[int] = None,
             floor_z_map: Optional[Dict[int, float]] = None
             ):
        if not graph.nodes:
            # Ensure logger is defined/imported
            logger.warning("PlotlyPlotter: Graph has no nodes to plot.")
            return

        node_traces = []
        edge_traces = []  # Renamed from edge_trace to edge_traces as it's a list

        nodes_data_by_type: Dict[str, Dict[str, list]] = {}
        all_node_objects = [data.get('node_obj', node_id)
                            for node_id, data in graph.nodes(data=True)]
        all_node_objects = [n for n in all_node_objects if isinstance(n, Node)]

        if not all_node_objects:
            logger.warning(
                "PlotlyPlotter: No Node objects found in graph nodes. Cannot plot.")
            return

        all_z_coords_present = sorted(
            list(set(n.pos[2] for n in all_node_objects)))
        min_z = min(all_z_coords_present) if all_z_coords_present else 0
        max_z = max(all_z_coords_present) if all_z_coords_present else 0

        for node_obj in all_node_objects:
            node_type = node_obj.node_type
            if node_type not in nodes_data_by_type:
                nodes_data_by_type[node_type] = {
                    'x': [], 'y': [], 'z': [],
                    'visible_text': [],  # For text always visible next to node
                    'hover_text': [],   # For text visible on hover
                    'sizes': [],
                    'ids': []
                }

            x, y, z = node_obj.pos
            plot_x = (
                graph_width - x) if self.config.IMAGE_MIRROR and graph_width is not None else x

            nodes_data_by_type[node_type]['x'].append(plot_x)
            nodes_data_by_type[node_type]['y'].append(y)
            nodes_data_by_type[node_type]['z'].append(z)
            nodes_data_by_type[node_type]['ids'].append(node_obj.id)

            # --- Text Configuration ---
            # 1. Visible text (always shown next to the marker if mode includes 'text')
            #    Only show node_type if SHOW_PEDESTRIAN_LABELS is True or it's not a pedestrian node.
            #    Otherwise, show empty string to hide permanent text for certain types.
            is_ped_type = node_type in self.config.PEDESTRIAN_TYPES
            can_show_permanent_label = not is_ped_type or self.config.SHOW_PEDESTRIAN_LABELS

            nodes_data_by_type[node_type]['visible_text'].append(
                node_type if can_show_permanent_label else "")

            # 2. Hover text (always detailed)
            hover_label = (
                f"ID: {node_obj.id}<br>"
                f"Type: {node_type}<br>"
                f"Pos: ({x},{y},{z})<br>"
                f"Time: {node_obj.time:.2f}<br>"
                f"Area: {node_obj.area:.2f}"
            )
            if node_obj.door_type:
                hover_label += f"<br>Door: {node_obj.door_type}"
            nodes_data_by_type[node_type]['hover_text'].append(hover_label)

            # Node size
            size = self.config.NODE_SIZE_DEFAULT
            if is_ped_type:
                size = self.config.NODE_SIZE_PEDESTRIAN
            elif node_type in self.config.CONNECTION_TYPES:
                size = self.config.NODE_SIZE_CONNECTION
            elif node_type in self.config.VERTICAL_TYPES:
                size = self.config.NODE_SIZE_VERTICAL
            elif node_type in self.config.ROOM_TYPES:
                size = self.config.NODE_SIZE_ROOM
            elif node_type in self.config.OUTSIDE_TYPES:
                size = self.config.NODE_SIZE_OUTSIDE
            nodes_data_by_type[node_type]['sizes'].append(size)

        for node_type, data in nodes_data_by_type.items():
            if not data['x']:
                continue

            # Determine mode: if all 'visible_text' for this type are empty, just use 'markers'
            # Otherwise, use 'markers+text' to show the type.
            current_mode = 'markers'
            # Check if any visible text is non-empty
            if any(vt for vt in data['visible_text']):
                current_mode = 'markers+text'

            # If SHOW_PEDESTRIAN_LABELS is False and it's a pedestrian type, override to 'markers'
            if node_type in self.config.PEDESTRIAN_TYPES and not self.config.SHOW_PEDESTRIAN_LABELS:
                current_mode = 'markers'

            node_trace = go.Scatter3d(
                x=data['x'], y=data['y'], z=data['z'],
                mode=current_mode,  # Dynamically set mode
                marker=dict(
                    size=data['sizes'],
                    sizemode='diameter',  # This should make size in screen pixels
                    color=self._get_node_color(node_type),
                    opacity=self.config.NODE_OPACITY,
                    line=dict(width=1, color='DarkSlateGrey')
                ),
                # Text to display next to markers if mode includes 'text'
                text=data['visible_text'],
                hovertext=data['hover_text'],  # Text for hover box
                # Use 'text' from hovertext (Plotly default is 'all')
                hoverinfo='text',
                # if hovertext is set, hoverinfo='text' uses hovertext.
                # if hovertext is not set, hoverinfo='text' uses the 'text' property.
                name=node_type,
                customdata=data['ids'],
                textposition="top center",
                textfont=dict(  # Optional: style the permanently visible text
                    size=9,  # Smaller font for permanent labels
                    # color='black'
                )
            )
            node_traces.append(node_trace)

        # --- Prepare Edge Data (remains largely the same) ---
        edge_x_horiz, edge_y_horiz, edge_z_horiz = [], [], []
        edge_x_vert, edge_y_vert, edge_z_vert = [], [], []

        for edge_start_node, edge_end_node in graph.edges():
            if not (isinstance(edge_start_node, Node) and isinstance(edge_end_node, Node)):
                continue

            x0, y0, z0 = edge_start_node.pos
            x1, y1, z1 = edge_end_node.pos
            plot_x0 = (
                graph_width - x0) if self.config.IMAGE_MIRROR and graph_width is not None else x0
            plot_x1 = (
                graph_width - x1) if self.config.IMAGE_MIRROR and graph_width is not None else x1

            if abs(z0 - z1) < 0.1:
                edge_x_horiz.extend([plot_x0, plot_x1, None])
                edge_y_horiz.extend([y0, y1, None])
                edge_z_horiz.extend([z0, z1, None])
            else:
                edge_x_vert.extend([plot_x0, plot_x1, None])
                edge_y_vert.extend([y0, y1, None])
                edge_z_vert.extend([z0, z1, None])

        if edge_x_horiz:
            edge_traces.append(go.Scatter3d(
                x=edge_x_horiz, y=edge_y_horiz, z=edge_z_horiz,
                mode='lines',
                line=dict(color=self.config.HORIZONTAL_EDGE_COLOR,
                          width=self.config.EDGE_WIDTH),
                hoverinfo='none', name='水平连接'
            ))
        if edge_x_vert:
            edge_traces.append(go.Scatter3d(
                x=edge_x_vert, y=edge_y_vert, z=edge_z_vert,
                mode='lines',
                line=dict(color=self.config.VERTICAL_EDGE_COLOR,
                          width=self.config.EDGE_WIDTH),
                hoverinfo='none', name='垂直连接'
            ))

        # --- Layout and Figure (remains largely the same) ---
        layout = go.Layout(
            title=title,
            showlegend=True,
            hovermode='closest',  # Important for hover behavior
            margin=dict(b=20, l=5, r=5, t=40),
            scene=dict(
                xaxis=dict(
                    title='X', autorange='reversed' if self.config.IMAGE_MIRROR else True),
                yaxis=dict(
                    title='Y',
                    autorange='reversed', # 反转Y轴
                ),
                zaxis=dict(title='Z (楼层)', range=[min_z - 1, max_z + 1]),
                aspectmode='data',  # 'data' is often good for spatial data
                camera=dict(eye=dict(x=1.25, y=1.25, z=1.25))
            ),
            legend=dict(
                orientation="v",    # 垂直排列
                x=0.02,             # X 位置 (靠近左边缘)
                y=1.0,              # Y 位置 (靠近顶部)
                xanchor="left",     # X 锚点
                yanchor="top",      # Y 锚点
                bgcolor="rgba(255, 255, 255, 0.7)", # 可选：浅色背景提高可读性
                bordercolor="rgba(120, 120, 120, 0.7)", # 可选：边框颜色
                borderwidth=1         # 可选：边框宽度
            )
        )

        if len(all_z_coords_present) > 1:
            floor_controls = self._create_floor_selection_controls(
                all_z_coords_present, min_z, max_z, floor_z_map)
            layout.update(floor_controls)

        fig = go.Figure(data=node_traces + edge_traces, layout=layout)

        if output_path:
            output_path.parent.mkdir(parents=True, exist_ok=True)
            fig.write_html(str(output_path))
            # Ensure logger
            logger.info(f"Plotly graph saved to {output_path}")
        else:
            fig.show()
</file>

<file path="src/analysis/travel_time.py">
"""
Calculates travel times between specified room-like nodes in the graph.
"""
import csv
import pathlib
import networkx as nx
import logging  # Added for logging
from typing import Dict, List, Union, Optional

from src.config import NetworkConfig
from src.graph.node import Node

# Get a logger for this module
logger = logging.getLogger(__name__)


def calculate_room_travel_times(
    graph: nx.Graph,
    config: NetworkConfig,
    output_dir: pathlib.Path,
    output_filename: str = "room_travel_times.csv",
    ground_floor_z: Optional[float] = None
) -> Dict[str, Dict[str, Union[float, str]]]:
    """
    Calculates the shortest travel times between all pairs of individual "room" instances
    and designated "outward-facing door" nodes in the graph.

    Each room instance and each relevant outward-facing door is treated as a unique location.
    Room instance names will be 'NodeType_NodeID'.
    Outward-facing door names will be 'OutDoor_NodeID'.
    The output CSV will also include a final row with the area of each unique location.

    Args:
        graph: The input NetworkX graph. Nodes are expected to be `Node` objects
               or have a `node_obj` attribute pointing to a `Node` object.
        config: The NetworkConfig object.
        output_dir: The directory to save the resulting CSV file.
        output_filename: The name of the output CSV file.
        ground_floor_z: The Z-coordinate of the designated ground floor.
                        Only 'out' doors on this floor will be considered.

    Returns:
        A dictionary where keys are source location names and values are
        dictionaries mapping target location names to travel times.
    """
    if not graph.nodes:
        logger.warning("Graph is empty. Cannot calculate travel times.")
        return {}

    room_nodes: List[Node] = []
    out_door_nodes: List[Node] = []

    if ground_floor_z is None:
        logger.warning(
            "ground_floor_z not provided to calculate_room_travel_times. "
            "No 'out' doors will be specifically included as distinct locations for travel time analysis "
            "unless this behavior is changed in the filtering logic below."
        )

    for G_node_id, G_node_data in graph.nodes(data=True):
        node_obj = G_node_data.get('node_obj', G_node_id)
        if not isinstance(node_obj, Node):
            continue

        if node_obj.node_type in config.ROOM_TYPES:
            room_nodes.append(node_obj)
        elif node_obj.node_type in config.CONNECTION_TYPES and node_obj.door_type == 'out':
            if ground_floor_z is not None:
                # Tolerance for Z comparison
                if abs(node_obj.pos[2] - ground_floor_z) < 0.1:
                    out_door_nodes.append(node_obj)
            # If ground_floor_z is None, no out_door_nodes are added from this path based on current logic.
            # If you want a fallback, it would be here. For "only ground floor", this is correct.

    location_nodes: List[Node] = room_nodes + out_door_nodes
    # Map Node object to its unique name
    location_names_map: Dict[Node, str] = {}
    # Map unique location name to its area
    location_areas_map: Dict[str, float] = {}

    for node_obj in room_nodes:
        unique_name = f"{node_obj.node_type}_{node_obj.id}"
        location_names_map[node_obj] = unique_name
        location_areas_map[unique_name] = node_obj.area

    for node_obj in out_door_nodes:  # These are already filtered for ground floor
        unique_name = f"门_{node_obj.id}"
        location_names_map[node_obj] = unique_name
        location_areas_map[unique_name] = node_obj.area

    if not location_nodes:
        logger.warning(
            "No room instances or designated (ground floor) outward-facing door nodes found to calculate travel times.")
        return {}

    def weight_function(u_node_obj, v_node_obj, edge_data):  # u,v are Node objects
        if not isinstance(v_node_obj, Node):  # Should not happen if graph is consistent
            logger.error(
                f"Target node {v_node_obj} in edge is not a valid Node object for weight func.")
            raise ValueError(
                f"Invalid node object for weight function: {v_node_obj}")
        return v_node_obj.time

    travel_times_data: Dict[str, Dict[str, Union[float, str]]] = {}
    logger.info(
        f"Calculating travel times for {len(location_nodes)} unique locations...")

    for start_node_obj in location_nodes:
        start_location_name = location_names_map[start_node_obj]
        travel_times_data.setdefault(start_location_name, {})

        try:
            lengths = nx.single_source_dijkstra_path_length(
                graph,
                source=start_node_obj,
                weight=weight_function
            )
        except nx.NodeNotFound:
            logger.warning(
                f"Start node {start_location_name} (ID: {start_node_obj.id}) not in graph for Dijkstra. Skipping.")
            continue
        except Exception as e:
            logger.error(
                f"Error during Dijkstra for {start_location_name} (ID: {start_node_obj.id}): {e}")
            continue

        for target_node_obj in location_nodes:
            target_location_name = location_names_map[target_node_obj]

            if start_node_obj == target_node_obj:
                travel_times_data[start_location_name][target_location_name] = round(
                    start_node_obj.time, 2)
                continue

            if target_node_obj in lengths:
                total_time = start_node_obj.time + lengths[target_node_obj]
                travel_times_data[start_location_name][target_location_name] = round(
                    total_time, 2)
            else:
                travel_times_data[start_location_name][target_location_name] = '∞'

    logger.info("Travel time calculation complete.")

    output_dir.mkdir(parents=True, exist_ok=True)
    csv_file_path = output_dir / output_filename

    # all_location_names will now be unique identifiers like "RoomType_ID" or "OutDoor_ID"
    all_location_names = sorted(list(location_names_map.values()))

    if not all_location_names:
        logger.warning(
            "No unique location names generated to write to CSV for travel times and areas.")
        return travel_times_data

    try:
        with open(csv_file_path, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)

            header = ['来源/目标'] + all_location_names
            writer.writerow(header)

            for source_name in all_location_names:
                row_data = [source_name]
                for target_name in all_location_names:
                    time_val = travel_times_data.get(
                        source_name, {}).get(target_name, 'N/A')
                    row_data.append(time_val)
                writer.writerow(row_data)

            # "Area (px²)" or "面积 (m²)" depending on your unit
            area_row_label = "面积"
            area_row_data = [area_row_label]
            for loc_name in all_location_names:  # loc_name is now unique
                area_val = location_areas_map.get(
                    loc_name)  # Get area by unique name
                if area_val is not None:
                    area_row_data.append(f"{area_val:.2f}")
                else:
                    area_row_data.append("N/A")  # Should ideally not happen
            writer.writerow(area_row_data)

        logger.info(f"Travel times and areas saved to {csv_file_path}")
    except IOError as e:
        logger.error(
            f"Failed to write travel times and areas CSV to {csv_file_path}: {e}")

    return travel_times_data
</file>

<file path="result/super_network_travel_times.csv">
来源/目标,ICU_30013,NICU_30012,中医科_10008,中医科_40003,中心供应室_10003,产前诊断门诊_30009,产科_30006,介入科_20013,儿科_10006,全科_10004,内诊药房_10001,内镜中心_20002,口腔一区_40005,口腔科二区_40009,呼吸内科_20011,妇科_30005,心血管内科_20006,急诊科_1,手术室_30007,手术室_40007,挂号收费_10002,挂号收费_20001,挂号收费_30001,挂号收费_40001,放射科_10005,检验中心_10007,检验中心_20003,泌尿外科_30003,消化内科_20004,烧伤整形科_30011,生殖医学科_30010,甲状腺外科_30002,病理科_20008,皮肤科_40006,眼科_40002,神经内科_20010,综合激光科_40010,耳鼻喉科_40004,肝胆胰外科_30004,肾内科_20005,肿瘤科_20012,超声科_10009,透析中心_30008,采血处_20007,门_11072,门_11083,门_11086,门_11087,门_11093,门_11094,门_11112,门_11113,门_11116,门_11118,门_11119,门_11122,门_11145,门_11146,门_11147,门_11149,门_11153,门_11154,门_11155,门_11165,门诊手术室_40008,骨科_20009
ICU_30013,1,13.25,1785.0,1548.25,10.0,496.25,454.75,3262.0,682.0,487.5,126.5,969.0,1048.75,1054.0,417.25,487.5,931.5,9056.25,19.0,10.0,166.25,114.5,124.5,127.75,268.0,232.25,218.5,403.5,296.75,291.0,463.5,298.0,19.0,517.75,619.75,472.75,531.75,799.25,454.75,361.0,2387.5,724.25,14432.0,240.5,18.0,38.5,36.75,727.25,34.75,42.0,17.25,75.0,75.0,34.75,75.0,61.0,48.75,54.25,63.0,63.0,1788.0,490.5,235.25,235.25,72.5,267.25
NICU_30012,13.25,1,1781.5,1544.75,22.25,508.5,467.0,3274.25,694.25,499.75,138.75,981.25,1045.25,1050.5,429.5,492.75,943.75,9068.5,31.25,22.25,178.5,126.75,136.75,140.0,274.25,244.5,230.75,415.75,293.25,287.5,460.0,294.5,31.25,530.0,632.0,469.25,544.0,795.75,451.25,373.25,2384.0,736.5,14444.25,252.75,30.25,50.75,49.0,739.5,47.0,54.25,29.5,87.25,87.25,47.0,87.25,62.25,61.0,66.5,72.75,75.25,1784.5,502.75,247.5,247.5,84.75,279.5
中医科_10008,1785.0,1781.5,1486,3281.5,1794.0,2269.75,2209.0,5030.25,2129.0,2269.25,1908.25,2753.0,2498.0,2804.75,2199.0,2222.5,2676.25,10838.0,1800.75,1791.75,1948.0,1896.25,1898.0,1909.5,2049.75,1980.75,2000.25,2177.0,2045.75,1750.0,2188.0,2029.5,1800.75,2299.5,2380.5,1922.0,2311.75,2527.25,2186.25,2142.75,4120.75,2506.0,16213.75,2022.25,1799.75,1814.75,1818.5,2509.0,1811.25,1823.75,1801.25,1856.75,1856.75,1804.25,1856.75,1797.25,1802.5,1827.0,1800.75,1821.75,1489.0,2272.25,1983.75,1983.75,1854.25,2037.75
中医科_40003,1548.25,1544.75,3281.5,1486,1557.25,2041.25,1982.75,4783.75,2192.5,2032.5,1662.5,2516.25,2525.75,2510.0,1962.25,1996.25,2440.25,10601.25,1564.0,1555.0,1711.25,1659.5,1669.5,1662.0,1806.0,1674.0,1763.5,1948.5,1760.75,1787.5,1961.75,1771.75,1564.0,2052.0,2133.0,1959.5,2066.0,2279.75,1891.0,1906.0,3814.0,2269.25,15977.0,1785.5,1563.0,1568.25,1581.75,2272.25,1543.75,1587.0,1554.25,1620.0,1620.0,1529.75,1620.0,1512.25,1528.0,1591.0,1563.0,1585.75,3284.5,2035.5,1677.0,1677.0,1608.5,1801.75
中心供应室_10003,10.0,22.25,1794.0,1557.25,1,496.25,454.75,3262.0,682.0,487.5,126.5,969.0,1057.75,1063.0,417.25,487.5,931.5,9056.25,19.0,10.0,166.25,114.5,124.5,127.75,268.0,230.75,218.5,403.5,302.75,300.0,472.5,307.0,19.0,517.75,619.75,478.75,531.75,799.25,463.75,361.0,2393.5,724.25,14432.0,240.5,18.0,38.5,36.75,727.25,33.25,42.0,17.5,75.0,75.0,33.25,75.0,59.5,47.25,54.25,63.0,63.0,1797.0,490.5,233.75,233.75,72.5,267.25
产前诊断门诊_30009,496.25,508.5,2269.75,2041.25,496.25,442,871.25,3724.5,1136.25,900.25,547.5,1455.25,1528.25,1547.0,830.75,930.25,1384.0,9461.75,490.5,487.25,575.25,561.25,539.25,549.0,739.5,713.25,667.5,799.0,780.0,775.75,939.5,791.0,481.5,932.0,1041.0,952.25,935.5,1236.75,947.75,788.5,2870.75,1161.0,14881.25,671.5,495.25,515.75,506.75,1164.0,517.5,494.5,503.5,492.75,492.75,519.25,492.75,542.0,526.25,489.25,533.0,506.75,2272.75,903.25,716.25,716.25,499.0,710.5
产科_30006,454.75,467.0,2209.0,1982.75,454.75,871.25,404,3683.0,1052.0,879.75,513.25,1413.75,1467.5,1488.5,810.25,869.5,1323.25,9441.25,449.0,445.75,554.75,519.75,499.5,518.0,698.0,664.5,626.0,778.5,729.5,715.0,878.75,730.75,440.0,901.0,976.0,891.5,913.25,1176.0,887.5,764.5,2820.25,1124.75,14841.5,637.0,453.75,472.25,465.25,1127.75,472.25,460.0,462.0,472.25,472.25,474.0,472.25,493.25,472.25,451.25,472.25,446.0,2212.0,882.75,667.5,667.5,459.25,635.0
介入科_20013,3262.0,3274.25,5030.25,4783.75,3262.0,3724.5,3683.0,3240,3925.0,3721.5,3351.5,4221.0,4284.25,4289.5,3642.25,3730.5,4164.0,12281.25,3253.0,3253.0,3391.25,3339.5,3352.75,3352.75,3502.0,3479.0,3445.75,3630.0,3522.5,3536.25,3708.75,3543.25,3244.0,3742.75,3844.75,3698.5,3756.75,4034.75,3700.0,3586.0,5613.25,3963.0,17666.0,3465.5,3261.0,3281.5,3277.25,3966.0,3283.25,3284.25,3269.25,3300.0,3300.0,3285.0,3300.0,3301.25,3292.0,3296.5,3306.0,3305.25,5033.25,3724.5,3482.0,3482.0,3297.5,3492.25
儿科_10006,682.0,694.25,2129.0,2192.5,682.0,1136.25,1052.0,3925.0,640,1117.75,763.75,1641.0,1677.25,1698.25,1065.5,1111.75,1555.75,9696.5,682.0,673.0,800.25,764.5,764.5,764.5,915.25,866.25,872.5,1043.5,941.0,936.25,1100.0,952.0,682.0,1154.5,1212.0,1103.0,1168.5,1406.75,1108.75,1009.25,3031.75,1354.5,15095.0,890.5,681.0,686.25,684.5,1357.5,682.75,679.25,689.25,717.75,717.75,675.75,717.75,695.0,674.0,674.0,658.25,658.25,2132.0,1120.75,869.25,869.25,711.0,871.0
全科_10004,487.5,499.75,2269.25,2032.5,487.5,900.25,879.75,3721.5,1117.75,435,525.5,1446.5,1519.5,1538.25,815.5,931.25,1375.25,9443.0,487.5,478.5,556.75,547.75,547.75,546.0,727.75,704.5,657.5,793.5,771.25,775.25,940.5,782.25,478.5,920.25,1038.0,943.5,918.5,1228.0,939.0,773.25,2862.0,1130.25,14880.5,656.25,486.5,507.0,488.25,1133.25,508.75,463.75,494.75,474.25,474.25,510.5,474.25,533.25,514.5,456.75,514.5,488.25,2272.25,438.0,707.5,707.5,486.75,698.75
内诊药房_10001,126.5,138.75,1908.25,1662.5,126.5,547.5,513.25,3351.5,763.75,525.5,46,1085.5,1147.25,1168.25,450.5,570.5,1014.5,9054.0,117.5,117.5,174.75,179.5,179.5,169.75,366.5,343.5,234.0,440.75,410.25,414.25,579.75,421.25,108.5,561.5,663.5,582.75,565.75,855.75,578.0,408.25,2501.0,757.0,14454.0,268.75,125.5,146.0,134.25,760.0,147.75,101.0,133.75,69.5,71.25,149.5,76.5,172.25,156.5,104.5,160.0,134.25,1911.25,528.5,346.5,346.5,55.0,332.25
内镜中心_20002,969.0,981.25,2753.0,2516.25,969.0,1455.25,1413.75,4221.0,1641.0,1446.5,1085.5,960,2016.75,2022.0,1376.25,1446.5,1890.5,10015.25,978.0,969.0,1125.25,1073.5,1083.5,1086.75,1227.0,1195.0,1177.5,1362.5,1254.75,1259.0,1431.5,1266.0,978.0,1476.75,1578.75,1430.75,1490.75,1758.25,1422.75,1320.0,3345.5,1683.25,15391.0,1199.5,977.0,997.5,995.75,1686.25,999.25,1001.0,985.25,1034.0,1034.0,1001.0,1034.0,1023.75,1008.0,1013.25,1022.0,1022.0,2756.0,1449.5,1198.0,1198.0,1031.5,1226.25
口腔一区_40005,1048.75,1045.25,2498.0,2525.75,1057.75,1528.25,1467.5,4284.25,1677.25,1519.5,1147.25,2016.75,1004,2031.5,1457.5,1481.0,1925.0,10086.75,1064.5,1055.5,1202.0,1156.5,1156.5,1146.75,1301.25,1234.75,1264.0,1435.5,1299.75,1268.0,1465.75,1310.75,1064.5,1536.75,1617.75,1440.0,1550.75,1764.5,1451.75,1401.25,3374.75,1756.25,15477.5,1280.75,1063.5,1072.25,1070.5,1759.25,1058.25,1074.0,1065.0,1117.0,1117.0,1051.25,1117.0,1051.25,1049.5,1075.75,1047.75,1070.5,2501.0,1522.5,1237.75,1237.75,1093.25,1286.5
口腔科二区_40009,1054.0,1050.5,2804.75,2510.0,1063.0,1547.0,1488.5,4289.5,1698.25,1538.25,1168.25,2022.0,2031.5,1004,1468.0,1502.0,1946.0,10107.0,1069.75,1060.75,1217.0,1165.25,1175.25,1167.75,1318.75,1219.0,1269.25,1454.25,1284.0,1310.75,1483.25,1295.0,1069.75,1557.75,1638.75,1482.75,1571.75,1785.5,1436.0,1411.75,3359.0,1775.0,15482.75,1291.25,1068.75,1083.75,1087.5,1778.0,1067.0,1092.75,1070.25,1125.75,1125.75,1053.0,1125.75,1035.5,1051.25,1096.75,1068.75,1091.5,2807.75,1541.25,1222.0,1222.0,1114.25,1307.5
呼吸内科_20011,417.25,429.5,2199.0,1962.25,417.25,830.75,810.25,3642.25,1065.5,815.5,450.5,1376.25,1457.5,1468.0,359,869.25,1303.5,9367.0,408.25,408.25,446.0,468.5,478.25,478.25,657.25,634.25,569.5,704.0,701.0,705.0,877.5,712.0,399.25,852.5,970.25,871.75,829.0,1166.0,868.75,694.0,2791.75,1074.5,14801.0,568.25,416.25,436.75,432.5,1077.5,438.5,409.75,424.5,395.75,395.75,440.25,395.75,463.0,447.25,404.5,461.25,436.0,2202.0,818.5,637.25,637.25,405.5,619.5
妇科_30005,487.5,492.75,2222.5,1996.25,487.5,930.25,869.5,3730.5,1111.75,931.25,570.5,1446.5,1481.0,1502.0,869.25,442,1359.5,9500.25,487.5,478.5,613.75,568.25,558.5,570.0,736.5,679.75,676.25,837.5,743.0,728.5,892.25,744.25,487.5,960.0,1041.0,905.0,972.25,1212.25,901.0,813.0,2833.75,1168.0,14898.75,694.25,486.5,499.75,499.75,1171.0,496.25,496.25,494.75,528.75,528.75,489.25,528.75,508.5,487.5,487.5,485.75,482.25,2225.5,934.25,682.75,682.75,516.5,698.25
心血管内科_20006,931.5,943.75,2676.25,2440.25,931.5,1384.0,1323.25,4164.0,1555.75,1375.25,1014.5,1890.5,1925.0,1946.0,1303.5,1359.5,886,9944.25,930.0,922.5,1054.25,1002.5,1012.25,1014.0,1179.0,1123.75,1110.5,1291.25,1177.25,1182.25,1346.0,1198.0,921.0,1404.0,1485.0,1339.25,1418.0,1656.25,1354.75,1247.25,3268.0,1612.0,15343.0,1128.5,930.5,947.25,942.0,1615.0,940.25,940.25,938.75,963.0,963.0,933.25,963.0,952.5,931.5,931.5,929.75,926.25,2679.25,1378.25,1126.75,1126.75,960.5,1132.5
急诊科_1,9056.25,9068.5,10838.0,10601.25,9056.25,9461.75,9441.25,12281.25,9696.5,9443.0,9054.0,10015.25,10086.75,10107.0,9367.0,9500.25,9944.25,9000,9047.25,9047.25,9087.0,9109.25,9109.25,9109.25,9296.25,9273.25,9188.0,9345.0,9340.0,9344.0,9509.5,9351.0,9038.25,9483.5,9601.25,9512.5,9470.0,9795.25,9507.75,9334.75,11430.75,9689.75,23408.0,9200.25,9055.25,9075.75,9067.0,9692.75,9077.5,9033.75,9063.5,9037.25,9037.25,9079.25,9037.25,9102.0,9086.25,9035.5,9093.25,9067.0,10841.0,9446.0,9276.25,9276.25,9009.0,9260.25
手术室_30007,19.0,31.25,1800.75,1564.0,19.0,490.5,449.0,3253.0,682.0,487.5,117.5,978.0,1064.5,1069.75,408.25,487.5,930.0,9047.25,1,10.0,157.25,105.5,118.75,118.75,259.0,236.0,211.75,396.0,302.75,306.75,479.25,313.75,10.0,508.75,610.75,478.75,522.75,799.25,470.5,352.0,2393.5,724.25,14414.0,231.5,18.0,38.5,36.75,727.25,40.25,42.0,26.25,66.0,66.0,42.0,66.0,64.75,49.0,54.25,63.0,63.0,1803.75,490.5,239.0,239.0,63.5,258.25
手术室_40007,10.0,22.25,1791.75,1555.0,10.0,487.25,445.75,3253.0,673.0,478.5,117.5,969.0,1055.5,1060.75,408.25,478.5,922.5,9047.25,10.0,1,157.25,105.5,115.5,118.75,259.0,227.0,209.5,394.5,293.75,297.75,470.25,304.75,10.0,508.75,610.75,469.75,522.75,790.25,461.5,352.0,2384.5,715.25,14423.0,231.5,9.0,29.5,27.75,718.25,31.25,33.0,17.25,66.0,66.0,33.0,66.0,55.75,40.0,45.25,54.0,54.0,1794.75,481.5,230.0,230.0,63.5,258.25
挂号收费_10002,166.25,178.5,1948.0,1711.25,166.25,575.25,554.75,3391.25,800.25,556.75,174.75,1125.25,1202.0,1217.0,446.0,613.75,1054.25,9087.0,157.25,157.25,79,219.25,222.75,222.75,406.25,383.25,313.25,424.0,450.0,454.0,623.0,461.0,148.25,597.0,714.75,622.5,549.0,910.5,617.75,444.75,2540.75,795.25,14538.5,312.0,165.25,185.75,170.75,798.25,187.5,139.25,173.5,113.0,107.75,189.25,106.0,212.0,196.25,139.25,197.0,170.75,1951.0,559.75,386.25,386.25,143.0,370.25
挂号收费_20001,114.5,126.75,1896.25,1659.5,114.5,561.25,519.75,3339.5,764.5,547.75,179.5,1073.5,1156.5,1165.25,468.5,568.25,1002.5,9109.25,105.5,105.5,219.25,79,189.5,189.5,354.5,331.5,275.5,456.25,398.25,402.25,574.75,409.25,96.5,579.5,681.5,570.75,583.0,865.0,566.0,412.25,2489.0,791.0,14517.5,293.5,113.5,134.0,129.75,794.0,135.75,135.0,121.75,128.0,128.0,137.5,128.0,160.25,144.5,136.75,158.5,135.0,1899.25,550.75,334.5,334.5,125.5,318.5
挂号收费_30001,124.5,136.75,1898.0,1669.5,124.5,539.25,499.5,3352.75,764.5,547.75,179.5,1083.5,1156.5,1175.25,478.25,558.5,1012.25,9109.25,118.75,115.5,222.75,189.5,79,186.0,367.75,341.5,295.75,446.5,408.25,404.0,567.75,419.25,109.75,569.0,678.0,580.5,581.25,865.0,576.0,434.25,2499.0,791.0,14507.75,303.25,123.5,144.0,135.0,794.0,145.75,135.0,131.75,140.25,140.25,147.5,140.25,170.25,154.5,136.75,161.25,135.0,1901.0,550.75,344.5,344.5,125.5,338.75
挂号收费_40001,127.75,140.0,1909.5,1662.0,127.75,549.0,518.0,3352.75,764.5,546.0,169.75,1086.75,1146.75,1167.75,478.25,570.0,1014.0,9109.25,118.75,118.75,222.75,189.5,186.0,79,367.75,344.75,295.75,456.25,411.5,415.5,579.25,422.5,109.75,557.5,659.5,582.25,571.5,855.25,579.25,434.25,2502.25,791.0,14517.5,303.25,126.75,147.25,143.0,794.0,149.0,135.0,135.0,140.25,140.25,150.75,140.25,173.5,157.75,136.75,159.5,135.0,1912.5,549.0,347.75,347.75,115.75,338.75
放射科_10005,268.0,274.25,2049.75,1806.0,268.0,739.5,698.0,3502.0,915.25,727.75,366.5,1227.0,1301.25,1318.75,657.25,736.5,1179.0,9296.25,259.0,259.0,406.25,354.5,367.75,367.75,250,478.0,460.25,645.0,551.75,555.75,724.0,562.75,259.0,757.75,859.75,727.0,771.75,1032.5,719.5,601.0,2642.5,964.5,14672.0,480.5,267.0,266.5,257.75,967.5,282.25,282.25,275.25,315.0,315.0,284.0,315.0,306.75,285.75,289.25,296.25,294.5,2052.75,730.75,481.0,481.0,312.5,507.25
检验中心_10007,232.25,244.5,1980.75,1674.0,230.75,713.25,664.5,3479.0,866.25,704.5,343.5,1195.0,1234.75,1219.0,634.25,679.75,1123.75,9273.25,236.0,227.0,383.25,331.5,341.5,344.75,478.0,180,435.5,620.5,460.0,486.75,661.0,471.0,236.0,734.75,826.25,658.75,748.75,973.0,585.0,578.0,2508.0,941.25,14649.0,457.5,235.0,240.25,253.75,944.25,215.75,259.0,226.25,292.0,292.0,201.75,292.0,196.5,200.0,263.0,236.75,257.75,1983.75,707.5,183.0,183.0,289.5,483.5
检验中心_20003,218.5,230.75,2000.25,1763.5,218.5,667.5,626.0,3445.75,872.5,657.5,234.0,1177.5,1264.0,1269.25,569.5,676.25,1110.5,9188.0,211.75,209.5,313.25,275.5,295.75,295.75,460.25,435.5,180,566.0,502.25,506.25,678.75,513.25,202.75,685.75,787.75,678.25,691.0,973.0,670.0,522.0,2593.0,878.0,14588.0,380.5,208.5,238.0,236.0,881.0,239.75,222.0,225.75,222.0,222.0,241.5,222.0,264.25,248.5,230.75,262.5,243.0,2003.25,660.5,438.5,438.5,189.0,428.25
泌尿外科_30003,403.5,415.75,2177.0,1948.5,403.5,799.0,778.5,3630.0,1043.5,793.5,440.75,1362.5,1435.5,1454.25,704.0,837.5,1291.25,9345.0,396.0,394.5,424.0,456.25,446.5,456.25,645.0,620.5,566.0,337,687.25,683.0,846.75,698.25,387.0,830.5,948.25,859.5,807.0,1144.0,855.0,681.75,2778.0,1054.25,14788.5,564.75,402.5,423.0,414.0,1057.25,424.75,387.75,410.75,386.0,386.0,426.5,386.0,449.25,433.5,382.5,440.25,414.0,2180.0,796.5,623.5,623.5,402.0,607.25
消化内科_20004,296.75,293.25,2045.75,1760.75,302.75,780.0,729.5,3522.5,941.0,771.25,410.25,1254.75,1299.75,1284.0,701.0,743.0,1177.25,9340.0,302.75,293.75,450.0,398.25,408.25,411.5,551.75,460.0,502.25,687.25,245,551.75,719.0,536.0,302.75,801.5,891.25,696.5,815.5,1038.0,677.0,644.75,2588.5,1008.0,14715.75,524.25,301.75,316.75,320.5,1011.0,308.0,325.75,304.5,358.75,358.75,294.0,358.75,276.5,292.25,337.75,311.5,332.5,2048.75,774.25,463.0,463.0,356.25,538.75
烧伤整形科_30011,291.0,287.5,1750.0,1787.5,300.0,775.75,715.0,3536.25,936.25,775.25,414.25,1259.0,1268.0,1310.75,705.0,728.5,1182.25,9344.0,306.75,297.75,454.0,402.25,404.0,415.5,555.75,486.75,506.25,683.0,551.75,256,694.0,535.5,306.75,805.5,886.5,692.0,817.75,1033.25,692.25,648.75,2626.75,1012.0,14719.75,528.25,305.75,320.75,324.5,1015.0,317.25,329.75,307.25,362.75,362.75,310.25,362.75,303.25,308.5,333.0,306.75,327.75,1753.0,778.25,489.75,489.75,360.25,543.75
生殖医学科_30010,463.5,460.0,2188.0,1961.75,472.5,939.5,878.75,3708.75,1100.0,940.5,579.75,1431.5,1465.75,1483.25,877.5,892.25,1346.0,9509.5,479.25,470.25,623.0,574.75,567.75,579.25,724.0,661.0,678.75,846.75,719.0,694.0,425,709.75,479.25,969.25,1050.25,881.0,981.5,1197.0,866.5,821.25,2801.0,1177.25,14892.25,700.75,478.25,493.25,493.25,1180.25,481.0,496.75,479.75,535.25,535.25,474.0,535.25,477.5,472.25,496.75,470.5,491.5,2191.0,943.5,664.0,664.0,525.75,707.5
甲状腺外科_30002,298.0,294.5,2029.5,1771.75,307.0,791.0,730.75,3543.25,952.0,782.25,421.25,1266.0,1310.75,1295.0,712.0,744.25,1198.0,9351.0,313.75,304.75,461.0,409.25,419.25,422.5,562.75,471.0,513.25,698.25,536.0,535.5,709.75,256,313.75,812.5,902.25,733.0,826.5,1049.0,676.5,655.75,2611.0,1019.0,14726.75,535.25,312.75,327.75,331.5,1022.0,319.0,336.75,314.25,369.75,369.75,305.0,369.75,287.5,303.25,348.75,322.5,343.5,2032.5,785.25,474.0,474.0,367.25,559.5
病理科_20008,19.0,31.25,1800.75,1564.0,19.0,481.5,440.0,3244.0,682.0,478.5,108.5,978.0,1064.5,1069.75,399.25,487.5,921.0,9038.25,10.0,10.0,148.25,96.5,109.75,109.75,259.0,236.0,202.75,387.0,302.75,306.75,479.25,313.75,1,499.75,601.75,478.75,513.75,794.0,470.5,343.0,2393.5,720.0,14423.0,222.5,18.0,38.5,34.25,723.0,40.25,41.25,26.25,57.0,57.0,42.0,57.0,64.75,49.0,53.5,63.0,62.25,1803.75,481.5,239.0,239.0,54.5,249.25
皮肤科_40006,517.75,530.0,2299.5,2052.0,517.75,932.0,901.0,3742.75,1154.5,920.25,561.5,1476.75,1536.75,1557.75,852.5,960.0,1404.0,9483.5,508.75,508.75,597.0,579.5,569.0,557.5,757.75,734.75,685.75,830.5,801.5,805.5,969.25,812.5,499.75,462,1049.5,972.25,945.75,1245.25,969.25,810.25,2892.25,1182.75,14909.25,693.25,516.75,537.25,533.0,1185.75,539.0,516.25,525.0,514.5,514.5,540.75,514.5,563.5,547.75,511.0,549.5,525.0,2302.5,923.25,737.75,737.75,507.5,728.75
眼科_40002,619.75,632.0,2380.5,2133.0,619.75,1041.0,976.0,3844.75,1212.0,1038.0,663.5,1578.75,1617.75,1638.75,970.25,1041.0,1485.0,9601.25,610.75,610.75,714.75,681.5,678.0,659.5,859.75,826.25,787.75,948.25,891.25,886.5,1050.25,902.25,601.75,1049.5,564,1053.25,1063.5,1326.25,1059.0,926.25,2982.0,1284.75,15011.25,797.0,618.75,634.0,630.5,1287.75,634.0,620.0,627.0,632.25,632.25,635.75,632.25,655.0,634.0,611.25,630.5,606.0,2383.5,1041.0,829.25,829.25,609.5,795.0
神经内科_20010,472.75,469.25,1922.0,1959.5,478.75,952.25,891.5,3698.5,1103.0,943.5,582.75,1430.75,1440.0,1482.75,871.75,905.0,1339.25,9512.5,478.75,469.75,622.5,570.75,580.5,582.25,727.0,658.75,678.25,859.5,696.5,692.0,881.0,733.0,478.75,972.25,1053.25,428,986.25,1200.0,875.75,815.5,2787.25,1180.25,14891.75,696.75,477.75,492.75,496.25,1183.25,484.0,499.75,480.5,531.25,531.25,477.0,531.25,475.25,475.25,499.75,473.5,494.5,1925.0,946.5,661.75,661.75,528.75,700.75
综合激光科_40010,531.75,544.0,2311.75,2066.0,531.75,935.5,913.25,3756.75,1168.5,918.5,565.75,1490.75,1550.75,1571.75,829.0,972.25,1418.0,9470.0,522.75,522.75,549.0,583.0,581.25,571.5,771.75,748.75,691.0,807.0,815.5,817.75,981.5,826.5,513.75,945.75,1063.5,986.25,462,1259.25,983.25,808.5,2906.25,1179.25,14922.5,689.75,530.75,551.25,540.75,1182.25,553.0,514.5,539.0,511.0,511.0,554.75,511.0,577.5,561.75,509.25,563.5,539.0,2314.75,921.5,751.75,751.75,521.5,734.0
耳鼻喉科_40004,799.25,795.75,2527.25,2279.75,799.25,1236.75,1176.0,4034.75,1406.75,1228.0,855.75,1758.25,1764.5,1785.5,1166.0,1212.25,1656.25,9795.25,799.25,790.25,910.5,865.0,865.0,855.25,1032.5,973.0,973.0,1144.0,1038.0,1033.25,1197.0,1049.0,794.0,1245.25,1326.25,1200.0,1259.25,737,1205.75,1109.75,3128.75,1464.75,15203.5,989.25,798.25,803.5,801.75,1467.75,789.5,793.0,803.5,825.5,825.5,782.5,825.5,801.75,780.75,784.25,777.25,779.0,2530.25,1231.0,976.0,976.0,801.75,995.0
肝胆胰外科_30004,454.75,451.25,2186.25,1891.0,463.75,947.75,887.5,3700.0,1108.75,939.0,578.0,1422.75,1451.75,1436.0,868.75,901.0,1354.75,9507.75,470.5,461.5,617.75,566.0,576.0,579.25,719.5,585.0,670.0,855.0,677.0,692.25,866.5,676.5,470.5,969.25,1059.0,875.75,983.25,1205.75,397,812.5,2725.0,1175.75,14883.5,692.0,469.5,484.5,488.25,1178.75,460.0,493.5,470.5,526.5,526.5,446.0,526.5,428.5,444.25,505.5,479.25,500.25,2189.25,942.0,588.0,588.0,524.0,716.25
肾内科_20005,361.0,373.25,2142.75,1906.0,361.0,788.5,764.5,3586.0,1009.25,773.25,408.25,1320.0,1401.25,1411.75,694.0,813.0,1247.25,9334.75,352.0,352.0,444.75,412.25,434.25,434.25,601.0,578.0,522.0,681.75,644.75,648.75,821.25,655.75,343.0,810.25,926.25,815.5,808.5,1109.75,812.5,315,2735.5,1032.25,14758.75,526.0,360.0,380.5,376.25,1035.25,382.25,367.5,368.25,353.5,353.5,384.0,353.5,406.75,391.0,362.25,405.0,379.75,2145.75,776.25,581.0,581.0,363.25,563.25
肿瘤科_20012,2387.5,2384.0,4120.75,3814.0,2393.5,2870.75,2820.25,5613.25,3031.75,2862.0,2501.0,3345.5,3374.75,3359.0,2791.75,2833.75,3268.0,11430.75,2393.5,2384.5,2540.75,2489.0,2499.0,2502.25,2642.5,2508.0,2593.0,2778.0,2588.5,2626.75,2801.0,2611.0,2393.5,2892.25,2982.0,2787.25,2906.25,3128.75,2725.0,2735.5,2320,3098.75,16806.5,2615.0,2392.5,2407.5,2411.25,3101.75,2383.0,2416.5,2393.5,2449.5,2449.5,2369.0,2449.5,2351.5,2367.25,2428.5,2402.25,2423.25,4123.75,2865.0,2511.0,2511.0,2447.0,2629.5
超声科_10009,724.25,736.5,2506.0,2269.25,724.25,1161.0,1124.75,3963.0,1354.5,1130.25,757.0,1683.25,1756.25,1775.0,1074.5,1168.0,1612.0,9689.75,724.25,715.25,795.25,791.0,791.0,791.0,964.5,941.25,878.0,1054.25,1008.0,1012.0,1177.25,1019.0,720.0,1182.75,1284.75,1180.25,1179.25,1464.75,1175.75,1032.25,3098.75,670,15098.0,883.75,723.25,743.75,725.0,673.0,745.5,691.75,731.5,711.0,711.0,747.25,711.0,770.0,751.25,700.5,751.25,725.0,2509.0,1133.25,944.25,944.25,706.0,943.75
透析中心_30008,14432.0,14444.25,16213.75,15977.0,14432.0,14881.25,14841.5,17666.0,15095.0,14880.5,14454.0,15391.0,15477.5,15482.75,14801.0,14898.75,15343.0,23408.0,14414.0,14423.0,14538.5,14517.5,14507.75,14517.5,14672.0,14649.0,14588.0,14788.5,14715.75,14719.75,14892.25,14726.75,14423.0,14909.25,15011.25,14891.75,14922.5,15203.5,14883.5,14758.75,16806.5,15098.0,14400,14610.25,14423.25,14451.5,14449.75,15101.0,14453.25,14442.0,14439.25,14447.25,14447.25,14455.0,14447.25,14477.75,14462.0,14450.75,14476.0,14475.25,16216.75,14883.5,14652.0,14652.0,14409.0,14670.25
采血处_20007,240.5,252.75,2022.25,1785.5,240.5,671.5,637.0,3465.5,890.5,656.25,268.75,1199.5,1280.75,1291.25,568.25,694.25,1128.5,9200.25,231.5,231.5,312.0,293.5,303.25,303.25,480.5,457.5,380.5,564.75,524.25,528.25,700.75,535.25,222.5,693.25,797.0,696.75,689.75,989.25,692.0,526.0,2615.0,883.75,14610.25,184,239.5,260.0,255.75,886.75,261.75,227.75,247.75,220.75,220.75,263.5,220.75,286.25,270.5,236.5,284.5,261.0,2025.25,659.25,460.5,460.5,214.75,446.25
门_11072,18.0,30.25,1799.75,1563.0,18.0,495.25,453.75,3261.0,681.0,486.5,125.5,977.0,1063.5,1068.75,416.25,486.5,930.5,9055.25,18.0,9.0,165.25,113.5,123.5,126.75,267.0,235.0,208.5,402.5,301.75,305.75,478.25,312.75,18.0,516.75,618.75,477.75,530.75,798.25,469.5,360.0,2392.5,723.25,14423.25,239.5,3.0,37.5,35.75,726.25,39.25,41.0,25.25,74.0,74.0,41.0,74.0,63.75,48.0,53.25,62.0,62.0,1802.75,489.5,238.0,238.0,71.5,266.25
门_11083,38.5,50.75,1814.75,1568.25,38.5,515.75,472.25,3281.5,686.25,507.0,146.0,997.5,1072.25,1083.75,436.75,499.75,947.25,9075.75,38.5,29.5,185.75,134.0,144.0,147.25,266.5,240.25,238.0,423.0,316.75,320.75,493.25,327.75,38.5,537.25,634.0,492.75,551.25,803.5,484.5,380.5,2407.5,743.75,14451.5,260.0,37.5,3.0,56.25,746.75,44.5,61.5,45.75,94.5,94.5,46.25,94.5,69.0,53.25,70.75,67.25,65.5,1817.75,510.0,243.25,243.25,92.0,286.75
门_11086,36.75,49.0,1818.5,1581.75,36.75,506.75,465.25,3277.25,684.5,488.25,134.25,995.75,1070.5,1087.5,432.5,499.75,942.0,9067.0,36.75,27.75,170.75,129.75,135.0,143.0,257.75,253.75,236.0,414.0,320.5,324.5,493.25,331.5,34.25,533.0,630.5,496.25,540.75,801.75,488.25,376.25,2411.25,725.0,14449.75,255.75,35.75,56.25,3.0,728.0,58.0,42.75,44.0,88.25,88.25,59.75,88.25,82.5,65.5,55.0,65.5,63.75,1821.5,491.25,256.75,256.75,83.25,282.5
门_11087,727.25,739.5,2509.0,2272.25,727.25,1164.0,1127.75,3966.0,1357.5,1133.25,760.0,1686.25,1759.25,1778.0,1077.5,1171.0,1615.0,9692.75,727.25,718.25,798.25,794.0,794.0,794.0,967.5,944.25,881.0,1057.25,1011.0,1015.0,1180.25,1022.0,723.0,1185.75,1287.75,1183.25,1182.25,1467.75,1178.75,1035.25,3101.75,673.0,15101.0,886.75,726.25,746.75,728.0,3.0,748.5,694.75,734.5,714.0,714.0,750.25,714.0,773.0,754.25,703.5,754.25,728.0,2512.0,1136.25,947.25,947.25,709.0,946.75
门_11093,34.75,47.0,1811.25,1543.75,33.25,517.5,472.25,3283.25,682.75,508.75,147.75,999.25,1058.25,1067.0,438.5,496.25,940.25,9077.5,40.25,31.25,187.5,135.75,145.75,149.0,282.25,215.75,239.75,424.75,308.0,317.25,481.0,319.0,40.25,539.0,634.0,484.0,553.0,789.5,460.0,382.25,2383.0,745.5,14453.25,261.75,39.25,44.5,58.0,748.5,3.0,63.25,28.75,96.25,96.25,18.25,96.25,44.5,32.25,70.75,53.25,65.5,1814.25,511.75,218.75,218.75,93.75,288.5
门_11094,42.0,54.25,1823.75,1587.0,42.0,494.5,460.0,3284.25,679.25,463.75,101.0,1001.0,1074.0,1092.75,409.75,496.25,940.25,9033.75,42.0,33.0,139.25,135.0,135.0,135.0,282.25,259.0,222.0,387.75,325.75,329.75,496.75,336.75,41.25,516.25,620.0,499.75,514.5,793.0,493.5,367.5,2416.5,691.75,14442.0,227.75,41.0,61.5,42.75,694.75,63.25,3.0,49.25,55.0,55.0,65.0,55.0,87.75,69.0,34.0,69.0,53.25,1826.75,466.75,262.0,262.0,50.0,279.0
门_11112,17.25,29.5,1801.25,1554.25,17.5,503.5,462.0,3269.25,689.25,494.75,133.75,985.25,1065.0,1070.25,424.5,494.75,938.75,9063.5,26.25,17.25,173.5,121.75,131.75,135.0,275.25,226.25,225.75,410.75,304.5,307.25,479.75,314.25,26.25,525.0,627.0,480.5,539.0,803.5,470.5,368.25,2393.5,731.5,14439.25,247.75,25.25,45.75,44.0,734.5,28.75,49.25,3.0,82.25,82.25,28.75,82.25,55.0,42.75,61.5,67.25,70.25,1804.25,497.75,229.25,229.25,79.75,274.5
门_11113,75.0,87.25,1856.75,1620.0,75.0,492.75,472.25,3300.0,717.75,474.25,69.5,1034.0,1117.0,1125.75,395.75,528.75,963.0,9037.25,66.0,66.0,113.0,128.0,140.25,140.25,315.0,292.0,222.0,386.0,358.75,362.75,535.25,369.75,57.0,514.5,632.25,531.25,511.0,825.5,526.5,353.5,2449.5,711.0,14447.25,220.75,74.0,94.5,88.25,714.0,96.25,55.0,82.25,3.0,9.5,98.0,14.75,120.75,105.0,56.75,114.5,88.25,1859.75,477.25,295.0,295.0,51.75,279.0
门_11116,75.0,87.25,1856.75,1620.0,75.0,492.75,472.25,3300.0,717.75,474.25,71.25,1034.0,1117.0,1125.75,395.75,528.75,963.0,9037.25,66.0,66.0,107.75,128.0,140.25,140.25,315.0,292.0,222.0,386.0,358.75,362.75,535.25,369.75,57.0,514.5,632.25,531.25,511.0,825.5,526.5,353.5,2449.5,711.0,14447.25,220.75,74.0,94.5,88.25,714.0,96.25,55.0,82.25,9.5,3.0,98.0,9.5,120.75,105.0,56.75,114.5,88.25,1859.75,477.25,295.0,295.0,51.75,279.0
门_11118,34.75,47.0,1804.25,1529.75,33.25,519.25,474.0,3285.0,675.75,510.5,149.5,1001.0,1051.25,1053.0,440.25,489.25,933.25,9079.25,42.0,33.0,189.25,137.5,147.5,150.75,284.0,201.75,241.5,426.5,294.0,310.25,474.0,305.0,42.0,540.75,635.75,477.0,554.75,782.5,446.0,384.0,2369.0,747.25,14455.0,263.5,41.0,46.25,59.75,750.25,18.25,65.0,28.75,98.0,98.0,3.0,98.0,30.5,18.25,72.5,46.25,67.25,1807.25,513.5,204.75,204.75,95.5,290.25
门_11119,75.0,87.25,1856.75,1620.0,75.0,492.75,472.25,3300.0,717.75,474.25,76.5,1034.0,1117.0,1125.75,395.75,528.75,963.0,9037.25,66.0,66.0,106.0,128.0,140.25,140.25,315.0,292.0,222.0,386.0,358.75,362.75,535.25,369.75,57.0,514.5,632.25,531.25,511.0,825.5,526.5,353.5,2449.5,711.0,14447.25,220.75,74.0,94.5,88.25,714.0,96.25,55.0,82.25,14.75,9.5,98.0,3.0,120.75,105.0,56.75,114.5,88.25,1859.75,477.25,295.0,295.0,51.75,279.0
门_11122,61.0,62.25,1797.25,1512.25,59.5,542.0,493.25,3301.25,695.0,533.25,172.25,1023.75,1051.25,1035.5,463.0,508.5,952.5,9102.0,64.75,55.75,212.0,160.25,170.25,173.5,306.75,196.5,264.25,449.25,276.5,303.25,477.5,287.5,64.75,563.5,655.0,475.25,577.5,801.75,428.5,406.75,2351.5,770.0,14477.75,286.25,63.75,69.0,82.5,773.0,44.5,87.75,55.0,120.75,120.75,30.5,120.75,3.0,28.75,91.75,65.5,86.5,1800.25,536.25,199.5,199.5,118.25,312.25
门_11145,48.75,61.0,1802.5,1528.0,47.25,526.25,472.25,3292.0,674.0,514.5,156.5,1008.0,1049.5,1051.25,447.25,487.5,931.5,9086.25,49.0,40.0,196.25,144.5,154.5,157.75,285.75,200.0,248.5,433.5,292.25,308.5,472.25,303.25,49.0,547.75,634.0,475.25,561.75,780.75,444.25,391.0,2367.25,751.25,14462.0,270.5,48.0,53.25,65.5,754.25,32.25,69.0,42.75,105.0,105.0,18.25,105.0,28.75,3.0,70.75,44.5,65.5,1805.5,517.5,203.0,203.0,102.5,291.25
门_11146,54.25,66.5,1827.0,1591.0,54.25,489.25,451.25,3296.5,674.0,456.75,104.5,1013.25,1075.75,1096.75,404.5,487.5,931.5,9035.5,54.25,45.25,139.25,136.75,136.75,136.75,289.25,263.0,230.75,382.5,337.75,333.0,496.75,348.75,53.5,511.0,611.25,499.75,509.25,784.25,505.5,362.25,2428.5,700.5,14450.75,236.5,53.25,70.75,55.0,703.5,70.75,34.0,61.5,56.75,56.75,72.5,56.75,91.75,70.75,3.0,70.75,44.5,1830.0,459.75,266.0,266.0,58.75,270.25
门_11147,63.0,72.75,1800.75,1563.0,63.0,533.0,472.25,3306.0,658.25,514.5,160.0,1022.0,1047.75,1068.75,461.25,485.75,929.75,9093.25,63.0,54.0,197.0,158.5,161.25,159.5,296.25,236.75,262.5,440.25,311.5,306.75,470.5,322.5,63.0,549.5,630.5,473.5,563.5,777.25,479.25,405.0,2402.25,751.25,14476.0,284.5,62.0,67.25,65.5,754.25,53.25,69.0,67.25,114.5,114.5,46.25,114.5,65.5,44.5,70.75,3.0,65.5,1803.75,517.5,239.75,239.75,106.0,291.25
门_11149,63.0,75.25,1821.75,1585.75,63.0,506.75,446.0,3305.25,658.25,488.25,134.25,1022.0,1070.5,1091.5,436.0,482.25,926.25,9067.0,63.0,54.0,170.75,135.0,135.0,135.0,294.5,257.75,243.0,414.0,332.5,327.75,491.5,343.5,62.25,525.0,606.0,494.5,539.0,779.0,500.25,379.75,2423.25,725.0,14475.25,261.0,62.0,65.5,63.75,728.0,65.5,53.25,70.25,88.25,88.25,67.25,88.25,86.5,65.5,44.5,65.5,3.0,1824.75,491.25,260.75,260.75,81.5,265.0
门_11153,1788.0,1784.5,1489.0,3284.5,1797.0,2272.75,2212.0,5033.25,2132.0,2272.25,1911.25,2756.0,2501.0,2807.75,2202.0,2225.5,2679.25,10841.0,1803.75,1794.75,1951.0,1899.25,1901.0,1912.5,2052.75,1983.75,2003.25,2180.0,2048.75,1753.0,2191.0,2032.5,1803.75,2302.5,2383.5,1925.0,2314.75,2530.25,2189.25,2145.75,4123.75,2509.0,16216.75,2025.25,1802.75,1817.75,1821.5,2512.0,1814.25,1826.75,1804.25,1859.75,1859.75,1807.25,1859.75,1800.25,1805.5,1830.0,1803.75,1824.75,3.0,2275.25,1986.75,1986.75,1857.25,2040.75
门_11154,490.5,502.75,2272.25,2035.5,490.5,903.25,882.75,3724.5,1120.75,438.0,528.5,1449.5,1522.5,1541.25,818.5,934.25,1378.25,9446.0,490.5,481.5,559.75,550.75,550.75,549.0,730.75,707.5,660.5,796.5,774.25,778.25,943.5,785.25,481.5,923.25,1041.0,946.5,921.5,1231.0,942.0,776.25,2865.0,1133.25,14883.5,659.25,489.5,510.0,491.25,1136.25,511.75,466.75,497.75,477.25,477.25,513.5,477.25,536.25,517.5,459.75,517.5,491.25,2275.25,3.0,710.5,710.5,489.75,701.75
门_11155,235.25,247.5,1983.75,1677.0,233.75,716.25,667.5,3482.0,869.25,707.5,346.5,1198.0,1237.75,1222.0,637.25,682.75,1126.75,9276.25,239.0,230.0,386.25,334.5,344.5,347.75,481.0,183.0,438.5,623.5,463.0,489.75,664.0,474.0,239.0,737.75,829.25,661.75,751.75,976.0,588.0,581.0,2511.0,944.25,14652.0,460.5,238.0,243.25,256.75,947.25,218.75,262.0,229.25,295.0,295.0,204.75,295.0,199.5,203.0,266.0,239.75,260.75,1986.75,710.5,3.0,186.0,292.5,486.5
门_11165,235.25,247.5,1983.75,1677.0,233.75,716.25,667.5,3482.0,869.25,707.5,346.5,1198.0,1237.75,1222.0,637.25,682.75,1126.75,9276.25,239.0,230.0,386.25,334.5,344.5,347.75,481.0,183.0,438.5,623.5,463.0,489.75,664.0,474.0,239.0,737.75,829.25,661.75,751.75,976.0,588.0,581.0,2511.0,944.25,14652.0,460.5,238.0,243.25,256.75,947.25,218.75,262.0,229.25,295.0,295.0,204.75,295.0,199.5,203.0,266.0,239.75,260.75,1986.75,710.5,186.0,3.0,292.5,486.5
门诊手术室_40008,72.5,84.75,1854.25,1608.5,72.5,499.0,459.25,3297.5,711.0,486.75,55.0,1031.5,1093.25,1114.25,405.5,516.5,960.5,9009.0,63.5,63.5,143.0,125.5,125.5,115.75,312.5,289.5,189.0,402.0,356.25,360.25,525.75,367.25,54.5,507.5,609.5,528.75,521.5,801.75,524.0,363.25,2447.0,706.0,14409.0,214.75,71.5,92.0,83.25,709.0,93.75,50.0,79.75,51.75,51.75,95.5,51.75,118.25,102.5,58.75,106.0,81.5,1857.25,489.75,292.5,292.5,1,278.25
骨科_20009,267.25,279.5,2037.75,1801.75,267.25,710.5,635.0,3492.25,871.0,698.75,332.25,1226.25,1286.5,1307.5,619.5,698.25,1132.5,9260.25,258.25,258.25,370.25,318.5,338.75,338.75,507.25,483.5,428.25,607.25,538.75,543.75,707.5,559.5,249.25,728.75,795.0,700.75,734.0,995.0,716.25,563.25,2629.5,943.75,14670.25,446.25,266.25,286.75,282.5,946.75,288.5,279.0,274.5,279.0,279.0,290.25,279.0,312.25,291.25,270.25,291.25,265.0,2040.75,701.75,486.5,486.5,278.25,223
面积,554606.00,326947.00,114436.00,158725.00,469641.00,156895.00,205904.00,292003.00,553919.00,317620.00,207908.00,463762.00,210075.00,137990.00,133453.00,273260.00,273260.00,1503304.00,432330.00,1617941.00,126317.00,6762.00,6762.00,6762.00,664487.00,482857.00,429863.00,133453.00,137990.00,116365.00,86640.00,137990.00,411610.00,156895.00,209503.00,206055.00,133453.00,273260.00,133105.00,156895.00,133067.00,217902.00,465431.00,34333.00,150.00,190.00,170.00,170.00,395.00,380.00,235.00,162.00,438.00,190.00,162.00,250.00,415.00,370.00,145.00,145.00,145.00,165.00,145.00,235.00,465431.00,205904.00
</file>

<file path="src/network/network.py">
"""
Orchestrates the construction of a single-floor network graph.
"""

import numpy as np
import networkx as nx
import logging
from typing import Dict, Tuple, Any, List, Optional
from scipy.spatial import KDTree  # For connecting doors to mesh nodes

from src.config import NetworkConfig
from src.graph.graph_manager import GraphManager
from src.graph.node import Node
from src.image_processing.processor import ImageProcessor
from .node_creators import (  # Assuming node_creators.py is in the same directory
    BaseNodeCreator,
    RoomNodeCreator,
    VerticalNodeCreator,
    PedestrianNodeCreator,
    OutsideNodeCreator,
    ConnectionNodeCreator
)

logger = logging.getLogger(__name__)

class Network:
    """
    Manages the creation of a network graph for a single floor from an image.

    The process involves:
    1. Loading and preprocessing the image.
    2. Creating different types of nodes (rooms, doors, corridors, etc.) using
       specialized NodeCreator strategies.
    3. Establishing connections between nodes, including specific logic for
       connecting doors to pedestrian/outside mesh areas.
    """

    def __init__(self,
                 config: NetworkConfig,
                 color_map_data: Dict[Tuple[int, int, int], Dict[str, Any]],
                 id_generator_start_value: int):
        """
        Initializes the Network orchestrator.

        Args:
            config: The main configuration object.
            color_map_data: The RGB color to type mapping.
            id_generator_start_value: The starting ID for nodes in this network.
                                      Crucial for `SuperNetwork` to ensure global ID uniqueness
                                      when processing multiple floors in parallel.
        """
        self.config = config
        self.color_map_data = color_map_data  # Passed to creators

        self.image_processor = ImageProcessor(config, color_map_data)
        self.graph_manager = GraphManager(id_generator_start_value)

        # Initialize node creators
        self._node_creators: List[BaseNodeCreator] = [
            RoomNodeCreator(config, color_map_data,
                            self.image_processor, self.graph_manager),
            VerticalNodeCreator(config, color_map_data,
                                self.image_processor, self.graph_manager),
            # Pedestrian and Outside creators mark areas in id_map first, then create mesh.
            # Connection creator relies on these id_map markings.
            PedestrianNodeCreator(config, color_map_data,
                                  self.image_processor, self.graph_manager),
            # Outside creator might be conditional based on 'outside' flag in run()
            # Connection creator should run after rooms, vertical, pedestrian, outside areas are marked/created
            ConnectionNodeCreator(config, color_map_data,
                                  self.image_processor, self.graph_manager)
        ]

        # have an instance of OutsideNodeCreator for conditional use
        self._outside_node_creator = OutsideNodeCreator(
            config, color_map_data, self.image_processor, self.graph_manager)

        self._current_image_data: Optional[np.ndarray] = None
        self._id_map: Optional[np.ndarray] = None
        self._image_height: Optional[int] = None
        self._image_width: Optional[int] = None

    def _initialize_run(self, image_path: str) -> None:
        """Loads image, prepares internal data structures for a run."""
        # Load and preprocess image (quantize colors)
        raw_image_data = self.image_processor.load_and_prepare_image(
            image_path)
        self._current_image_data = self.image_processor.quantize_colors(
            raw_image_data)

        self._image_height, self._image_width = self.image_processor.get_image_dimensions()

        # id_map stores the ID of the node occupying each pixel, or special area IDs
        self._id_map = np.full((self._image_height, self._image_width),
                               self.config.BACKGROUND_ID_MAP_VALUE, dtype=np.int32)  # Use int32 for IDs

        # This ensures 'out' doors can be identified even if OutsideNodeCreator doesn't run
        # to create detailed outside mesh nodes.
        if self.config.OUTSIDE_TYPES:  # Check if there are any outside types defined
            for outside_type_name in self.config.OUTSIDE_TYPES:
                # Create a mask for this outside type from the quantized image
                # We don't need full morphology here, just the raw areas.
                # ConnectionNodeCreator will use its own dilation.
                outside_mask = self._outside_node_creator._create_mask_for_type(
                    self._current_image_data,
                    outside_type_name,
                    apply_morphology=True  # Apply basic morphology to clean up the mask
                )
                if outside_mask is not None:
                    self._id_map[outside_mask !=
                                 0] = self.config.OUTSIDE_ID_MAP_VALUE

    def _create_all_node_types(self, z_level: int, process_outside_nodes: bool) -> None:
        """Iterates through node creators to populate the graph."""
        if self._current_image_data is None or self._id_map is None:
            raise RuntimeError(
                "Network run not initialized properly. Call _initialize_run first.")

        # Specific order of creation can be important
        # 1. Rooms and Vertical transport (solid areas with own IDs)
        # 2. Pedestrian areas (mesh + special ID in id_map)
        # 3. Outside areas (mesh + special ID in id_map) - if requested
        # 4. Connections (doors - rely on previously set IDs in id_map)

        # Execute creators in the predefined order
        for creator in self._node_creators:
            logger.info(f"Running creator: {creator.__class__.__name__}")
            creator.create_nodes(self._current_image_data,
                                 self._id_map, z_level)

        # Conditionally run the OutsideNodeCreator to create actual mesh nodes for outside
        # FIXME: 不需要室外节点
        # if process_outside_nodes:
        #     logger.info(f"Running creator: {self._outside_node_creator.__class__.__name__} (for mesh)")
        #     # Note: _create_mesh_nodes_for_mask in OutsideNodeCreator also sets id_map,
        #     # but it's okay as it will set the same OUTSIDE_ID_MAP_VALUE before creating nodes.
        #     self._outside_node_creator.create_nodes(
        #         self._current_image_data, self._id_map, z_level)

    def _connect_doors_to_mesh_areas(self, z_level: int) -> None:
        """
        Connects door nodes (type 'in' or 'out') to the nearest pedestrian/outside
        mesh nodes respectively.
        """
        if not self.graph_manager.get_all_nodes():
            return  # Optimization

        connection_nodes = [
            node for node in self.graph_manager.get_all_nodes()
            if node.node_type in self.config.CONNECTION_TYPES and node.pos[2] == z_level
            # Only connect these
            and (node.door_type == 'in' or node.door_type == 'out')
        ]
        if not connection_nodes:
            return

        pedestrian_mesh_nodes = [
            node for node in self.graph_manager.get_all_nodes()
            if node.node_type in self.config.PEDESTRIAN_TYPES and node.pos[2] == z_level
        ]
        outside_mesh_nodes = [
            node for node in self.graph_manager.get_all_nodes()
            if node.node_type in self.config.OUTSIDE_TYPES and node.pos[2] == z_level
        ]

        ped_tree = None
        if pedestrian_mesh_nodes:
            ped_positions = np.array([p_node.pos[:2]
                                     for p_node in pedestrian_mesh_nodes])
            if ped_positions.size > 0:  # Ensure not empty before creating KDTree
                ped_tree = KDTree(ped_positions)

        out_tree = None
        if outside_mesh_nodes:
            out_positions = np.array([o_node.pos[:2]
                                     for o_node in outside_mesh_nodes])
            if out_positions.size > 0:
                out_tree = KDTree(out_positions)

        max_door_to_mesh_distance = self.config.GRID_SIZE * 3

        for conn_node in connection_nodes:
            door_pos_2d = conn_node.pos[:2]

            if conn_node.door_type == 'in' and ped_tree:
                dist, idx = ped_tree.query(door_pos_2d, k=1)
                # Check if idx is a valid index and not out of bounds (e.g. if ped_tree was empty for some reason)
                if idx < len(pedestrian_mesh_nodes) and dist <= max_door_to_mesh_distance:
                    nearest_ped_node = pedestrian_mesh_nodes[idx]
                    self.graph_manager.connect_nodes_by_ids(
                        conn_node.id, nearest_ped_node.id)

            elif conn_node.door_type == 'out':  # 'out' doors connect to outside AND potentially nearby pedestrian areas
                connected_to_main_outside = False
                if out_tree:
                    dist, idx = out_tree.query(door_pos_2d, k=1)
                    if idx < len(outside_mesh_nodes) and dist <= max_door_to_mesh_distance:
                        nearest_out_node = outside_mesh_nodes[idx]
                        self.graph_manager.connect_nodes_by_ids(
                            conn_node.id, nearest_out_node.id)
                        connected_to_main_outside = True

                # Also check for nearby pedestrian nodes if this 'out' door is on a path
                if ped_tree:
                    # Query for potentially multiple pedestrian nodes within a smaller radius
                    # This is for cases like an exit onto a patio (pedestrian) then to lawn (outside)
                    indices_in_ball = ped_tree.query_ball_point(
                        door_pos_2d, r=np.sqrt(2 * self.config.GRID_SIZE ** 2))
                    for ped_idx in indices_in_ball:
                        if ped_idx < len(pedestrian_mesh_nodes):
                            self.graph_manager.connect_nodes_by_ids(
                                conn_node.id, pedestrian_mesh_nodes[ped_idx].id)
                            # If it connects to outside mesh AND pedestrian mesh, that's fine.
                            # The pathfinding will choose the best route.

    def run(self, image_path: str, z_level: int = 0, process_outside_nodes: bool = False) \
            -> Tuple[nx.Graph, int, int, int]:
        """
        Executes the full network generation pipeline.
        Args:
            process_outside_nodes: If True, detailed mesh nodes for outside areas are created.
                                   If False, outside areas are only marked in id_map (for door typing)
                                   but no actual outside mesh nodes are generated by OutsideNodeCreator.
        """
        logger.info(f"--- Processing floor: {image_path} at z={z_level}, process_outside_nodes={process_outside_nodes} ---")
        
        # self.graph_manager.clear(...) # Only if reusing Network instance, typically not.

        self._initialize_run(image_path) # This now pre-marks OUTSIDE_ID_MAP_VALUE
        
        self._create_all_node_types(z_level, process_outside_nodes) # process_outside_nodes controls mesh creation
        
        self._connect_doors_to_mesh_areas(z_level)
        
        logger.info(f"--- Finished floor. Nodes: {len(self.graph_manager.get_all_nodes())} ---")
        
        if self._image_width is None or self._image_height is None:
            raise RuntimeError("Image dimensions not set.")

        return (
            self.graph_manager.get_graph_copy(),
            self._image_width,
            self._image_height,
            self.graph_manager.get_next_available_node_id_estimate()
        )
</file>

<file path="src/config.py">
"""Configuration module for the network generation project."""

import pathlib
from typing import Dict, Tuple, List, Any, Optional

# Global COLOR_MAP - Consider encapsulating or making it part of a config loader
COLOR_MAP: Dict[Tuple[int, int, int], Dict[str, Any]] = {
    (244, 67, 54): {'name': '内诊药房', 'time': 46},
    (0, 150, 136): {'name': '挂号收费', 'time': 79},
    (103, 58, 183): {'name': '急诊科', 'time': 9000},
    (145, 102, 86): {'name': '中心供应室', 'time': 1},
    (33, 150, 243): {'name': '全科', 'time': 435},
    (3, 169, 244): {'name': '放射科', 'time': 250},
    (0, 188, 212): {'name': '儿科', 'time': 640},
    (207, 216, 220): {'name': '走廊', 'time': 1},
    (117, 117, 117): {'name': '楼梯', 'time': 1},
    (189, 189, 189): {'name': '电梯', 'time': 1},
    (158, 158, 158): {'name': '扶梯', 'time': 1},
    (76, 175, 80): {'name': '绿化', 'time': 1},
    (255, 235, 59): {'name': '墙', 'time': 1},
    (121, 85, 72): {'name': '门', 'time': 1},
    (156, 39, 176): {'name': '室外', 'time': 1},
    (139, 195, 74): {'name': '内镜中心', 'time': 960},
    (205, 220, 57): {'name': '检验中心', 'time': 180},
    (255, 193, 7): {'name': '消化内科', 'time': 245},
    (255, 152, 0): {'name': '甲状腺外科', 'time': 256},
    (254, 87, 34): {'name': '肾内科', 'time': 315},
    (169, 238, 90): {'name': '心血管内科', 'time': 886},
    (88, 67, 60): {'name': '采血处', 'time': 184},
    (239, 199, 78): {'name': '眼科', 'time': 564},
    (253, 186, 87): {'name': '中医科', 'time': 1486},
    (250, 133, 96): {'name': '耳鼻喉科', 'time': 737},
    (197, 254, 130): {'name': '口腔一区', 'time': 1004},
    (173, 133, 11): {'name': '超声科', 'time': 670},
    (119, 90, 10): {'name': '病理科', 'time': 1},
    (250, 146, 138): {'name': '骨科', 'time': 223},
    (255, 128, 171): {'name': '泌尿外科', 'time': 337},
    (33, 250, 230): {'name': '肝胆胰外科', 'time': 397},
    (82, 108, 255): {'name': '皮肤科', 'time': 462},
    (226, 58, 255): {'name': '妇科', 'time': 442},
    (100, 139, 55): {'name': '产科', 'time': 404},
    (188, 246, 126): {'name': '产房', 'time': 1},
    (113, 134, 91): {'name': '手术室', 'time': 1},
    (175, 207, 142): {'name': '门诊手术室', 'time': 1},
    (179, 116, 190): {'name': '中庭', 'time': 1},
    (232, 137, 248): {'name': '口腔科二区', 'time': 1004},
    (63, 100, 23): {'name': '神经内科', 'time': 428},
    (240, 222, 165): {'name': '呼吸内科', 'time': 359},
    (187, 24, 80): {'name': '综合激光科', 'time': 462},
    (150, 133, 179): {'name': '透析中心', 'time': 14400},
    (112, 40, 236): {'name': '肿瘤科', 'time': 2320},
    (241, 190, 186): {'name': '产前诊断门诊', 'time': 442},
    (186, 146, 160): {'name': '体检科', 'time': 1260},
    (71, 195, 180): {'name': '生殖医学科', 'time': 425},
    (187, 152, 247): {'name': '烧伤整形科', 'time': 256},
    (254, 210, 145): {'name': '介入科', 'time': 3240},
    (251, 242, 159): {'name': '栏杆', 'time': 1},
    (240, 61, 123): {'name': 'NICU', 'time': 1},
    (250, 162, 193): {'name': 'ICU', 'time': 1},
    (252, 201, 126): {'name': '静配中心', 'time': 1},
    (255, 255, 255): {'name': '空房间', 'time': 1}
}

class NetworkConfig:
    """Stores configuration parameters for network generation and plotting."""

    def __init__(self, color_map_data: Dict[Tuple[int, int, int], Dict[str, Any]] = COLOR_MAP):
        self.RESULT_PATH: pathlib.Path = pathlib.Path(__file__).parent.parent / 'result'
        self.DEBUG_PATH: pathlib.Path = pathlib.Path(__file__).parent.parent / 'debug'
        self.IMAGE_ROTATE: int = 180
        self.AREA_THRESHOLD: int = 60  # Minimum area for a component to be considered a node

        # Node Type Definitions (derived from COLOR_MAP)
        self.ALL_TYPES: List[str] = [v['name'] for v in color_map_data.values()]

        self.CONNECTION_TYPES: List[str] = ['门']
        _ban_type_base: List[str] = ['墙', '栏杆', '室外', '走廊', '电梯', '扶梯', '楼梯', '空房间', '绿化', '中庭']
        self.BAN_TYPES: List[str] = [name for name in _ban_type_base if name in self.ALL_TYPES]

        self.ROOM_TYPES: List[str] = [
            v['name'] for v in color_map_data.values()
            if v['name'] not in self.BAN_TYPES and v['name'] not in self.CONNECTION_TYPES
        ]
        self.VERTICAL_TYPES: List[str] = [name for name in ['电梯', '扶梯', '楼梯'] if name in self.ALL_TYPES]
        self.PEDESTRIAN_TYPES: List[str] = [name for name in ['走廊'] if name in self.ALL_TYPES]
        self.OUTSIDE_TYPES: List[str] = [name for name in ['室外'] if name in self.ALL_TYPES]

        # Grid and Special IDs for pixel-level identification in id_map
        self.GRID_SIZE: int = 40  # Base grid size for mesh node generation
        self.OUTSIDE_ID_MAP_VALUE: int = -1  # Special ID for 'outside' areas in the id_map
        self.BACKGROUND_ID_MAP_VALUE: int = -2 # Special ID for 'background' in the id_map
        self.PEDESTRIAN_ID_MAP_VALUE: int = -3 # Special ID for 'pedestrian' areas in the id_map

        # Node Property Times
        self.OUTSIDE_MESH_TIMES_FACTOR: int = 2  # Multiplier for grid size and time for outside nodes
        self.PEDESTRIAN_TIME: float = 1.75  # Default time for pedestrian nodes
        self.CONNECTION_TIME: float = 3.0  # Default time for connection nodes (e.g., doors)

        # Plotting and Visualization
        self.IMAGE_MIRROR: bool = True  # Whether to mirror the image horizontally in plots
        self.NODE_COLOR_FROM_MAP: bool = True  # Use colors from COLOR_MAP for nodes in plots

        self.NODE_SIZE_DEFAULT: int = 10
        self.NODE_SIZE_PEDESTRIAN: int = 5
        self.NODE_SIZE_CONNECTION: int = 8
        self.NODE_SIZE_VERTICAL: int = 10
        self.NODE_SIZE_ROOM: int = 7
        self.NODE_SIZE_OUTSIDE: int = 4
        self.NODE_OPACITY: float = 0.8
        self.SHOW_PEDESTRIAN_LABELS: bool = False

        self.HORIZONTAL_EDGE_COLOR: str = "#1f77b4"
        self.VERTICAL_EDGE_COLOR: str = "#ff7f0e"
        self.EDGE_WIDTH: float = 0.5

        # SuperNetwork Specific
        self.DEFAULT_FLOOR_HEIGHT: float = 10.0
        self.DEFAULT_VERTICAL_CONNECTION_TOLERANCE: int = 0 # Default pixel distance for connecting vertical nodes across floors
        # Estimated max nodes per floor. Used for pre-allocating ID ranges in multi-processing.
        # Should be an overestimate to avoid ID collisions.
        self.ESTIMATED_MAX_NODES_PER_FLOOR: int = 10000
        self.DEFAULT_OUTSIDE_PROCESSING_IN_SUPERNETWORK: bool = False # Default for processing outside nodes per floor in SuperNetwork
        self.GROUND_FLOOR_NUMBER_FOR_OUTSIDE: Optional[int] = None # Or 0, or None to rely on auto-detection

        # Morphology Kernel
        self.MORPHOLOGY_KERNEL_SIZE: Tuple[int, int] = (5, 5)
        self.CONNECTION_DILATION_KERNEL_SIZE: Tuple[int, int] = (3,3)

        # KDTree query parameters
        self.MESH_NODE_CONNECTIVITY_K: int = 9 # k-nearest neighbors for mesh node connection

        # Ensure paths exist
        self.RESULT_PATH.mkdir(parents=True, exist_ok=True)
        self.DEBUG_PATH.mkdir(parents=True, exist_ok=True)
</file>

<file path="main.py">
"""
Main entry point for the network generation and analysis application.

This script demonstrates how to:
1. Configure logging.
2. Load application configurations.
3. Build a single-floor network (optional).
4. Build a multi-floor super-network.
5. Plot the generated network(s) using Plotly.
6. Calculate and save room-to-room/out-door travel times.
7. Generate and evaluate workflow paths.
"""
import logging
import sys
import pathlib  # For path operations
import os  # For os.path.join if needed, but prefer pathlib
from typing import Dict, List, Optional

# 
import cProfile
import snakeviz

# Import necessary modules from the 'src' package
# Assuming COLOR_MAP is still defined in config
from src.config import NetworkConfig, COLOR_MAP
from src.network.network import Network
from src.network.super_network import SuperNetwork
from src.plotting.plotter import PlotlyPlotter
from src.analysis.travel_time import calculate_room_travel_times

# Analysis modules
from src.analysis.process_flow import PathFinder
from src.analysis.word_detect import WordDetect

# Optimization modules
from src.optimization.optimizer import (
    PhysicalLocation,
    FunctionalAssignment,
    WorkflowDefinition,
    LayoutObjectiveCalculator,  # Not directly used by main, but optimizer uses it
    LayoutOptimizer,
    EvaluatedWorkflowOutcome
)

# --- Global Logger Setup ---


def setup_logging(level=logging.INFO, log_file: Optional[pathlib.Path] = None):
    """
    Configures basic logging for the application.

    Args:
        level: The minimum logging level to output (e.g., logging.INFO, logging.DEBUG).
        log_file: Optional path to a file where logs should also be written.
    """
    root_logger = logging.getLogger()  # Get the root logger

    # Prevent adding handlers multiple times if this function is called again
    if root_logger.hasHandlers():
        # If you want to clear and reconfigure, uncomment the next line
        # for handler in root_logger.handlers[:]: root_logger.removeHandler(handler)
        # For now, if handlers exist, assume it's configured.
        # Or, more robustly, check for specific handler types if needed.
        pass  # Already configured

    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s',
                                  datefmt='%Y-%m-%d %H:%M:%S')

    # Console Handler
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setFormatter(formatter)
    console_handler.setLevel(level)  # Console can have its own level
    root_logger.addHandler(console_handler)

    # File Handler (optional)
    if log_file:
        log_file.parent.mkdir(parents=True, exist_ok=True)
        file_handler = logging.FileHandler(
            log_file, mode='a', encoding='utf-8')
        file_handler.setFormatter(formatter)
        file_handler.setLevel(level)  # File can also have its own level
        root_logger.addHandler(file_handler)

    root_logger.setLevel(level)  # Set the overall level for the root logger

# --- Main Application Logic ---


def run_single_floor_example(app_config: NetworkConfig, app_color_map: Dict):
    """
    Demonstrates building and plotting a single-floor network.
    """
    logger = logging.getLogger(__name__)
    logger.info("--- Running Single-Floor Network Example ---")

    # Define the image path for the single floor
    # Ensure this path is correct for your setup
    single_image_path_str = "./data/label/1F-meng.png"  # Example path
    single_image_path = pathlib.Path(single_image_path_str)

    if not single_image_path.exists():
        logger.error(
            f"Single floor image not found: {single_image_path}. Skipping single-floor example.")
        return

    try:
        # Initialize Network builder for a single floor
        # id_generator_start_value can be 1 for a standalone single network
        network_builder = Network(
            config=app_config,
            color_map_data=app_color_map,
            id_generator_start_value=1
        )

        # Run the network generation
        # process_outside_nodes=True to generate outside mesh if applicable for this floor
        graph, width, height, _ = network_builder.run(
            image_path=str(single_image_path),  # Network.run expects str
            z_level=0,  # Example Z-level for the single floor
            process_outside_nodes=False
        )

        logger.info(
            f"Single-floor network generated with {graph.number_of_nodes()} nodes and {graph.number_of_edges()} edges.")

        # Plot the single-floor network
        plotter = PlotlyPlotter(
            config=app_config, color_map_data=app_color_map)
        plot_output_path = app_config.RESULT_PATH / "single_floor_network_3d.html"
        plotter.plot(
            graph=graph,
            output_path=plot_output_path,
            title=f"Single-Floor Network: {single_image_path.name}",
            graph_width=width,
            graph_height=height
        )
        logger.info(f"Single-floor network plot saved to {plot_output_path}")

    except Exception as e:
        logger.error(f"Error in single-floor example: {e}", exc_info=True)


def run_multi_floor_example(app_config: NetworkConfig, app_color_map: Dict):
    """
    Demonstrates building, plotting, and analyzing a multi-floor super-network.
    """
    logger = logging.getLogger(__name__)
    logger.info("--- Running Multi-Floor SuperNetwork Example ---")

    # Define image paths for multiple floors
    # Ensure this directory and images exist
    label_dir_str = "./data/label/"
    label_dir = pathlib.Path(label_dir_str)

    if not label_dir.is_dir():
        logger.error(
            f"Label directory not found: {label_dir}. Skipping multi-floor example.")
        return

    # Collect image paths (e.g., all PNG files in the directory)
    # Using a list of strings as SuperNetwork.run expects List[str]
    image_file_paths_str: List[str] = [
        str(p) for p in sorted(label_dir.glob('*.png')) if p.is_file()]
    # Example: image_file_paths_str = ["./data/label/B1.png", "./data/label/1F.png", "./data/label/2F.png"]

    if not image_file_paths_str:
        logger.warning(
            f"No image files found in {label_dir}. Skipping multi-floor example.")
        return

    logger.info(
        f"Found {len(image_file_paths_str)} images for SuperNetwork: {image_file_paths_str}")

    try:
        # Initialize SuperNetwork builder
        # base_floor=-1 if your floors are like B-1, F1, F2...
        # num_processes can be app_config.NUM_PROCESSES if defined, or None for auto
        super_network_builder = SuperNetwork(
            config=app_config,
            color_map_data=app_color_map,
            base_floor=-1,  # Example: if B-1 is the lowest floor detected/assigned as -1
            num_processes=None  # Use os.cpu_count() or 1 if single core
        )

        # Run the SuperNetwork generation
        super_graph = super_network_builder.run(
            image_file_paths=image_file_paths_str)

        ground_floor_z_for_travel_calc = super_network_builder.designated_ground_floor_z
        if ground_floor_z_for_travel_calc is not None:
            logger.info(
                f"Using designated ground floor Z={ground_floor_z_for_travel_calc:.2f} for travel time 'OutDoor' filtering.")
        else:
            logger.warning(
                "Could not determine designated ground floor Z from SuperNetwork. 'OutDoor' filtering in travel times may be affected.")

        logger.info(
            f"SuperNetwork generated with {super_graph.number_of_nodes()} nodes and {super_graph.number_of_edges()} edges.")
        logger.info(
            f"Detected image dimensions for SuperNetwork: Width={super_network_builder.width}, Height={super_network_builder.height}")

        # Plot the SuperNetwork
        plotter = PlotlyPlotter(
            config=app_config, color_map_data=app_color_map)
        plot_output_path = app_config.RESULT_PATH / "super_network_3d.html"
        plotter.plot(
            graph=super_graph,
            output_path=plot_output_path,
            title="Multi-Floor SuperNetwork",
            graph_width=super_network_builder.width,  # Pass determined width
            graph_height=super_network_builder.height,  # Pass determined height
            floor_z_map=super_network_builder.floor_z_map  # Pass for slider labels
        )
        logger.info(f"SuperNetwork plot saved to {plot_output_path}")

        # Calculate and save room travel times for the SuperNetwork
        logger.info("Calculating travel times for SuperNetwork...")
        travel_times_output_dir = app_config.RESULT_PATH
        calculate_room_travel_times(
            graph=super_graph,
            config=app_config,
            output_dir=travel_times_output_dir,
            # Specific name for super_network results
            output_filename="super_network_travel_times.csv",
            ground_floor_z=ground_floor_z_for_travel_calc
        )

    except Exception as e:
        logger.error(f"Error in multi-floor example: {e}", exc_info=True)


def run_layout_optimization_example(app_config: NetworkConfig):
    main_logger = logging.getLogger(__name__)
    main_logger.info("--- Running Facility Layout Optimization Example ---")

    # 1. Initialize PathFinder (it loads the travel_times.csv)
    # This CSV represents the fixed physical network.
    try:
        # Assumes CSV is at default path
        path_finder = PathFinder(config=app_config)
        if path_finder.travel_times_df is None:
            main_logger.error(
                "Failed to load travel times data in PathFinder. Optimization cannot proceed.")
            return
    except Exception as e:
        main_logger.error(f"Error initializing PathFinder: {e}", exc_info=True)
        return

    # 2. Define Workflows
    # These are sequences of *functional types*.
    word_detect = WordDetect(config=app_config)
    workflow_defs = [
        WorkflowDefinition(workflow_id='WF_GynecologyA',
                           functional_sequence=word_detect.detect_nearest_word(
                               ['入口', '妇科', '采血处', '超声科', '妇科', '门诊药房', '入口']),
                           weight=1.0),
        # WorkflowDefinition(workflow_id='WF_GynecologyB',
        #                    functional_sequence=word_detect.detect_nearest_word(
        #                        ['入口', '挂号收费', '妇科', '挂号收费', '采血处', '超声科', '妇科', '挂号收费', '门诊药房', '入口']),
        #                    weight=1.0),
        # WorkflowDefinition(workflow_id='WF_PulmonologyA',
        #                    functional_sequence=word_detect.detect_nearest_word(
        #                        ['入口', '呼吸内科', '采血处', '放射科', '呼吸内科', '门诊药房', '入口']),
        #                    weight=0.5),
        # WorkflowDefinition(workflow_id='WF_PulmonologyB',
        #                    functional_sequence=word_detect.detect_nearest_word(
        #                        ['入口', '挂号收费', '呼吸内科', '挂号收费', '采血处', '放射科', '呼吸内科', '挂号收费', '门诊药房', '入口']),
        #                    weight=0.5),
        # WorkflowDefinition(workflow_id='WF_CardiologyA',
        #                    functional_sequence=word_detect.detect_nearest_word(
        #                        ['入口', '心血管内科', '采血处', '超声科', '放射科', '心血管内科', '门诊药房', '入口']),
        #                    weight=1.2),
        # WorkflowDefinition(workflow_id='WF_CardiologyB',
        #                    functional_sequence=word_detect.detect_nearest_word(
        #                        ['入口', '挂号收费', '心血管内科', '挂号收费', '采血处', '超声科', '放射科', '心血管内科', '挂号收费', '门诊药房', '入口']),
        #                    weight=1.2),
    ]
    main_logger.info(
        f"Defined {len(workflow_defs)} workflows for optimization.")

    # 3. Create Initial FunctionalAssignment
    # This typically comes from the default `name_to_ids_map` in PathFinder,
    # which reflects the "as-is" layout from the original drawings.
    initial_assignment_map = path_finder.name_to_ids_map
    if not initial_assignment_map:
        main_logger.error(
            "PathFinder's name_to_ids_map is empty. Cannot create initial assignment.")
        return
    initial_functional_assignment = FunctionalAssignment(
        initial_assignment_map)
    main_logger.info(
        "Initial functional assignment created based on PathFinder's default map.")

    # 4. Initialize and Run Optimizer
    optimizer = LayoutOptimizer(
        path_finder=path_finder,
        workflow_definitions=workflow_defs,
        config=app_config,
        area_tolerance_ratio=0.3  # TODO: Allow up to 30% area difference for swaps
    )

    best_assignment, best_objective, best_outcomes = optimizer.run_optimization(
        initial_assignment=initial_functional_assignment,
        max_iterations=50  # TODO: Adjust as needed
    )

    # 5. Report Results
    main_logger.info("\n--- Optimization Results ---")
    main_logger.info(f"Final Optimized Objective Value: {best_objective:.2f}")

    main_logger.info(
        "\nFinal Functional Assignment (Functional Type -> [Physical Name_IDs]):")
    for func_type, phys_ids in sorted(best_assignment.assignment_map.items()):
        original_ids_for_type = sorted(
            initial_assignment_map.get(func_type, []))
        # Should already be sorted if FunctionalAssignment does it
        current_ids_for_type = sorted(phys_ids)

        changed_marker = ""
        if set(original_ids_for_type) != set(current_ids_for_type):  # Compare as sets for content
            changed_marker = " << MODIFIED"
            main_logger.info(f"  Function: {func_type}{changed_marker}")
            main_logger.info(
                f"    Original Locations: {original_ids_for_type}")
            main_logger.info(f"    New Locations     : {current_ids_for_type}")
        else:
            # Use debug for unchanged to reduce log noise, or info if you want to see all
            main_logger.debug(
                f"  Function: {func_type} -> Locations: {current_ids_for_type} (Unchanged)")

    main_logger.info("\nDetails of Optimized Workflows:")
    for outcome in best_outcomes:
        flow_details = "N/A"
        if outcome.shortest_flow:
            seq = outcome.shortest_flow.actual_node_id_sequence
            if seq:
                flow_details = f"[{seq[0]} ... {seq[-1]}] (len: {len(seq)})"
        main_logger.info(f"  Workflow: {outcome.workflow_definition.workflow_id} "
                         f"(Weight: {outcome.workflow_definition.weight:.1f}) - "
                         f"Optimized Time: {outcome.average_time:.2f} - Path: {flow_details}")


def initialize_setup():
    log_dir = pathlib.Path("./logs")
    log_dir.mkdir(parents=True, exist_ok=True)
    setup_logging(level=logging.INFO, log_file=log_dir / "application.log")

    main_logger = logging.getLogger(__name__)  # Logger for this main script
    main_logger.info("Application started.")

    app_config = NetworkConfig(color_map_data=COLOR_MAP)
    # Pass explicitly if needed, or rely on config's internal copy
    app_color_map_data = COLOR_MAP

    main_logger.info(f"Results will be saved in: {app_config.RESULT_PATH}")
    main_logger.info(
        f"Debug images (if any) will be saved in: {app_config.DEBUG_PATH}")

    return main_logger, app_config, app_color_map_data


if __name__ == "__main__":
    main_logger, app_config, app_color_map_data = initialize_setup()

    # ---- Example 1: Single-floor network (optional) ----
    # main_logger.info("Attempting to run single-floor example...")
    # run_single_floor_example(app_config, app_color_map_data)

    # ---- Example 2: Multi-floor SuperNetwork (primary use case) ----
    # main_logger.info("Attempting to run multi-floor SuperNetwork example...")
    # run_multi_floor_example(app_config, app_color_map_data)

    # ---- Example 3: Process Flow ----
    # main_logger.info("Attempting to run process flow example...")
    # workflow_list = ['大门', '妇科', '采血处', '超声科', '妇科', '门诊药房', '入口']
    # finder = PathFinder(config=app_config)
    # flows = finder.generate_flows(workflow_list)
    # for flow in flows:
    #     total_time = finder.calculate_flow_total_time(flow)

    # ---- Example 4: Layout Optimization ----
    travel_times_csv_path = app_config.RESULT_PATH / 'super_network_travel_times.csv'
    if not travel_times_csv_path.exists():
        main_logger.warning(f"{travel_times_csv_path} not found.")
        main_logger.warning("Please ensure `super_network_travel_times.csv` is generated first "
                            "(e.g., by running `run_multi_floor_example`).")
        main_logger.warning("Skipping layout optimization example.")
    else:
        profiler = cProfile.Profile()
        profiler.enable()
        run_layout_optimization_example(app_config)
        profiler.disable()
        profiler.dump_stats("layout_opt.prof")

    main_logger.info("Application finished.")
</file>

</files>
