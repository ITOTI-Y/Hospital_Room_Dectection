This file is a merged representation of a subset of the codebase, containing specifically included files, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of a subset of the repository's contents that is considered the most important context.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Only files matching these patterns are included: src/, main.py, result/super_network_travel_times.csv
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
src/
  algorithms/
    adjacency/
      __init__.py
      spatial_calculator.py
      utils.py
    __init__.py
    base_optimizer.py
    constraint_manager.py
    genetic_algorithm.py
    ppo_optimizer.py
    simulated_annealing.py
  analysis/
    __init__.py
    process_flow.py
    travel_time.py
    word_detect.py
  comparison/
    __init__.py
    results_comparator.py
  core/
    __init__.py
    algorithm_manager.py
    network_generator.py
  image_processing/
    processor.py
  network/
    floor_manager.py
    graph_manager.py
    network.py
    node_creators.py
    node.py
    super_network.py
  plotting/
    __init__.py
    plotter.py
  rl_optimizer/
    data/
      cache/
        node_variants.json
        traffic_distribution.json
      cache_manager.py
      process_templates_traditional.json
      process_templates.json
    env/
      adjacency_reward_calculator.py
      cost_calculator.py
      layout_env.py
      vec_env_wrapper.py
    model/
      policy_network.py
    utils/
      baseline_monitor.py
      checkpoint_callback.py
      lr_scheduler.py
      reward_normalizer.py
      setup.py
      shared_state_manager.py
      tensorboard_callback.py
  config.py
main.py
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="src/algorithms/adjacency/__init__.py">
"""
医院布局相邻性奖励计算模块

主要功能：
1. 动态相邻性判定算法
2. 多维度相邻性评分
3. 医疗功能导向的奖励计算
4. 高性能缓存机制

使用示例：
    from src.algorithms.adjacency import AdjacencyAnalyzer
    
    analyzer = AdjacencyAnalyzer(config, travel_times, slots, depts, pathways)
    reward = analyzer.calculate_adjacency_reward(layout)
"""

# 版本信息
__version__ = "1.0.0"
__author__ = "Hospital Layout Optimization Team"

# 模块级配置
DEFAULT_CONFIG = {
    'ENABLE_ADJACENCY_REWARD': True,
    'ADJACENCY_REWARD_WEIGHT': 0.15,
    'ADJACENCY_CACHE_SIZE': 500,
    'ADJACENCY_PRECOMPUTE': True
}
</file>

<file path="src/algorithms/adjacency/spatial_calculator.py">
"""
空间相邻性计算器

基于通行时间的空间距离计算相邻性关系
"""

import numpy as np
from typing import List, Dict, Tuple
from sklearn.cluster import DBSCAN
from scipy.spatial.distance import pdist
from src.config import RLConfig
from src.rl_optimizer.utils.setup import setup_logger
from .utils import AdjacencyCalculator, MatrixBasedCalculator, safe_adjacency_calculation, calculate_distance_percentile

logger = setup_logger(__name__)


class SpatialAdjacencyCalculator(MatrixBasedCalculator):
    """
    空间相邻性计算器
    
    基于通行时间的空间距离计算相邻性关系
    """
    
    def _initialize(self, travel_times: np.ndarray):
        """
        初始化空间相邻性计算器
        
        Args:
            travel_times: 通行时间矩阵 (n_slots x n_slots)
        """
        self.travel_times = travel_times
        self.n_slots = len(travel_times)
        
        # 动态计算相邻性参数
        self.percentile_threshold = self.config.ADJACENCY_PERCENTILE_THRESHOLD
        self.k_nearest = self.config.ADJACENCY_K_NEAREST or max(2, int(np.sqrt(self.n_slots)))
        
        self._initialized = True
        
        logger.debug(f"空间相邻性计算器初始化：{self.n_slots}个槽位，"
                    f"分位数阈值={self.percentile_threshold}，K近邻={self.k_nearest}")
    
    @safe_adjacency_calculation
    def calculate_adjacency_matrix(self) -> np.ndarray:
        """
        计算空间相邻性关系矩阵
        
        Returns:
            adjacency_matrix: n_slots x n_slots 的相邻性强度矩阵
        """
        adjacency_matrix = np.zeros((self.n_slots, self.n_slots))
        
        for slot_idx in range(self.n_slots):
            # 获取该槽位到其他所有槽位的通行时间
            times_from_slot = self.travel_times[slot_idx]
            
            # 计算分位数阈值
            threshold_time = calculate_distance_percentile(times_from_slot, self.percentile_threshold)
            
            # 计算相邻性强度（基于相对距离）
            for other_slot_idx in range(self.n_slots):
                if slot_idx != other_slot_idx:
                    travel_time = times_from_slot[other_slot_idx]
                    if travel_time <= threshold_time and travel_time > 0:
                        # 相邻性强度与距离成反比
                        strength = 1.0 - (travel_time / threshold_time)
                        adjacency_matrix[slot_idx, other_slot_idx] = max(0.0, strength)
        
        logger.debug(f"空间相邻性矩阵计算完成，平均相邻槽位数="
                    f"{np.mean(np.sum(adjacency_matrix > 0, axis=1)):.2f}")
        
        return adjacency_matrix
    
    def get_adjacent_slots(self, slot_index: int) -> List[int]:
        """
        获取指定槽位的相邻槽位列表
        
        Args:
            slot_index: 槽位索引
            
        Returns:
            相邻槽位索引列表
        """
        if slot_index >= self.n_slots:
            return []
            
        # 获取该槽位的通行时间
        times_from_slot = self.travel_times[slot_index]
        
        # 计算分位数阈值
        threshold_time = calculate_distance_percentile(times_from_slot, self.percentile_threshold)
        
        # 筛选相邻槽位
        adjacent_slots = [i for i in range(self.n_slots) 
                         if i != slot_index and 
                         times_from_slot[i] <= threshold_time and 
                         times_from_slot[i] > 0]
        
        return adjacent_slots
    
    @safe_adjacency_calculation
    def calculate_adjacency_score(self, layout: List[str]) -> float:
        """
        计算布局的空间相邻性得分
        
        Args:
            layout: 当前布局（科室名称列表）
            
        Returns:
            空间相邻性得分
        """
        if not self.validate_layout(layout):
            return 0.0
            
        adjacency_matrix = self.get_or_compute_matrix()
        return self.calculate_score_from_matrix(layout, adjacency_matrix)
    
    def calculate_score_from_matrix(self, layout: List[str], 
                                  adjacency_matrix: np.ndarray) -> float:
        """
        基于预计算的相邻性矩阵计算得分
        
        Args:
            layout: 当前布局
            adjacency_matrix: 相邻性关系矩阵
            
        Returns:
            空间相邻性得分
        """
        total_score = 0.0
        adjacency_count = 0
        
        for slot_idx, dept_name in enumerate(layout):
            if dept_name is None:
                continue
                
            for other_slot_idx, other_dept_name in enumerate(layout):
                if other_dept_name is None or slot_idx == other_slot_idx:
                    continue
                
                # 获取相邻性强度
                adjacency_strength = adjacency_matrix[slot_idx, other_slot_idx]
                if adjacency_strength > 0:
                    # 检查科室偏好
                    preference = self._get_department_preference(dept_name, other_dept_name)
                    
                    if preference != 0:
                        score_contribution = preference * adjacency_strength
                        total_score += score_contribution
                        adjacency_count += 1
                        
                        if logger.isEnabledFor(10):  # DEBUG级别
                            logger.debug(f"空间相邻性：{dept_name}-{other_dept_name}，"
                                       f"强度={adjacency_strength:.3f}，偏好={preference:.3f}，"
                                       f"贡献={score_contribution:.3f}")
        
        # 归一化得分
        if adjacency_count > 0:
            normalized_score = total_score / adjacency_count
        else:
            normalized_score = 0.0
            
        return normalized_score * self.config.ADJACENCY_REWARD_BASE
    
    def _get_department_preference(self, dept1: str, dept2: str) -> float:
        """
        获取两个科室之间的相邻偏好值
        
        Args:
            dept1, dept2: 科室名称
            
        Returns:
            相邻偏好值（正数表示偏好相邻，负数表示避免相邻，0表示无偏好）
        """
        # 从配置中获取医疗功能相邻性偏好
        preferences = self.config.MEDICAL_ADJACENCY_PREFERENCES
        
        # 检查正向偏好
        if dept1 in preferences and dept2 in preferences[dept1]:
            return preferences[dept1][dept2]
        
        # 检查反向偏好
        if dept2 in preferences and dept1 in preferences[dept2]:
            return preferences[dept2][dept1]
        
        # 无特殊偏好
        return 0.0
    
    def calculate_cluster_based_adjacency(self) -> Dict[int, List[int]]:
        """
        基于密度聚类的区域相邻性分析
        
        Returns:
            聚类相邻性映射字典
        """
        try:
            # 构建通行时间特征矩阵
            time_features = self.travel_times.copy()
            
            # 动态计算DBSCAN参数
            eps_percentile = self.config.ADJACENCY_CLUSTER_EPS_PERCENTILE
            min_samples = self.config.ADJACENCY_MIN_CLUSTER_SIZE
            
            # 计算eps参数（基于数据分布）
            distances = pdist(time_features)
            if len(distances) == 0:
                return {}
            
            eps = np.percentile(distances, eps_percentile * 100)
            
            # 执行DBSCAN聚类
            clustering = DBSCAN(eps=eps, min_samples=min_samples)
            cluster_labels = clustering.fit_predict(time_features)
            
            # 构建相邻性映射
            adjacency_map = {}
            for slot_idx in range(self.n_slots):
                cluster_id = cluster_labels[slot_idx]
                if cluster_id != -1:  # 非噪声点
                    cluster_members = [i for i, label in enumerate(cluster_labels) 
                                     if label == cluster_id and i != slot_idx]
                    adjacency_map[slot_idx] = cluster_members
                else:
                    adjacency_map[slot_idx] = []
            
            logger.debug(f"聚类相邻性分析完成：eps={eps:.2f}，"
                        f"聚类数={len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)}，"
                        f"噪声点数={list(cluster_labels).count(-1)}")
            
            return adjacency_map
        
        except Exception as e:
            logger.warning(f"聚类相邻性分析失败：{e}")
            return {}
    
    def get_adjacency_statistics(self) -> Dict[str, float]:
        """
        获取空间相邻性统计信息
        
        Returns:
            统计信息字典
        """
        try:
            adjacency_matrix = self.get_or_compute_matrix()
            
            # 计算基础统计
            non_zero_count = np.count_nonzero(adjacency_matrix)
            total_pairs = self.n_slots * (self.n_slots - 1)  # 排除对角线
            
            stats = {
                'total_slots': self.n_slots,
                'adjacency_pairs': non_zero_count,
                'adjacency_density': non_zero_count / total_pairs if total_pairs > 0 else 0.0,
                'avg_adjacency_strength': np.mean(adjacency_matrix[adjacency_matrix > 0]) if non_zero_count > 0 else 0.0,
                'max_adjacency_strength': np.max(adjacency_matrix),
                'avg_neighbors_per_slot': np.mean(np.sum(adjacency_matrix > 0, axis=1))
            }
            
            return stats
            
        except Exception as e:
            logger.error(f"获取空间相邻性统计信息失败：{e}")
            return {}
</file>

<file path="src/algorithms/adjacency/utils.py">
"""
相邻性计算器抽象基类和工具函数

提供所有相邻性计算器的统一接口定义和通用功能
"""

from abc import ABC, abstractmethod
from typing import List, Dict, Any, Tuple
import numpy as np
from src.rl_optimizer.utils.setup import setup_logger

logger = setup_logger(__name__)


class AdjacencyCalculator(ABC):
    """
    相邻性计算器抽象基类
    
    定义所有相邻性计算器必须实现的接口
    """
    
    def __init__(self, config, *args, **kwargs):
        """
        初始化计算器
        
        Args:
            config: 配置对象
            *args, **kwargs: 计算器特定参数
        """
        self.config = config
        self._initialize(*args, **kwargs)
    
    @abstractmethod
    def _initialize(self, *args, **kwargs):
        """
        计算器特定的初始化逻辑
        子类必须实现此方法
        """
        pass
    
    @abstractmethod
    def calculate_adjacency_matrix(self) -> np.ndarray:
        """
        计算相邻性关系矩阵
        
        Returns:
            相邻性矩阵，形状根据计算器类型确定
        """
        pass
    
    @abstractmethod
    def get_adjacent_slots(self, slot_index: int) -> List[int]:
        """
        获取指定槽位的相邻槽位列表
        
        Args:
            slot_index: 槽位索引
            
        Returns:
            相邻槽位索引列表
        """
        pass
    
    @abstractmethod
    def calculate_adjacency_score(self, layout: List[str]) -> float:
        """
        计算布局的相邻性得分
        
        Args:
            layout: 当前布局
            
        Returns:
            相邻性得分
        """
        pass
    
    def calculate_score_from_matrix(self, layout: List[str], 
                                  adjacency_matrix: np.ndarray) -> float:
        """
        基于预计算矩阵计算得分
        
        默认实现，子类可重写以提高效率
        """
        return self.calculate_adjacency_score(layout)
    
    def validate_layout(self, layout: List[str]) -> bool:
        """
        验证布局有效性
        
        Args:
            layout: 待验证的布局
            
        Returns:
            布局是否有效
        """
        if not isinstance(layout, list):
            return False
        
        if len(layout) == 0:
            return False
        
        # 检查是否有有效的科室放置
        placed_count = sum(1 for dept in layout if dept is not None)
        return placed_count > 0
    
    def get_calculation_metadata(self) -> Dict[str, Any]:
        """
        获取计算器元数据信息
        
        Returns:
            包含计算器状态和配置的元数据字典
        """
        return {
            'calculator_type': self.__class__.__name__,
            'config': {
                key: getattr(self.config, key) for key in dir(self.config)
                if key.startswith('ADJACENCY_') and not key.startswith('_')
            },
            'is_initialized': hasattr(self, '_initialized') and self._initialized
        }


class MatrixBasedCalculator(AdjacencyCalculator):
    """
    基于矩阵的计算器基类
    
    为需要预计算矩阵的计算器提供通用功能
    """
    
    def __init__(self, config, *args, **kwargs):
        super().__init__(config, *args, **kwargs)
        self._adjacency_matrix = None
        self._matrix_computed = False
    
    def get_or_compute_matrix(self) -> np.ndarray:
        """
        获取或计算相邻性矩阵
        
        Returns:
            相邻性矩阵
        """
        if not self._matrix_computed:
            self._adjacency_matrix = self.calculate_adjacency_matrix()
            self._matrix_computed = True
        
        return self._adjacency_matrix
    
    def invalidate_matrix(self):
        """使矩阵缓存失效"""
        self._matrix_computed = False
        self._adjacency_matrix = None


def safe_adjacency_calculation(func):
    """相邻性计算的异常处理装饰器"""
    def wrapper(*args, **kwargs):
        try:
            return func(*args, **kwargs)
        except (ValueError, IndexError) as e:
            logger.error(f"相邻性计算错误：{e}")
            return 0.0
        except Exception as e:
            logger.error(f"未预期的相邻性计算错误：{e}")
            return 0.0
    return wrapper


def calculate_distance_percentile(distances: np.ndarray, percentile: float) -> float:
    """
    计算距离数组的指定分位数
    
    Args:
        distances: 距离数组
        percentile: 分位数（0-1之间）
        
    Returns:
        分位数值
    """
    if len(distances) == 0:
        return 0.0
    
    # 过滤掉零值（自身距离）
    valid_distances = distances[distances > 0]
    if len(valid_distances) == 0:
        return 0.0
    
    return np.percentile(valid_distances, percentile * 100)


def normalize_adjacency_matrix(matrix: np.ndarray) -> np.ndarray:
    """
    归一化相邻性矩阵
    
    Args:
        matrix: 原始相邻性矩阵
        
    Returns:
        归一化后的矩阵
    """
    if matrix.size == 0:
        return matrix
    
    # 按行归一化
    row_sums = np.sum(matrix, axis=1, keepdims=True)
    # 避免除零
    row_sums[row_sums == 0] = 1
    
    return matrix / row_sums


def validate_adjacency_preferences(preferences: Dict[str, Dict[str, float]]) -> bool:
    """
    验证医疗相邻性偏好配置的有效性
    
    Args:
        preferences: 相邻性偏好配置字典
        
    Returns:
        配置是否有效
    """
    try:
        for dept, dept_prefs in preferences.items():
            if not isinstance(dept, str) or not isinstance(dept_prefs, dict):
                return False
            
            for target_dept, preference in dept_prefs.items():
                if not isinstance(target_dept, str):
                    return False
                    
                if not isinstance(preference, (int, float)):
                    return False
                
                # 偏好值应在合理范围内
                if not -1 <= preference <= 1:
                    return False
        
        return True
    except Exception:
        return False
</file>

<file path="src/algorithms/__init__.py">
"""
算法模块 - 包含所有优化算法的实现
"""
</file>

<file path="src/analysis/__init__.py">
# src/analysis/__init__.py
"""Analysis module for calculating travel times and other graph metrics."""
from .travel_time import calculate_room_travel_times

__all__ = ["calculate_room_travel_times"]
</file>

<file path="src/comparison/__init__.py">
"""
对比分析模块 - 用于算法结果对比和可视化
"""
</file>

<file path="src/core/__init__.py">
"""
核心模块 - 包含网络生成器和算法管理器
"""
</file>

<file path="src/plotting/__init__.py">
# src/plotting/__init__.py
"""Plotting module for network visualization."""

from .plotter import BasePlotter, PlotlyPlotter # MatplotlibPlotter can be added later

__all__ = ["BasePlotter", "PlotlyPlotter"]
</file>

<file path="src/rl_optimizer/data/process_templates_traditional.json">
[
    {
        "process_id": "GENERAL_PRACTICE",
        "description": "全科流程",
        "core_sequence": ["挂号收费", "全科", "挂号收费", "采血处", "心血管内科", "全科", "挂号收费", "药房"],
        "start_nodes": ["门"],
        "end_nodes": ["门"],
        "base_weight": 1.0
    },
    {
        "process_id": "PHYSICAL_EXAM",
        "description": "体检流程",
        "core_sequence": ["挂号收费", "体检科", "采血处", "检验中心", "放射科", "心血管内科", "超声科", "体检科"],
        "start_nodes": ["门"],
        "end_nodes": ["门"],
        "base_weight": 1.0
    },
    {
        "process_id": "PEDIATRICS_ROUTINE",
        "description": "儿科",
        "core_sequence": ["挂号收费", "儿科", "挂号收费", "药房"],
        "start_nodes": ["门"],
        "end_nodes": ["门"],
        "base_weight": 1.0
    },
    {
        "process_id": "TCM_ROUTINE",
        "description": "中医科",
        "core_sequence": ["挂号收费", "中医科", "挂号收费", "药房"],
        "start_nodes": ["门"],
        "end_nodes": ["门"],
        "base_weight": 1.0
    },
    {
        "process_id": "RESPIRATORY_ROUTINE",
        "description": "呼吸科",
        "core_sequence": ["挂号收费", "呼吸科", "挂号收费", "采血处", "放射科", "呼吸科", "挂号收费", "药房"],
        "start_nodes": ["门"],
        "end_nodes": ["门"],
        "base_weight": 1.0
    },
    {
        "process_id": "NEPHROLOGY_ROUTINE",
        "description": "肾内科",
        "core_sequence": ["挂号收费", "肾内科", "挂号收费", "采血处", "检验中心", "超声科", "肾内科", "挂号收费", "药房"],
        "start_nodes": ["门"],
        "end_nodes": ["门"],
        "base_weight": 1.0
    },
    {
        "process_id": "RENAL_DIALYSIS",
        "description": "肾透析",
        "core_sequence": ["挂号收费", "肾内科", "挂号收费", "透析中心", "挂号收费", "药房"],
        "start_nodes": ["门"],
        "end_nodes": ["门"],
        "base_weight": 1.0
    },
    {
        "process_id": "ORTHOPEDICS_ROUTINE",
        "description": "骨科",
        "core_sequence": ["挂号收费", "骨科", "挂号收费", "采血处", "放射科", "骨科", "挂号收费", "药房"],
        "start_nodes": ["门"],
        "end_nodes": ["门"],
        "base_weight": 1.0
    },
    {
        "process_id": "CARDIOLOGY_ROUTINE",
        "description": "心血管内科",
        "core_sequence": ["挂号收费", "心血管内科", "挂号收费", "放射科", "心血管内科", "挂号收费", "药房"],
        "start_nodes": ["门"],
        "end_nodes": ["门"],
        "base_weight": 1.0
    },
    {
        "process_id": "NEUROLOGY_ROUTINE",
        "description": "神经科",
        "core_sequence": ["挂号收费", "神经科", "挂号收费", "采血处", "放射科", "神经科", "挂号收费", "药房"],
        "start_nodes": ["门"],
        "end_nodes": ["门"],
        "base_weight": 1.0
    },
    {
        "process_id": "GASTROENTEROLOGY_ROUTINE",
        "description": "消化内科",
        "core_sequence": ["挂号收费", "消化内科", "挂号收费", "采血处", "检验中心", "消化内科", "挂号收费", "内镜中心", "药房"],
        "start_nodes": ["门"],
        "end_nodes": ["门"],
        "base_weight": 1.0
    },
    {
        "process_id": "GI_ENDOSCOPY",
        "description": "消化内镜",
        "core_sequence": ["挂号收费", "内镜中心", "消化内科", "挂号收费", "药房"],
        "start_nodes": ["门"],
        "end_nodes": ["门"],
        "base_weight": 1.0
    },
    {
        "process_id": "ONCOLOGY_ROUTINE",
        "description": "肿瘤科",
        "core_sequence": ["挂号收费", "肿瘤科", "挂号收费", "采血处", "放射科", "病理科", "肿瘤科", "挂号收费", "药房"],
        "start_nodes": ["门"],
        "end_nodes": ["门"],
        "base_weight": 1.0
    },
    {
        "process_id": "UROLOGY_ROUTINE",
        "description": "泌尿外科",
        "core_sequence": ["挂号收费", "泌尿外科", "挂号收费", "采血处", "检验中心", "超声科", "泌尿外科", "挂号收费", "药房"],
        "start_nodes": ["门"],
        "end_nodes": ["门"],
        "base_weight": 1.0
    },
    {
        "process_id": "BURN_PLASTIC_SURGERY",
        "description": "烧伤整形科",
        "core_sequence": ["挂号收费", "烧伤整形科", "挂号收费", "药房"],
        "start_nodes": ["门"],
        "end_nodes": ["门"],
        "base_weight": 1.0
    },
    {
        "process_id": "THYROID_SURGERY",
        "description": "甲状腺外科",
        "core_sequence": ["挂号收费", "甲状腺外科", "挂号收费", "采血处", "超声科", "甲状腺外科", "挂号收费", "药房"],
        "start_nodes": ["门"],
        "end_nodes": ["门"],
        "base_weight": 1.0
    },
    {
        "process_id": "HEPATOBILIARY_SURGERY",
        "description": "肝胆胰外科",
        "core_sequence": ["挂号收费", "肝胆胰外科", "挂号收费", "放射科", "肝胆胰外科", "挂号收费", "内镜中心", "药房"],
        "start_nodes": ["门"],
        "end_nodes": ["门"],
        "base_weight": 1.0
    },
    {
        "process_id": "HEPATOBILIARY_ENDOSCOPY",
        "description": "肝胆内镜",
        "core_sequence": ["挂号收费", "内镜中心", "肝胆胰外科", "挂号收费", "药房"],
        "start_nodes": ["门"],
        "end_nodes": ["门"],
        "base_weight": 1.0
    },
    {
        "process_id": "REPRODUCTIVE_MEDICINE",
        "description": "生殖医学科",
        "core_sequence": ["挂号收费", "生殖医学科", "挂号收费", "检验中心", "生殖医学科", "挂号收费", "药房"],
        "start_nodes": ["门"],
        "end_nodes": ["门"],
        "base_weight": 1.0
    },
    {
        "process_id": "PRENATAL_DIAGNOSIS",
        "description": "产前诊断门诊",
        "core_sequence": ["挂号收费", "产前诊断门诊", "挂号收费", "检验中心", "超声科", "产前诊断门诊", "挂号收费", "药房"],
        "start_nodes": ["门"],
        "end_nodes": ["门"],
        "base_weight": 1.0
    },
    {
        "process_id": "OBSTETRICS_ROUTINE",
        "description": "产科",
        "core_sequence": ["挂号收费", "产科", "挂号收费", "采血处", "检验中心", "超声科", "产科", "挂号收费", "药房"],
        "start_nodes": ["门"],
        "end_nodes": ["门"],
        "base_weight": 1.0
    },
    {
        "process_id": "GYNECOLOGY_ROUTINE",
        "description": "妇科",
        "core_sequence": ["挂号收费", "妇科", "挂号收费", "采血处", "检验中心", "超声科", "妇科", "挂号收费", "药房"],
        "start_nodes": ["门"],
        "end_nodes": ["门"],
        "base_weight": 1.0
    },
    {
        "process_id": "DERMATOLOGY_ROUTINE",
        "description": "皮肤科",
        "core_sequence": ["挂号收费", "皮肤科", "挂号收费", "药房"],
        "start_nodes": ["门"],
        "end_nodes": ["门"],
        "base_weight": 1.0
    },
    {
        "process_id": "LASER_THERAPY",
        "description": "综合激光科",
        "core_sequence": ["挂号收费", "综合激光科", "挂号收费", "药房"],
        "start_nodes": ["门"],
        "end_nodes": ["门"],
        "base_weight": 1.0
    },
    {
        "process_id": "OPHTHALMOLOGY_ROUTINE",
        "description": "眼科",
        "core_sequence": ["挂号收费", "眼科", "挂号收费", "药房"],
        "start_nodes": ["门"],
        "end_nodes": ["门"],
        "base_weight": 1.0
    },
    {
        "process_id": "ENT_ROUTINE",
        "description": "耳鼻喉科",
        "core_sequence": ["挂号收费", "耳鼻喉科", "挂号收费", "放射科", "耳鼻喉科", "挂号收费", "药房"],
        "start_nodes": ["门"],
        "end_nodes": ["门"],
        "base_weight": 1.0
    },
    {
        "process_id": "DENTISTRY_AREA1_ROUTINE",
        "description": "口腔一区",
        "core_sequence": ["挂号收费", "口腔一区", "挂号收费", "放射科", "口腔一区", "挂号收费", "药房"],
        "start_nodes": ["门"],
        "end_nodes": ["门"],
        "base_weight": 1.0
    },
    {
        "process_id": "DENTISTRY_AREA2_ROUTINE",
        "description": "口腔科二区",
        "core_sequence": ["挂号收费", "口腔科二区", "挂号收费", "放射科", "口腔科二区", "挂号收费", "药房"],
        "start_nodes": ["门"],
        "end_nodes": ["门"],
        "base_weight": 1.0
    },
    {
        "process_id": "EMERGENCY_ROUTINE",
        "description": "急诊科",
        "core_sequence": ["挂号收费", "急诊科", "挂号收费", "超声科", "放射科", "急诊科", "挂号收费", "药房"],
        "start_nodes": ["门"],
        "end_nodes": ["门"],
        "base_weight": 1.0
    }
]
</file>

<file path="src/rl_optimizer/utils/baseline_monitor.py">
"""
BaselineMonitor - 动态基线监控器

监控和记录动态基线的变化，提供实时统计和可视化支持。
"""

import time
from src.rl_optimizer.utils.setup import setup_logger
from typing import Dict, Any, List, Optional, Tuple
from pathlib import Path
import json
from dataclasses import dataclass, asdict
from datetime import datetime
import matplotlib.pyplot as plt
import numpy as np

from src.config import RLConfig
from src.rl_optimizer.utils.shared_state_manager import SharedStateManager

logger = setup_logger(__name__)


@dataclass
class BaselineSnapshot:
    """基线快照数据类"""
    timestamp: float
    episode_count: int
    time_cost_baseline: Optional[float]
    adjacency_baseline: Optional[float]
    area_match_baseline: Optional[float]
    time_cost_std: Optional[float]
    adjacency_std: Optional[float]
    area_match_std: Optional[float]
    warmup_complete: bool
    
    def to_dict(self) -> Dict[str, Any]:
        """转换为字典格式"""
        return asdict(self)


class BaselineMonitor:
    """
    动态基线监控器
    
    负责监控动态基线的变化，记录历史数据，并提供可视化功能。
    """
    
    def __init__(self, config: RLConfig, shared_state_manager: SharedStateManager, 
                 log_dir: Optional[Path] = None):
        """
        初始化基线监控器
        
        Args:
            config: RL配置对象
            shared_state_manager: 共享状态管理器
            log_dir: 日志目录，默认使用config中的LOG_PATH
        """
        self.config = config
        self.shared_state = shared_state_manager
        self.log_dir = log_dir or config.LOG_PATH
        
        # 创建监控输出目录
        self.monitor_dir = self.log_dir / 'baseline_monitor'
        self.monitor_dir.mkdir(exist_ok=True)
        
        # 历史数据存储
        self.snapshots: List[BaselineSnapshot] = []
        self.last_snapshot_time = 0.0
        self.snapshot_interval = 60.0  # 每60秒记录一次快照
        
        # 统计文件路径
        self.stats_file = self.monitor_dir / 'baseline_history.json'
        self.plots_dir = self.monitor_dir / 'plots'
        self.plots_dir.mkdir(exist_ok=True)
        
        logger.info(f"基线监控器初始化完成，监控目录: {self.monitor_dir}")
    
    def take_snapshot(self) -> BaselineSnapshot:
        """
        记录当前状态的快照
        
        Returns:
            BaselineSnapshot: 当前状态快照
        """
        current_time = time.time()
        
        # 从共享状态管理器获取当前状态
        stats = self.shared_state.get_statistics()
        
        snapshot = BaselineSnapshot(
            timestamp=current_time,
            episode_count=stats['global_episode_count'],
            time_cost_baseline=self.shared_state.get_time_cost_baseline(),
            adjacency_baseline=self.shared_state.get_adjacency_baseline(),
            area_match_baseline=self.shared_state.get_area_match_baseline(),
            time_cost_std=self.shared_state.get_ema_std('time_cost_std'),
            adjacency_std=self.shared_state.get_ema_std('adjacency_reward_std'),
            area_match_std=self.shared_state.get_ema_std('area_match_std'),
            warmup_complete=stats['warmup_complete']
        )
        
        return snapshot
    
    def record_snapshot(self, force: bool = False) -> bool:
        """
        记录快照（如果满足时间间隔或强制记录）
        
        Args:
            force: 是否强制记录，忽略时间间隔
            
        Returns:
            bool: 是否成功记录了快照
        """
        current_time = time.time()
        
        if not force and (current_time - self.last_snapshot_time) < self.snapshot_interval:
            return False
        
        try:
            snapshot = self.take_snapshot()
            self.snapshots.append(snapshot)
            self.last_snapshot_time = current_time
            
            # 定期保存到文件
            if len(self.snapshots) % 10 == 0:  # 每10个快照保存一次
                self.save_history()
            
            return True
            
        except Exception as e:
            logger.error(f"记录基线快照时发生错误: {e}")
            return False
    
    def save_history(self) -> None:
        """将快照历史保存到文件"""
        try:
            history_data = {
                'config': {
                    'ema_alpha': self.config.EMA_ALPHA,
                    'warmup_episodes': self.config.BASELINE_WARMUP_EPISODES,
                    'enable_dynamic_baseline': self.config.ENABLE_DYNAMIC_BASELINE,
                    'enable_relative_improvement': self.config.ENABLE_RELATIVE_IMPROVEMENT_REWARD
                },
                'snapshots': [snapshot.to_dict() for snapshot in self.snapshots],
                'last_updated': datetime.now().isoformat()
            }
            
            with open(self.stats_file, 'w') as f:
                json.dump(history_data, f, indent=2)
                
            logger.debug(f"基线历史数据已保存: {len(self.snapshots)}个快照")
            
        except Exception as e:
            logger.error(f"保存基线历史数据时发生错误: {e}")
    
    def load_history(self) -> bool:
        """从文件加载快照历史"""
        try:
            if not self.stats_file.exists():
                return False
            
            with open(self.stats_file, 'r') as f:
                history_data = json.load(f)
            
            # 重建快照列表
            self.snapshots = []
            for snapshot_dict in history_data.get('snapshots', []):
                snapshot = BaselineSnapshot(**snapshot_dict)
                self.snapshots.append(snapshot)
            
            logger.info(f"已加载 {len(self.snapshots)} 个历史快照")
            return True
            
        except Exception as e:
            logger.error(f"加载基线历史数据时发生错误: {e}")
            return False
    
    def get_baseline_trends(self) -> Dict[str, List[Tuple[float, float]]]:
        """
        获取基线变化趋势
        
        Returns:
            dict: 包含各基线的时间序列数据
        """
        if not self.snapshots:
            return {}
        
        trends = {
            'time_cost': [],
            'adjacency': [],
            'area_match': [],
            'time_cost_std': [],
            'adjacency_std': [],
            'area_match_std': []
        }
        
        for snapshot in self.snapshots:
            timestamp = snapshot.timestamp
            
            if snapshot.time_cost_baseline is not None:
                trends['time_cost'].append((timestamp, snapshot.time_cost_baseline))
            if snapshot.adjacency_baseline is not None:
                trends['adjacency'].append((timestamp, snapshot.adjacency_baseline))
            if snapshot.area_match_baseline is not None:
                trends['area_match'].append((timestamp, snapshot.area_match_baseline))
            if snapshot.time_cost_std is not None:
                trends['time_cost_std'].append((timestamp, snapshot.time_cost_std))
            if snapshot.adjacency_std is not None:
                trends['adjacency_std'].append((timestamp, snapshot.adjacency_std))
            if snapshot.area_match_std is not None:
                trends['area_match_std'].append((timestamp, snapshot.area_match_std))
        
        return trends
    
    def plot_baseline_evolution(self, save_plots: bool = True) -> Optional[str]:
        """
        绘制基线演变图表
        
        Args:
            save_plots: 是否保存图表到文件
            
        Returns:
            str: 保存的图表文件路径，如果未保存则返回None
        """
        if not self.snapshots:
            logger.warning("没有快照数据，无法生成图表")
            return None
        
        try:
            trends = self.get_baseline_trends()
            
            # 创建子图
            fig, axes = plt.subplots(2, 2, figsize=(15, 10))
            fig.suptitle('动态基线演变趋势', fontsize=16)
            
            # 设置中文字体
            plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
            plt.rcParams['axes.unicode_minus'] = False
            
            # 绘制时间成本基线
            if trends['time_cost']:
                times, values = zip(*trends['time_cost'])
                times_rel = [(t - times[0]) / 3600 for t in times]  # 转换为小时
                axes[0, 0].plot(times_rel, values, 'b-', linewidth=2, label='时间成本基线')
                axes[0, 0].set_title('时间成本基线演变')
                axes[0, 0].set_xlabel('训练时间 (小时)')
                axes[0, 0].set_ylabel('时间成本 (秒)')
                axes[0, 0].grid(True, alpha=0.3)
                axes[0, 0].legend()
            
            # 绘制相邻性基线
            if trends['adjacency']:
                times, values = zip(*trends['adjacency'])
                times_rel = [(t - times[0]) / 3600 for t in times]
                axes[0, 1].plot(times_rel, values, 'g-', linewidth=2, label='相邻性奖励基线')
                axes[0, 1].set_title('相邻性奖励基线演变')
                axes[0, 1].set_xlabel('训练时间 (小时)')
                axes[0, 1].set_ylabel('相邻性奖励')
                axes[0, 1].grid(True, alpha=0.3)
                axes[0, 1].legend()
            
            # 绘制面积匹配基线
            if trends['area_match']:
                times, values = zip(*trends['area_match'])
                times_rel = [(t - times[0]) / 3600 for t in times]
                axes[1, 0].plot(times_rel, values, 'r-', linewidth=2, label='面积匹配基线')
                axes[1, 0].set_title('面积匹配基线演变')
                axes[1, 0].set_xlabel('训练时间 (小时)')
                axes[1, 0].set_ylabel('面积匹配分数')
                axes[1, 0].grid(True, alpha=0.3)
                axes[1, 0].legend()
            
            # 绘制标准差演变
            ax_std = axes[1, 1]
            if trends['time_cost_std']:
                times, values = zip(*trends['time_cost_std'])
                times_rel = [(t - times[0]) / 3600 for t in times]
                ax_std.plot(times_rel, values, 'b--', linewidth=2, label='时间成本标准差')
            if trends['adjacency_std']:
                times, values = zip(*trends['adjacency_std'])
                times_rel = [(t - times[0]) / 3600 for t in times]
                ax_std.plot(times_rel, values, 'g--', linewidth=2, label='相邻性标准差')
            if trends['area_match_std']:
                times, values = zip(*trends['area_match_std'])
                times_rel = [(t - times[0]) / 3600 for t in times]
                ax_std.plot(times_rel, values, 'r--', linewidth=2, label='面积匹配标准差')
            
            ax_std.set_title('标准差演变')
            ax_std.set_xlabel('训练时间 (小时)')
            ax_std.set_ylabel('标准差')
            ax_std.grid(True, alpha=0.3)
            ax_std.legend()
            
            plt.tight_layout()
            
            if save_plots:
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                plot_file = self.plots_dir / f'baseline_evolution_{timestamp}.png'
                plt.savefig(plot_file, dpi=300, bbox_inches='tight')
                plt.close()
                logger.info(f"基线演变图表已保存: {plot_file}")
                return str(plot_file)
            else:
                plt.show()
                return None
                
        except Exception as e:
            logger.error(f"绘制基线演变图表时发生错误: {e}")
            return None
    
    def generate_summary_report(self) -> Dict[str, Any]:
        """
        生成基线监控摘要报告
        
        Returns:
            dict: 摘要报告数据
        """
        if not self.snapshots:
            return {'error': '没有可用的快照数据'}
        
        latest_snapshot = self.snapshots[-1]
        first_snapshot = self.snapshots[0]
        
        # 计算训练持续时间
        duration_hours = (latest_snapshot.timestamp - first_snapshot.timestamp) / 3600
        
        report = {
            'monitoring_period': {
                'start_time': datetime.fromtimestamp(first_snapshot.timestamp).isoformat(),
                'end_time': datetime.fromtimestamp(latest_snapshot.timestamp).isoformat(),
                'duration_hours': duration_hours,
                'total_snapshots': len(self.snapshots)
            },
            'training_progress': {
                'total_episodes': latest_snapshot.episode_count,
                'warmup_complete': latest_snapshot.warmup_complete,
                'warmup_episodes': self.config.BASELINE_WARMUP_EPISODES
            },
            'latest_baselines': {
                'time_cost': latest_snapshot.time_cost_baseline,
                'adjacency': latest_snapshot.adjacency_baseline,
                'area_match': latest_snapshot.area_match_baseline
            },
            'latest_std_devs': {
                'time_cost': latest_snapshot.time_cost_std,
                'adjacency': latest_snapshot.adjacency_std,
                'area_match': latest_snapshot.area_match_std
            },
            'baseline_stability': self._analyze_baseline_stability(),
            'recommendations': self._generate_recommendations()
        }
        
        return report
    
    def _analyze_baseline_stability(self) -> Dict[str, Any]:
        """分析基线稳定性"""
        if len(self.snapshots) < 10:
            return {'status': 'insufficient_data', 'message': '数据不足，需要更多快照'}
        
        # 取最近的快照分析稳定性
        recent_snapshots = self.snapshots[-10:]
        
        def coefficient_of_variation(values):
            """计算变异系数"""
            if not values or len(values) < 2:
                return None
            values_array = np.array(values)
            mean_val = np.mean(values_array)
            if mean_val == 0:
                return None
            return np.std(values_array) / abs(mean_val)
        
        # 提取最近的基线值
        time_costs = [s.time_cost_baseline for s in recent_snapshots if s.time_cost_baseline is not None]
        adjacencies = [s.adjacency_baseline for s in recent_snapshots if s.adjacency_baseline is not None]
        area_matches = [s.area_match_baseline for s in recent_snapshots if s.area_match_baseline is not None]
        
        stability = {
            'time_cost_cv': coefficient_of_variation(time_costs),
            'adjacency_cv': coefficient_of_variation(adjacencies),
            'area_match_cv': coefficient_of_variation(area_matches),
            'overall_stability': 'unknown'
        }
        
        # 评估整体稳定性
        cvs = [cv for cv in [stability['time_cost_cv'], stability['adjacency_cv'], stability['area_match_cv']] if cv is not None]
        if cvs:
            avg_cv = np.mean(cvs)
            if avg_cv < 0.1:
                stability['overall_stability'] = 'stable'
            elif avg_cv < 0.3:
                stability['overall_stability'] = 'moderate'
            else:
                stability['overall_stability'] = 'unstable'
        
        return stability
    
    def _generate_recommendations(self) -> List[str]:
        """生成基于监控数据的建议"""
        recommendations = []
        
        if not self.snapshots:
            return ["需要更多训练数据进行分析"]
        
        latest = self.snapshots[-1]
        
        # 检查预热状态
        if not latest.warmup_complete:
            recommendations.append("基线仍在预热期，建议继续训练直到预热完成")
        
        # 检查基线数据完整性
        if latest.time_cost_baseline is None:
            recommendations.append("时间成本基线未初始化，可能需要调整EMA参数")
        
        if latest.adjacency_baseline is None and self.config.ENABLE_ADJACENCY_REWARD:
            recommendations.append("相邻性奖励基线未初始化，检查相邻性奖励配置")
        
        # 分析稳定性
        stability = self._analyze_baseline_stability()
        if stability.get('overall_stability') == 'unstable':
            recommendations.append("基线变化较大，考虑降低EMA平滑因子alpha或增加预热期")
        
        # 检查训练进度
        if len(self.snapshots) > 1:
            episode_rate = (latest.episode_count - self.snapshots[0].episode_count) / len(self.snapshots)
            if episode_rate < 5:
                recommendations.append("训练进度较慢，考虑增加并行环境数量或调整训练参数")
        
        return recommendations if recommendations else ["基线监控正常，继续当前训练配置"]
    
    def close(self) -> None:
        """关闭监控器，保存最终数据"""
        self.record_snapshot(force=True)  # 强制记录最后一个快照
        self.save_history()
        
        # 生成最终报告
        final_report = self.generate_summary_report()
        report_file = self.monitor_dir / 'final_report.json'
        
        try:
            with open(report_file, 'w') as f:
                json.dump(final_report, f, indent=2)
            logger.info(f"基线监控最终报告已保存: {report_file}")
        except Exception as e:
            logger.error(f"保存最终报告时发生错误: {e}")
        
        # 生成最终图表
        self.plot_baseline_evolution(save_plots=True)
        
        logger.info("基线监控器已关闭")
</file>

<file path="src/rl_optimizer/utils/checkpoint_callback.py">
# src/rl_optimizer/utils/checkpoint_callback.py

import os
import json
import pickle
import time
from pathlib import Path
from typing import Dict, Any, Optional

from stable_baselines3.common.callbacks import BaseCallback
from stable_baselines3.common.logger import Logger

from src.rl_optimizer.utils.setup import setup_logger

logger = setup_logger(__name__)


class CheckpointCallback(BaseCallback):
    """
    自定义checkpoint回调，用于定期保存完整的训练状态。
    
    支持保存：
    - 模型参数
    - 优化器状态
    - 学习率调度器状态
    - 训练元数据（步数、时间等）
    """
    
    def __init__(
        self,
        save_freq: int,
        save_path: str,
        name_prefix: str = "checkpoint",
        save_replay_buffer: bool = False,
        save_vecnormalize: bool = False,
        verbose: int = 1
    ):
        """
        初始化checkpoint回调。
        
        Args:
            save_freq (int): 每多少训练步保存一次checkpoint
            save_path (str): checkpoint保存目录
            name_prefix (str): checkpoint文件名前缀
            save_replay_buffer (bool): 是否保存replay buffer（如果有）
            save_vecnormalize (bool): 是否保存VecNormalize状态（如果有）
            verbose (int): 日志详细级别
        """
        super().__init__(verbose)
        self.save_freq = save_freq
        self.save_path = Path(save_path)
        self.name_prefix = name_prefix
        self.save_replay_buffer = save_replay_buffer
        self.save_vecnormalize = save_vecnormalize
        
        # 确保保存目录存在
        self.save_path.mkdir(parents=True, exist_ok=True)
        
        # 训练状态跟踪
        self.start_time = time.time()
        self.checkpoint_count = 0
        
    def _init_callback(self) -> None:
        """初始化回调时的操作。"""
        if self.save_path is not None:
            os.makedirs(self.save_path, exist_ok=True)
            
    def _on_step(self) -> bool:
        """每个训练步后的回调。"""
        if self.n_calls % self.save_freq == 0:
            self._save_checkpoint()
        return True
        
    def _save_checkpoint(self) -> None:
        """保存完整的checkpoint。"""
        self.checkpoint_count += 1
        checkpoint_name = f"{self.name_prefix}_{self.n_calls:08d}_steps"
        checkpoint_path = self.save_path / checkpoint_name
        
        if self.verbose >= 1:
            logger.info(f"正在保存checkpoint: {checkpoint_name}")
            
        try:
            # 1. 保存模型
            model_path = checkpoint_path.with_suffix('.zip')
            self.model.save(str(model_path))
            
            # 2. 保存训练元数据
            metadata = self._collect_metadata()
            metadata_path = checkpoint_path.with_suffix('.json')
            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)
                
            # 3. 保存训练状态（如果启用）
            if hasattr(self.model, 'get_training_state'):
                state_path = checkpoint_path.with_suffix('.pkl')
                training_state = self.model.get_training_state()
                with open(state_path, 'wb') as f:
                    pickle.dump(training_state, f)
                    
            # 4. 保存VecNormalize状态（如果有）
            if self.save_vecnormalize and hasattr(self.training_env, 'normalize_obs'):
                vecnorm_path = checkpoint_path.with_suffix('.vecnorm.pkl')
                self.training_env.save(str(vecnorm_path))
                
            if self.verbose >= 1:
                logger.info(f"Checkpoint保存成功: {checkpoint_name}")
                
        except Exception as e:
            logger.error(f"保存checkpoint时发生错误: {e}", exc_info=True)
            
    def _collect_metadata(self) -> Dict[str, Any]:
        """收集训练元数据。"""
        current_time = time.time()
        training_duration = current_time - self.start_time
        
        metadata = {
            "checkpoint_info": {
                "timestamp": current_time,
                "checkpoint_count": self.checkpoint_count,
                "training_duration_seconds": training_duration,
                "training_duration_hours": training_duration / 3600,
            },
            "training_progress": {
                "n_calls": self.n_calls,
                "num_timesteps": self.model.num_timesteps,
                "total_timesteps": getattr(self.model, '_total_timesteps', None),
                "progress_percent": (
                    self.model.num_timesteps / getattr(self.model, '_total_timesteps', 1) * 100
                    if hasattr(self.model, '_total_timesteps') and self.model._total_timesteps > 0
                    else 0
                ),
            },
            "model_info": {
                "algorithm": self.model.__class__.__name__,
                "learning_rate": self.model.learning_rate,
                "n_envs": getattr(self.model.env, 'num_envs', 1),
            }
        }
        
        # 添加学习率调度器信息（如果有）
        if hasattr(self.model, 'lr_schedule'):
            try:
                current_lr = self.model.lr_schedule(self.model.num_timesteps / 
                                                  getattr(self.model, '_total_timesteps', self.model.num_timesteps))
                metadata["model_info"]["current_learning_rate"] = float(current_lr)
            except:
                pass
                
        return metadata
        
    def get_latest_checkpoint(self) -> Optional[Path]:
        """获取最新的checkpoint路径。"""
        if not self.save_path.exists():
            return None
            
        checkpoint_files = list(self.save_path.glob(f"{self.name_prefix}_*_steps.zip"))
        if not checkpoint_files:
            return None
            
        # 按文件名中的步数排序，获取最新的
        checkpoint_files.sort(key=lambda x: int(x.stem.split('_')[-2]))
        return checkpoint_files[-1]
        
    @staticmethod
    def load_checkpoint_metadata(checkpoint_path: Path) -> Optional[Dict[str, Any]]:
        """加载checkpoint的元数据。"""
        metadata_path = checkpoint_path.with_suffix('.json')
        if not metadata_path.exists():
            return None
            
        try:
            with open(metadata_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"加载checkpoint元数据失败: {e}")
            return None
</file>

<file path="src/rl_optimizer/utils/lr_scheduler.py">
# src/rl_optimizer/utils/lr_scheduler.py

from typing import Callable


def linear_schedule(initial_value: float, final_value: float = 0.0) -> Callable[[float], float]:
    """
    创建线性学习率调度器。
    
    该调度器会从初始学习率线性衰减到最终学习率。在训练开始时使用初始值，
    在训练结束时使用最终值，中间过程线性插值。
    
    Args:
        initial_value (float): 初始学习率值
        final_value (float): 最终学习率值，默认为0.0
    
    Returns:
        Callable[[float], float]: 学习率调度函数，接收progress_remaining参数
                                  (从1.0衰减到0.0表示训练进度)
    
    Example:
        >>> # 创建从3e-4线性衰减到1e-5的调度器
        >>> lr_scheduler = linear_schedule(3e-4, 1e-5)
        >>> # 训练开始时 (progress_remaining=1.0)
        >>> lr_scheduler(1.0)  # 返回 3e-4
        >>> # 训练中期 (progress_remaining=0.5)
        >>> lr_scheduler(0.5)  # 返回约 1.5e-4
        >>> # 训练结束时 (progress_remaining=0.0)
        >>> lr_scheduler(0.0)  # 返回 1e-5
    """
    def schedule_func(progress_remaining: float) -> float:
        """
        根据训练进度计算当前学习率。
        
        Args:
            progress_remaining (float): 剩余训练进度，从1.0（开始）递减到0.0（结束）
        
        Returns:
            float: 当前应使用的学习率值
        """
        # 线性插值公式：current_lr = final_value + progress_remaining * (initial_value - final_value)
        return final_value + progress_remaining * (initial_value - final_value)
    
    return schedule_func


def constant_schedule(value: float) -> Callable[[float], float]:
    """
    创建常数学习率调度器。
    
    该调度器在整个训练过程中保持固定的学习率不变。
    
    Args:
        value (float): 固定的学习率值
    
    Returns:
        Callable[[float], float]: 学习率调度函数，始终返回固定值
    
    Example:
        >>> # 创建固定3e-4学习率的调度器
        >>> lr_scheduler = constant_schedule(3e-4)
        >>> lr_scheduler(1.0)  # 返回 3e-4
        >>> lr_scheduler(0.5)  # 返回 3e-4
        >>> lr_scheduler(0.0)  # 返回 3e-4
    """
    def schedule_func(progress_remaining: float) -> float:
        """
        返回固定的学习率值，不受训练进度影响。
        
        Args:
            progress_remaining (float): 剩余训练进度（此参数被忽略）
        
        Returns:
            float: 固定的学习率值
        """
        return value
    
    return schedule_func


def get_lr_scheduler(schedule_type: str, initial_lr: float, final_lr: float = 0.0) -> Callable[[float], float]:
    """
    根据配置创建相应的学习率调度器。
    
    Args:
        schedule_type (str): 调度器类型，支持 "linear" 和 "constant"
        initial_lr (float): 初始学习率
        final_lr (float): 最终学习率，仅在线性调度器中使用
    
    Returns:
        Callable[[float], float]: 相应的学习率调度函数
    
    Raises:
        ValueError: 当调度器类型不支持时抛出异常
    
    Example:
        >>> # 创建线性衰减调度器
        >>> scheduler = get_lr_scheduler("linear", 3e-4, 1e-5)
        >>> # 创建常数调度器
        >>> scheduler = get_lr_scheduler("constant", 3e-4)
    """
    if schedule_type == "linear":
        return linear_schedule(initial_lr, final_lr)
    elif schedule_type == "constant":
        return constant_schedule(initial_lr)
    else:
        raise ValueError(
            f"不支持的学习率调度器类型: '{schedule_type}'. "
            f"支持的类型: 'linear', 'constant'"
        )
</file>

<file path="src/rl_optimizer/utils/reward_normalizer.py">
"""
RewardNormalizer - 奖励归一化器

实现基于动态基线的奖励归一化，将各种奖励组件标准化到合理范围内，
并支持相对改进奖励机制。
"""

import numpy as np
from src.rl_optimizer.utils.setup import setup_logger
from typing import Dict, Any, Optional, Tuple
from dataclasses import dataclass

from src.config import RLConfig
from src.rl_optimizer.utils.shared_state_manager import SharedStateManager

logger = setup_logger(__name__)


@dataclass
class RewardComponents:
    """奖励组件数据类"""
    time_cost: float = 0.0
    adjacency_reward: float = 0.0
    area_match_reward: float = 0.0
    skip_penalty: float = 0.0
    completion_bonus: float = 0.0
    placement_bonus: float = 0.0
    
    def to_dict(self) -> Dict[str, float]:
        """转换为字典格式"""
        return {
            'time_cost': self.time_cost,
            'adjacency_reward': self.adjacency_reward,
            'area_match_reward': self.area_match_reward,
            'skip_penalty': self.skip_penalty,
            'completion_bonus': self.completion_bonus,
            'placement_bonus': self.placement_bonus
        }


@dataclass
class NormalizedRewardInfo:
    """归一化奖励信息数据类"""
    normalized_components: RewardComponents
    total_normalized_reward: float
    raw_components: RewardComponents
    baselines_used: Dict[str, Optional[float]]
    improvement_scores: Dict[str, Optional[float]]
    is_relative_improvement: bool = False


class RewardNormalizer:
    """
    奖励归一化器
    
    负责将各种奖励组件归一化到统一范围，基于动态基线计算相对改进奖励，
    并提供统一的权重组合机制。
    """
    
    def __init__(self, config: RLConfig, shared_state_manager: SharedStateManager):
        """
        初始化奖励归一化器
        
        Args:
            config: RL配置对象
            shared_state_manager: 共享状态管理器
        """
        self.config = config
        self.shared_state = shared_state_manager
        
        # 验证配置
        if not config.ENABLE_DYNAMIC_BASELINE:
            logger.warning("动态基线未启用，将使用固定基线归一化")
        
        logger.info("奖励归一化器初始化完成")
    
    def normalize_reward_components(self, components: RewardComponents) -> NormalizedRewardInfo:
        """
        归一化所有奖励组件
        
        Args:
            components: 原始奖励组件
            
        Returns:
            NormalizedRewardInfo: 归一化后的奖励信息
        """
        normalized_components = RewardComponents()
        baselines_used = {}
        improvement_scores = {}
        
        # 1. 归一化时间成本（越小越好，所以取负值）
        time_cost_normalized, time_baseline, time_improvement = self._normalize_time_cost(
            -abs(components.time_cost)  # 确保时间成本为负值
        )
        normalized_components.time_cost = time_cost_normalized
        baselines_used['time_cost'] = time_baseline
        improvement_scores['time_cost'] = time_improvement
        
        # 2. 归一化相邻性奖励（越大越好）
        adj_normalized, adj_baseline, adj_improvement = self._normalize_adjacency_reward(
            components.adjacency_reward
        )
        normalized_components.adjacency_reward = adj_normalized
        baselines_used['adjacency_reward'] = adj_baseline
        improvement_scores['adjacency_reward'] = adj_improvement
        
        # 3. 归一化面积匹配奖励（越大越好）
        area_normalized, area_baseline, area_improvement = self._normalize_area_match_reward(
            components.area_match_reward
        )
        normalized_components.area_match_reward = area_normalized
        baselines_used['area_match_reward'] = area_baseline
        improvement_scores['area_match_reward'] = area_improvement
        
        # 4. 处理惩罚和奖励项（这些通常是预定义的，不需要基线归一化）
        normalized_components.skip_penalty = self._clip_value(
            components.skip_penalty, -5.0, 0.0
        )
        normalized_components.completion_bonus = self._clip_value(
            components.completion_bonus, 0.0, 5.0
        )
        normalized_components.placement_bonus = self._clip_value(
            components.placement_bonus, 0.0, 5.0
        )
        
        # 5. 计算加权总奖励
        total_normalized_reward = self._compute_weighted_reward(normalized_components, improvement_scores)
        
        # 6. 应用最终裁剪
        if self.config.ENABLE_REWARD_CLIPPING:
            total_normalized_reward = self._clip_value(
                total_normalized_reward, 
                self.config.REWARD_CLIP_RANGE[0],
                self.config.REWARD_CLIP_RANGE[1]
            )
        
        # 7. 检查是否使用了相对改进
        is_relative_improvement = any(score is not None for score in improvement_scores.values())
        
        return NormalizedRewardInfo(
            normalized_components=normalized_components,
            total_normalized_reward=total_normalized_reward,
            raw_components=components,
            baselines_used=baselines_used,
            improvement_scores=improvement_scores,
            is_relative_improvement=is_relative_improvement
        )
    
    def _normalize_time_cost(self, time_cost: float) -> Tuple[float, Optional[float], Optional[float]]:
        """
        归一化时间成本
        
        Args:
            time_cost: 原始时间成本（应为负值）
            
        Returns:
            tuple: (归一化值, 使用的基线, 改进分数)
        """
        baseline = self.shared_state.get_time_cost_baseline()
        
        if baseline is None or not self.config.ENABLE_DYNAMIC_BASELINE:
            # 没有基线时，使用固定缩放
            normalized = time_cost / 10000.0  # 固定缩放因子
            return self._clip_normalized_value(normalized), baseline, None
        
        # 使用动态基线进行相对改进计算
        if self.config.ENABLE_RELATIVE_IMPROVEMENT_REWARD:
            # 计算相对改进（时间成本越小越好）
            improvement_ratio = (baseline + time_cost) / (abs(baseline) + 1e-8)
            improvement_score = improvement_ratio
            normalized = self._clip_normalized_value(improvement_score)
            return normalized, baseline, improvement_ratio
        else:
            # 基于基线的标准化
            time_cost_std = self.shared_state.get_ema_std('time_cost_std')
            if time_cost_std and time_cost_std > self.config.REWARD_NORMALIZATION_MIN_STD:
                normalized = (time_cost - baseline) / time_cost_std
                normalized = self._clip_normalized_value(normalized)
            else:
                normalized = self._clip_normalized_value(time_cost / 10000.0)
            return normalized, baseline, None
    
    def _normalize_adjacency_reward(self, adjacency_reward: float) -> Tuple[float, Optional[float], Optional[float]]:
        """
        归一化相邻性奖励
        
        Args:
            adjacency_reward: 原始相邻性奖励
            
        Returns:
            tuple: (归一化值, 使用的基线, 改进分数)
        """
        baseline = self.shared_state.get_adjacency_baseline()
        
        if baseline is None or not self.config.ENABLE_DYNAMIC_BASELINE:
            # 没有基线时，直接裁剪到合理范围
            normalized = self._clip_value(adjacency_reward, -1.0, 1.0)
            return normalized, baseline, 0.0
        
        # 使用动态基线
        if self.config.ENABLE_RELATIVE_IMPROVEMENT_REWARD:
            # 计算相对改进（相邻性奖励越大越好）
            improvement_ratio = (adjacency_reward - baseline) / (abs(baseline) + 1e-8)
            improvement_score = improvement_ratio
            normalized = self._clip_normalized_value(improvement_score)
            return normalized, baseline, improvement_ratio
        else:
            # 基于基线的标准化
            adj_std = self.shared_state.get_ema_std('adjacency_reward_std')
            if adj_std and adj_std > self.config.REWARD_NORMALIZATION_MIN_STD:
                normalized = (adjacency_reward - baseline) / adj_std
                normalized = self._clip_normalized_value(normalized)
            else:
                normalized = self._clip_value(adjacency_reward, -1.0, 1.0)
            return normalized, baseline, 0.0
    
    def _normalize_area_match_reward(self, area_match_reward: float) -> Tuple[float, Optional[float], Optional[float]]:
        """
        归一化面积匹配奖励
        
        Args:
            area_match_reward: 原始面积匹配奖励
            
        Returns:
            tuple: (归一化值, 使用的基线, 改进分数)
        """
        baseline = self.shared_state.get_area_match_baseline()
        
        if baseline is None or not self.config.ENABLE_DYNAMIC_BASELINE:
            # 没有基线时，直接裁剪到合理范围
            normalized = self._clip_value(area_match_reward, -1.0, 1.0)
            return normalized, baseline, None
        
        # 使用动态基线
        if self.config.ENABLE_RELATIVE_IMPROVEMENT_REWARD:
            # 计算相对改进（面积匹配奖励越大越好）
            improvement_ratio = (area_match_reward - baseline) / (abs(baseline) + 1e-8)
            improvement_score = improvement_ratio
            normalized = self._clip_normalized_value(improvement_score)
            return normalized, baseline, improvement_ratio
        else:
            # 基于基线的标准化
            area_std = self.shared_state.get_ema_std('area_match_std')
            if area_std and area_std > self.config.REWARD_NORMALIZATION_MIN_STD:
                normalized = (area_match_reward - baseline) / area_std
                normalized = self._clip_normalized_value(normalized)
            else:
                normalized = self._clip_value(area_match_reward, -1.0, 1.0)
            return normalized, baseline, None
    
    def _compute_weighted_reward(self, components: RewardComponents, improvement_scores: Dict[str, Optional[float]]) -> float:
        """
        计算加权总奖励
        
        Args:
            components: 归一化后的奖励组件
            
        Returns:
            float: 加权总奖励
        """
        

        if any(improvement_scores.values()):
            total_reward = (
                improvement_scores['time_cost'] * self.config.NORMALIZED_TIME_WEIGHT +
                improvement_scores['adjacency_reward'] * self.config.NORMALIZED_ADJACENCY_WEIGHT +
                improvement_scores['area_match_reward'] * self.config.NORMALIZED_AREA_WEIGHT
            )
        
        else:
            total_reward = (
            components.time_cost * self.config.NORMALIZED_TIME_WEIGHT +
            components.adjacency_reward * self.config.NORMALIZED_ADJACENCY_WEIGHT +
            components.area_match_reward * self.config.NORMALIZED_AREA_WEIGHT +
            components.skip_penalty * self.config.NORMALIZED_SKIP_PENALTY_WEIGHT +
            components.completion_bonus * self.config.NORMALIZED_COMPLETION_BONUS_WEIGHT +
            components.placement_bonus * self.config.NORMALIZED_PLACEMENT_BONUS_WEIGHT
            )
        
        return total_reward
    
    def _clip_normalized_value(self, value: float) -> float:
        """
        裁剪归一化值到合理范围
        
        Args:
            value: 待裁剪的值
            
        Returns:
            float: 裁剪后的值
        """
        clip_range = self.config.REWARD_NORMALIZATION_CLIP_RANGE
        return self._clip_value(value, -clip_range, clip_range)
    
    def _clip_value(self, value: float, min_val: float, max_val: float) -> float:
        """
        裁剪值到指定范围
        
        Args:
            value: 待裁剪的值
            min_val: 最小值
            max_val: 最大值
            
        Returns:
            float: 裁剪后的值
        """
        return np.clip(value, min_val, max_val)
    
    def update_baselines(self, components: RewardComponents) -> None:
        """
        更新动态基线
        
        Args:
            components: 当前episode的奖励组件
        """
        if not self.config.ENABLE_DYNAMIC_BASELINE:
            return
        
        # 更新时间成本基线（使用绝对值）
        self.shared_state.update_time_cost_baseline(abs(components.time_cost))
        
        # 更新相邻性奖励基线
        if components.adjacency_reward != 0.0:  # 避免更新零值
            self.shared_state.update_adjacency_baseline(components.adjacency_reward)
        
        # 更新面积匹配基线
        if components.area_match_reward != 0.0:  # 避免更新零值
            self.shared_state.update_area_match_baseline(components.area_match_reward)
        
        # 更新标准差估计（用于计算方差的EMA）
        self._update_variance_baselines(components)
    
    def _update_variance_baselines(self, components: RewardComponents) -> None:
        """
        更新方差基线（用于标准差计算）
        
        Args:
            components: 当前episode的奖励组件
        """
        # 获取当前基线
        time_baseline = self.shared_state.get_time_cost_baseline()
        adj_baseline = self.shared_state.get_adjacency_baseline()
        area_baseline = self.shared_state.get_area_match_baseline()
        
        # 计算偏差的平方并更新方差EMA
        if time_baseline is not None:
            time_deviation_sq = (abs(components.time_cost) - time_baseline) ** 2
            self.shared_state.update_ema('time_cost_std', time_deviation_sq)
        
        if adj_baseline is not None and components.adjacency_reward != 0.0:
            adj_deviation_sq = (components.adjacency_reward - adj_baseline) ** 2
            self.shared_state.update_ema('adjacency_reward_std', adj_deviation_sq)
        
        if area_baseline is not None and components.area_match_reward != 0.0:
            area_deviation_sq = (components.area_match_reward - area_baseline) ** 2
            self.shared_state.update_ema('area_match_std', area_deviation_sq)
    
    def get_normalization_stats(self) -> Dict[str, Any]:
        """
        获取当前归一化统计信息
        
        Returns:
            dict: 包含基线、标准差等统计信息
        """
        stats = {
            'time_cost_baseline': self.shared_state.get_time_cost_baseline(),
            'adjacency_baseline': self.shared_state.get_adjacency_baseline(),
            'area_match_baseline': self.shared_state.get_area_match_baseline(),
            'time_cost_std': self.shared_state.get_ema_std('time_cost_std'),
            'adjacency_reward_std': self.shared_state.get_ema_std('adjacency_reward_std'),
            'area_match_std': self.shared_state.get_ema_std('area_match_std'),
            'warmup_complete': self.shared_state.is_warmup_complete(),
            'episode_count': self.shared_state.get_episode_count(),
            'config': {
                'enable_dynamic_baseline': self.config.ENABLE_DYNAMIC_BASELINE,
                'enable_relative_improvement': self.config.ENABLE_RELATIVE_IMPROVEMENT_REWARD,
                'ema_alpha': self.config.EMA_ALPHA,
                'warmup_episodes': self.config.BASELINE_WARMUP_EPISODES,
                'relative_improvement_scale': self.config.RELATIVE_IMPROVEMENT_SCALE
            }
        }
        
        return stats
    
    def log_reward_info(self, reward_info: NormalizedRewardInfo, episode: int) -> None:
        """
        记录奖励信息到日志
        
        Args:
            reward_info: 归一化奖励信息
            episode: 当前episode编号
        """
        
        raw = reward_info.raw_components
        norm = reward_info.normalized_components
        
        logger.debug(f"Episode {episode} 奖励归一化详情:")
        logger.debug(f"  原始组件: 时间成本={raw.time_cost:.2f}, 相邻性={raw.adjacency_reward:.4f}, "
                    f"面积匹配={raw.area_match_reward:.4f}")
        logger.debug(f"  归一化组件: 时间成本={norm.time_cost:.4f}, 相邻性={norm.adjacency_reward:.4f}, "
                    f"面积匹配={norm.area_match_reward:.4f}")
        logger.debug(f"  惩罚奖励: 跳过惩罚={norm.skip_penalty:.2f}, 完成奖励={norm.completion_bonus:.2f}")
        logger.debug(f"  总归一化奖励: {reward_info.total_normalized_reward:.4f}")
        
        if reward_info.is_relative_improvement:
            logger.debug(f"  改进分数: {reward_info.improvement_scores}")
            logger.debug(f"  使用基线: {reward_info.baselines_used}")
</file>

<file path="src/rl_optimizer/utils/shared_state_manager.py">
"""
SharedStateManager - 跨进程共享状态管理器

用于PPO多进程训练环境中维护全局共享的动态基线状态。
"""

import multiprocessing as mp
from multiprocessing import Lock, Manager
from threading import RLock
from typing import Dict, Any, Optional, Union
import time
from src.rl_optimizer.utils.setup import setup_logger
from dataclasses import dataclass

logger = setup_logger(__name__)


@dataclass
class EMAState:
    """指数移动平均状态数据类"""
    value: float = 0.0
    count: int = 0
    last_updated: float = 0.0
    is_initialized: bool = False


class SharedStateManager:
    """
    跨进程共享状态管理器
    
    实现PPO多进程环境之间的全局状态共享，主要用于维护动态基线的
    指数移动平均值和相关统计信息。使用multiprocessing.Manager
    实现真正的跨进程数据共享。
    """

    _instance = None
    _lock = RLock()
    
    def __new__(cls, *args, **kwargs):
        """单例模式，确保全局只有一个实例"""
        with cls._lock:
            if cls._instance is None:
                cls._instance = super().__new__(cls)
                cls._instance._initialized = False
            return cls._instance
    
    def __init__(self, alpha: float = 0.1, warmup_episodes: int = 100):
        """
        初始化共享状态管理器
        
        Args:
            alpha: 指数移动平均的平滑因子 (0 < alpha <= 1)
            warmup_episodes: 预热期间episode数量，在此期间不更新基线
        """
        if self._initialized:
            return
            
        self.alpha = alpha
        self.warmup_episodes = warmup_episodes
        
        # 创建进程管理器和共享数据结构
        self.manager = Manager()
        self.shared_dict = self.manager.dict()
        self.update_lock = self.manager.Lock()
        
        # 初始化各种EMA状态
        self._init_ema_states()
        
        # 初始化全局统计信息
        self.shared_dict['global_episode_count'] = 0
        self.shared_dict['total_episodes'] = 0
        self.shared_dict['start_time'] = time.time()
        
        self._initialized = True
        logger.info(f"SharedStateManager初始化完成: alpha={alpha}, warmup_episodes={warmup_episodes}")
    
    def _init_ema_states(self):
        """初始化各种指数移动平均状态"""
        ema_keys = [
            'time_cost_baseline',       # 时间成本基线
            'adjacency_reward_baseline', # 相邻性奖励基线
            'area_match_baseline',      # 面积匹配基线
            'total_reward_baseline',    # 总奖励基线
            'time_cost_std',           # 时间成本标准差
            'adjacency_reward_std',     # 相邻性奖励标准差
            'area_match_std',          # 面积匹配标准差
        ]
        
        for key in ema_keys:
            self.shared_dict[key] = {
                'value': 0.0,
                'count': 0,
                'last_updated': time.time(),
                'is_initialized': False,
                'variance': 0.0  # 用于计算标准差
            }
    
    def update_ema(self, key: str, new_value: float, force_update: bool = False) -> None:
        """
        更新指定键的指数移动平均值
        
        Args:
            key: 要更新的EMA键名
            new_value: 新的观测值
            force_update: 是否强制更新（忽略预热期）
        """
        if key not in self.shared_dict:
            logger.error(f"未知的EMA键: {key}")
            return
            
        with self.update_lock:
            current_episodes = self.shared_dict['global_episode_count']
            
            # 检查是否在预热期内
            if not force_update and current_episodes < self.warmup_episodes:
                # 预热期只收集数据，不更新基线
                ema_data = dict(self.shared_dict[key])
                ema_data['count'] += 1
                ema_data['last_updated'] = time.time()
                self.shared_dict[key] = ema_data
                return
            
            # 获取当前EMA状态
            ema_data = dict(self.shared_dict[key])
            
            if not ema_data['is_initialized']:
                # 首次初始化
                ema_data['value'] = new_value
                ema_data['is_initialized'] = True
                ema_data['variance'] = 0.0
                logger.info(f"初始化EMA基线 {key}: {new_value:.6f}")
            else:
                # 更新EMA值
                old_value = ema_data['value']
                ema_data['value'] = (1 - self.alpha) * old_value + self.alpha * new_value
                
                # 更新方差（用于计算标准差）
                if ema_data['count'] > 0:
                    delta = new_value - old_value
                    ema_data['variance'] = (1 - self.alpha) * ema_data['variance'] + self.alpha * delta * delta
            
            # 更新统计信息
            ema_data['count'] += 1
            ema_data['last_updated'] = time.time()
            
            # 写回共享字典
            self.shared_dict[key] = ema_data
            
            std_dev = ema_data['variance'] ** 0.5 if ema_data['variance'] > 0 else 0.0
            logger.debug(f"更新EMA {key}: 新值={new_value:.6f}, EMA={ema_data['value']:.6f}, 标准差={std_dev:.6f}")
    
    def get_ema_value(self, key: str) -> Optional[float]:
        """
        获取指定键的EMA值
        
        Args:
            key: EMA键名
            
        Returns:
            float: EMA值，如果未初始化返回None
        """
        if key not in self.shared_dict:
            return None
            
        ema_data = self.shared_dict[key]
        if not ema_data['is_initialized']:
            return None
            
        return ema_data['value']
    
    def get_ema_std(self, key: str) -> Optional[float]:
        """
        获取指定键的EMA标准差
        
        Args:
            key: EMA键名
            
        Returns:
            float: 标准差值，如果未初始化返回None
        """
        if key not in self.shared_dict:
            return None
            
        ema_data = self.shared_dict[key]
        if not ema_data['is_initialized'] or ema_data['variance'] <= 0:
            return None
            
        return ema_data['variance'] ** 0.5
    
    def get_ema_info(self, key: str) -> Optional[Dict[str, Any]]:
        """
        获取指定键的完整EMA信息
        
        Args:
            key: EMA键名
            
        Returns:
            dict: 包含value, count, std等完整信息的字典
        """
        if key not in self.shared_dict:
            return None
            
        ema_data = dict(self.shared_dict[key])
        ema_data['std'] = ema_data['variance'] ** 0.5 if ema_data['variance'] > 0 else 0.0
        return ema_data
    
    def is_warmup_complete(self) -> bool:
        """检查预热期是否完成"""
        return self.shared_dict['global_episode_count'] >= self.warmup_episodes
    
    def increment_episode_count(self) -> int:
        """增加全局episode计数并返回当前值"""
        with self.update_lock:
            self.shared_dict['global_episode_count'] += 1
            self.shared_dict['total_episodes'] += 1
            return self.shared_dict['global_episode_count']
    
    def get_episode_count(self) -> int:
        """获取当前的全局episode计数"""
        return self.shared_dict['global_episode_count']
    
    def get_statistics(self) -> Dict[str, Any]:
        """
        获取所有共享状态的统计信息
        
        Returns:
            dict: 包含所有EMA状态和全局统计信息
        """
        stats = {
            'global_episode_count': self.shared_dict['global_episode_count'],
            'total_episodes': self.shared_dict['total_episodes'],
            'warmup_complete': self.is_warmup_complete(),
            'uptime_seconds': time.time() - self.shared_dict['start_time'],
            'ema_states': {}
        }
        
        # 收集所有EMA状态信息
        for key in self.shared_dict:
            if isinstance(self.shared_dict[key], dict) and 'value' in self.shared_dict[key]:
                stats['ema_states'][key] = self.get_ema_info(key)
        
        return stats
    
    def reset(self):
        """重置所有状态（主要用于测试）"""
        with self.update_lock:
            logger.info("重置SharedStateManager状态")
            self._init_ema_states()
            self.shared_dict['global_episode_count'] = 0
            self.shared_dict['total_episodes'] = 0
            self.shared_dict['start_time'] = time.time()
    
    def update_time_cost_baseline(self, time_cost: float) -> None:
        """更新时间成本基线的便捷方法"""
        self.update_ema('time_cost_baseline', time_cost)
    
    def update_adjacency_baseline(self, adjacency_reward: float) -> None:
        """更新相邻性奖励基线的便捷方法"""
        self.update_ema('adjacency_reward_baseline', adjacency_reward)
    
    def update_area_match_baseline(self, area_match_score: float) -> None:
        """更新面积匹配基线的便捷方法"""
        self.update_ema('area_match_baseline', area_match_score)
    
    def update_total_reward_baseline(self, total_reward: float) -> None:
        """更新总奖励基线的便捷方法"""
        self.update_ema('total_reward_baseline', total_reward)
    
    def get_time_cost_baseline(self) -> Optional[float]:
        """获取时间成本基线的便捷方法"""
        return self.get_ema_value('time_cost_baseline')
    
    def get_adjacency_baseline(self) -> Optional[float]:
        """获取相邻性奖励基线的便捷方法"""
        return self.get_ema_value('adjacency_reward_baseline')
    
    def get_area_match_baseline(self) -> Optional[float]:
        """获取面积匹配基线的便捷方法"""
        return self.get_ema_value('area_match_baseline')
    
    def get_total_reward_baseline(self) -> Optional[float]:
        """获取总奖励基线的便捷方法"""
        return self.get_ema_value('total_reward_baseline')


# 全局实例（延迟初始化）
_global_shared_state_manager = None


def get_shared_state_manager(alpha: float = 0.1, warmup_episodes: int = 100) -> SharedStateManager:
    """
    获取全局共享状态管理器实例
    
    Args:
        alpha: EMA平滑因子（仅在首次调用时有效）
        warmup_episodes: 预热期episodes数量（仅在首次调用时有效）
        
    Returns:
        SharedStateManager: 全局实例
    """
    global _global_shared_state_manager
    
    if _global_shared_state_manager is None:
        _global_shared_state_manager = SharedStateManager(alpha=alpha, warmup_episodes=warmup_episodes)
    
    return _global_shared_state_manager


def reset_shared_state_manager():
    """重置全局共享状态管理器（主要用于测试）"""
    global _global_shared_state_manager
    if _global_shared_state_manager is not None:
        _global_shared_state_manager.reset()
</file>

<file path="src/rl_optimizer/utils/tensorboard_callback.py">
# src/rl_optimizer/utils/tensorboard_callback.py


from stable_baselines3.common.callbacks import BaseCallback

from src.rl_optimizer.utils.shared_state_manager import get_shared_state_manager
from src.rl_optimizer.utils.setup import setup_logger

logger = setup_logger(__name__)


class TensorboardBaselineCallback(BaseCallback):
    """
    自定义回调，用于将baseline指标记录到tensorboard
    """
    
    def __init__(
        self,
        log_freq: int = 1000,
        verbose: int = 1
    ):
        """
        初始化tensorboard baseline回调
        
        Args:
            log_freq (int): 每多少步记录一次指标
            verbose (int): 日志详细级别
        """
        super().__init__(verbose)
        self.log_freq = log_freq
        self.shared_state = get_shared_state_manager()
        
    def _init_callback(self) -> None:
        """初始化回调"""
        pass
        
    def _on_step(self) -> bool:
        """每个训练步后的回调"""
        if self.n_calls % self.log_freq == 0:
            self._log_baseline_metrics()
        return True
        
    def _log_baseline_metrics(self) -> None:
        """记录baseline指标到tensorboard"""
        try:
            # 获取各种baseline指标
            time_cost_baseline = self.shared_state.get_time_cost_baseline()
            adjacency_baseline = self.shared_state.get_adjacency_baseline()
            area_match_baseline = self.shared_state.get_area_match_baseline()
            total_reward_baseline = self.shared_state.get_total_reward_baseline()
            
            # 记录到tensorboard
            if time_cost_baseline is not None:
                self.logger.record("baseline/time_cost_baseline", time_cost_baseline)
                
            if adjacency_baseline is not None:
                self.logger.record("baseline/adjacency_baseline", adjacency_baseline)
                
            if area_match_baseline is not None:
                self.logger.record("baseline/area_match_baseline", area_match_baseline)
                
            if total_reward_baseline is not None:
                self.logger.record("baseline/total_reward_baseline", total_reward_baseline)
                
        except Exception as e:
            logger.error(f"记录baseline指标时发生错误: {e}", exc_info=True)
</file>

<file path="src/rl_optimizer/env/adjacency_reward_calculator.py">
# src/rl_optimizer/env/adjacency_reward_calculator.py

import numpy as np
import scipy.sparse as sp
from typing import List, Dict, Tuple, Optional
from functools import lru_cache
import time

from src.config import RLConfig
from src.algorithms.constraint_manager import ConstraintManager
from src.rl_optimizer.utils.setup import setup_logger

# @dataclass
# class AdjacencyMetrics:
#     """相邻性奖励计算的性能指标数据类。"""
#     total_time: float = 0.0
#     spatial_time: float = 0.0
#     functional_time: float = 0.0
#     connectivity_time: float = 0.0
#     cache_hits: int = 0
#     cache_misses: int = 0
#     computation_count: int = 0

class AdjacencyRewardCalculator:
    """
    相邻性奖励计算器。
    """
    
    def __init__(self, config: RLConfig, placeable_depts: List[str], 
                 travel_times_matrix, constraint_manager: ConstraintManager):
        self.config = config
        self.placeable_depts = placeable_depts
        self.dept_to_idx = {dept: idx for idx, dept in enumerate(placeable_depts)}
        self.dept_to_idx[None] = -1
        self.num_depts = len(placeable_depts)

        self.placeable_slots = constraint_manager.placeable_slots
        self.num_slots = len(self.placeable_slots)

        self.travel_times_matrix = travel_times_matrix

        self.logger = setup_logger(__name__)

        self.functional_preference_matrix = self._precompute_functional_preference_matrix()

        self.slot_adjacency_matrix = self._precompute_slot_adjacency_matrix()

        self.logger.info(f"动态相邻性奖励计算器初始化完成。")

    def _precompute_functional_preference_matrix(self):
        """
        预计算功能偏好矩阵。
        """
        matrix  = np.zeros((self.num_depts, self.num_depts), dtype=np.float32)
        for i, dept1 in enumerate(self.placeable_depts):
            generic1 = dept1.split('_')[0]
            for j, dept2 in enumerate(self.placeable_depts):
                if i != j:
                    generic2 = dept2.split('_')[0]
                    pref = self.config.MEDICAL_ADJACENCY_PREFERENCES.get(generic1, {}).get(generic2, 0)
                    if pref == 0:
                        pref = self.config.MEDICAL_ADJACENCY_PREFERENCES.get(generic2, {}).get(generic1, 0)
                    matrix[i, j] = pref
        return matrix
    
    def _precompute_slot_adjacency_matrix(self):
        """
        预计算槽位之间的空间相邻强度矩阵（使用连续强度值）。
        返回值: (N_slots, N_slots) 的矩阵，存储槽位间的相邻强度(0到1)。
        由于当前通行时间矩阵包含科室就诊时间，空间相邻性奖励不建议使用
        """

        slot_names = self.placeable_slots
        distance_matrix = self.travel_times_matrix.loc[slot_names, slot_names].values

        uppper_triangle = distance_matrix[np.triu_indices_from(distance_matrix, k=1)]
        valid_distances = uppper_triangle[uppper_triangle > 0]

        if len(valid_distances) == 0:
            self.logger.warning("所有槽位间的距离均为零，无法计算空间相邻强度。")
            return np.zeros((self.num_slots, self.num_slots), dtype=np.float32)
        
        threshold = np.percentile(valid_distances, self.config.ADJACENCY_PERCENTILE_THRESHOLD * 100)
        self.logger.info(f"空间相邻性距离阈值设为 {threshold:.2f}")

        with np.errstate(divide='ignore', invalid='ignore'):
            strength_matrix = 1.0 - (distance_matrix / threshold)

        strength_matrix[distance_matrix > threshold] = 0
        strength_matrix[distance_matrix <= 0] = 0
        np.fill_diagonal(strength_matrix, 0)
        strength_matrix = np.maximum(strength_matrix, 0)

        self.logger.debug(f"槽位相邻强度矩阵计算完成，平均强度: {np.mean(strength_matrix):.3f}")

        return strength_matrix
    
    @lru_cache(maxsize=1000)
    def calculate_reward(self, layout_tuple: Tuple[Optional[str], ...]) -> float:
        """
        计算给定完整布局的相邻性总奖励。
        
        Args:
            layout_tuple: 一个元组，长度为槽位数。每个元素是科室名称或None。
                          例如: ('儿科_10006', '全科_10004', None, ...)
        
        Returns:
            综合相邻性奖励分数。
        """
        if len(layout_tuple) != self.num_slots:
            self.logger.error(f"布局长度 ({len(layout_tuple)}) 与槽位数 ({self.num_slots}) 不匹配。")
            return 0.0
        
        dept_indices_in_layout = np.array([self.dept_to_idx[dept] for dept in layout_tuple], dtype=np.int32)

        slot_i_indices, slot_j_indices = np.meshgrid(np.arange(self.num_slots), np.arange(self.num_slots), indexing='ij')

        slot_adjacency_values = self.slot_adjacency_matrix[slot_i_indices, slot_j_indices]

        depts_at_i = dept_indices_in_layout[slot_i_indices]
        depts_at_j = dept_indices_in_layout[slot_j_indices]

        valid_pair_mask = (depts_at_i >= 0) & (depts_at_j >= 0) & (depts_at_i != depts_at_j)

        functional_pref_values = np.zeros_like(depts_at_i, dtype=np.float32)
        valid_depts_at_i = depts_at_i[valid_pair_mask]
        valid_depts_at_j = depts_at_j[valid_pair_mask]
        functional_pref_values[valid_pair_mask] = self.functional_preference_matrix[valid_depts_at_i, valid_depts_at_j]

        pair_scores = functional_pref_values * slot_adjacency_values

        total_score = np.sum(np.triu(pair_scores, k=1))

        num_valid_pairs = np.sum(np.triu(valid_pair_mask, k=1))
        if num_valid_pairs == 0:
            return 0.0
        
        normalized_score = total_score / num_valid_pairs
        
        final_reward = normalized_score * self.config.ADJACENCY_REWARD_BASE
        
        self.logger.info(f"动态相邻性奖励计算: 总分={total_score:.3f}, 有效对数={num_valid_pairs}, 归一化分={normalized_score:.3f}, 最终奖励={final_reward:.3f}")

        return {'total_reward': final_reward}
    
def create_adjacency_calculator(config: RLConfig, placeable_depts: List[str], 
                               travel_times_matrix, constraint_manager: ConstraintManager) -> AdjacencyRewardCalculator:
    return AdjacencyRewardCalculator(
            config=config,
            placeable_depts=placeable_depts,
            travel_times_matrix=travel_times_matrix,
            constraint_manager=constraint_manager
        )
</file>

<file path="src/rl_optimizer/model/policy_network.py">
# src/rl_optimizer/model/policy_network.py

import torch
import torch.nn as nn
from gymnasium import spaces
from stable_baselines3.common.torch_layers import BaseFeaturesExtractor
from typing import Dict

from src.config import RLConfig
from src.rl_optimizer.utils.setup import setup_logger

logger = setup_logger(__name__)

class LayoutTransformer(BaseFeaturesExtractor):
    """
    基于Transformer的特征提取器，用于处理布局状态。

    该网络接收环境的字典式观测，通过嵌入层和Transformer编码器，
    学习布局中已放置科室与待放置科室之间的复杂空间和逻辑关系，
    最终为策略网络（Actor-Critic）输出一个固定维度的特征向量。

    该设计特别适用于自回归构建任务，因为它能有效地处理序列信息
    和元素之间的长程依赖。
    """

    def __init__(self, observation_space: spaces.Dict, features_dim: int, config: RLConfig):
        """
        初始化Transformer特征提取器。

        Args:
            observation_space (spaces.Dict): 环境的观测空间。
            features_dim (int): 输出特征的维度。
            config (RLConfig): RL优化器的配置对象。
        """
        super().__init__(observation_space, features_dim)

        self.config = config

        # 从观测空间中提取维度信息
        # 槽位数量，决定了序列长度
        num_slots = observation_space["layout"].shape[0]
        
        # 科室种类数量 (包括0，代表"未放置"或"空")
        # 支持两种观测空间定义方式
        if hasattr(observation_space["layout"], 'nvec'):
            # MultiDiscrete 空间
            num_depts = int(observation_space["layout"].nvec[0])
        else:
            # Box 空间
            num_depts = int(observation_space["layout"].high[0]) + 1 # high是最大值，种类数是最大值+1

        embedding_dim = self.config.EMBEDDING_DIM

        # --- 网络层定义 ---
        
        # 1. 科室嵌入层 (Dept Embedding)
        # 将每个科室ID (包括0) 映射为一个高维向量
        self.dept_embedding = nn.Embedding(num_embeddings=num_depts, embedding_dim=embedding_dim)

        # 2. 槽位位置嵌入层 (Slot Positional Embedding)
        # 为每个槽位（位置）学习一个唯一的嵌入向量，以区分位置信息
        self.slot_position_embedding = nn.Embedding(num_embeddings=num_slots, embedding_dim=embedding_dim)

        # 4. Transformer 编码器层
        # 这是网络的核心，用于处理序列信息
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=embedding_dim,
            nhead=self.config.TRANSFORMER_HEADS,
            dim_feedforward=embedding_dim * 4,
            dropout=0.1,
            activation='gelu',
            batch_first=True  # 确保输入张量的维度顺序为 (batch, sequence, features)
        )
        self.transformer_encoder = nn.TransformerEncoder(
            encoder_layer, 
            num_layers=self.config.TRANSFORMER_LAYERS
        )

        # 5. 输出线性层 (Output Layer)
        # 将处理后的特征与当前待决策槽位的特征拼接，然后映射到最终的特征维度
        self.linear = nn.Sequential(
            nn.LayerNorm(embedding_dim * num_slots + embedding_dim),
            nn.Linear(embedding_dim * num_slots + embedding_dim, features_dim),
            nn.ReLU()
        )

        logger.info(f"LayoutTransformer 初始化成功。")
        logger.info(f"  - 槽位数量: {num_slots}")
        logger.info(f"  - 科室种类数量: {num_depts}")
        logger.info(f"  - 嵌入维度: {embedding_dim}")

    def forward(self, observations: Dict[str, torch.Tensor]) -> torch.Tensor:
        """
        定义模型的前向传播逻辑。

        Args:
            observations (Dict[str, torch.Tensor]): 从环境中获得的观测数据字典。
                - "layout": (batch_size, num_slots) 当前布局，值为科室ID
                - "current_slot_idx": (batch_size, 1) 当前待填充的槽位索引

        Returns:
            torch.Tensor: 提取出的状态特征，维度为 (batch_size, features_dim)。
        """
        # --- 1. 准备输入嵌入 ---
        layout_ids = observations["layout"]
        current_slot_idx = observations["current_slot_idx"].squeeze(-1)

        # 确保数据类型正确：嵌入层需要整数类型
        layout_ids = layout_ids.long()  # 转换为 LongTensor
        current_slot_idx = current_slot_idx.long()  # 转换为 LongTensor

        batch_size, num_slots = layout_ids.shape
        device = layout_ids.device

        # 获取科室嵌入
        dept_embeds = self.dept_embedding(layout_ids) # (B, num_slots, D_emb)

        # 获取槽位位置嵌入
        slot_positions = torch.arange(0, num_slots, device=device).unsqueeze(0).expand(batch_size, -1)
        slot_pos_embeds = self.slot_position_embedding(slot_positions) # (B, num_slots, D_emb)

        # 组合输入嵌入 (这是Transformer的标准做法)
        input_embeds = dept_embeds + slot_pos_embeds

        # --- 2. 通过 Transformer 处理布局信息 ---
        # Transformer的输出包含了每个槽位位置的上下文感知特征
        transformer_output = self.transformer_encoder(input_embeds) # (B, num_slots, D_emb)

        # 将所有槽位的特征展平，形成一个代表整个布局的向量
        flattened_layout_features = transformer_output.reshape(batch_size, -1) # (B, num_slots * D_emb)

        # --- 3. 整合当前待决策槽位的信息 ---
        # 获取当前待决策槽位的嵌入
        current_slot_embed = self.slot_position_embedding(current_slot_idx) # (B, D_emb)

        # --- 4. 拼接并输出最终特征 ---
        # 将布局特征和当前科室特征拼接在一起
        combined_features = torch.cat([flattened_layout_features, current_slot_embed], dim=1)
        
        # 通过最后的线性层得到最终的特征向量
        final_features = self.linear(combined_features)
        
        return final_features
</file>

<file path="src/algorithms/base_optimizer.py">
"""
基础优化器抽象类 - 定义所有优化算法的统一接口
"""

from abc import ABC, abstractmethod
from typing import List, Dict, Any, Tuple, Optional
import time
from src.rl_optimizer.utils.setup import setup_logger
from dataclasses import dataclass

from src.rl_optimizer.env.cost_calculator import CostCalculator

@dataclass
class OptimizationResult:
    """优化结果数据类"""
    algorithm_name: str
    best_layout: List[str]
    best_cost: float
    execution_time: float
    iterations: int
    convergence_history: List[float]
    additional_metrics: Dict[str, Any]
    original_layout: List[str] = None  # 原始布局（未经优化）
    original_cost: float = None  # 原始布局的成本


class BaseOptimizer(ABC):
    """
    优化算法基类
    
    所有优化算法（PPO、模拟退火、遗传算法）都应继承此类，
    确保使用统一的接口和相同的目标函数（CostCalculator）。
    """
    
    def __init__(self, 
                 cost_calculator: CostCalculator,
                 constraint_manager: 'ConstraintManager',
                 name: str = "BaseOptimizer"):
        """
        初始化基础优化器
        
        Args:
            cost_calculator: 统一的成本计算器
            constraint_manager: 约束管理器
            name: 算法名称
        """
        self.cost_calculator = cost_calculator
        self.constraint_manager = constraint_manager
        self.name = name
        self.logger = setup_logger(__name__)
        
        # 优化过程跟踪
        self.current_iteration = 0
        self.best_cost = float('inf')
        self.best_layout = None
        self.convergence_history = []
        self.start_time = None
        
        # 原始布局信息
        self.original_layout = None
        self.original_cost = None
        
    @abstractmethod
    def optimize(self, 
                 initial_layout: Optional[List[str]] = None,
                 max_iterations: int = 1000,
                 original_layout: Optional[List[str]] = None,
                 original_cost: Optional[float] = None,
                 **kwargs) -> OptimizationResult:
        """
        执行优化算法
        
        Args:
            initial_layout: 初始布局，如果为None则生成随机布局
            max_iterations: 最大迭代次数
            original_layout: 原始布局（未经优化的基准）
            original_cost: 原始布局的成本
            **kwargs: 算法特定参数
            
        Returns:
            OptimizationResult: 优化结果
        """
        pass
    
    def evaluate_layout(self, layout: List[str]) -> float:
        """
        评估布局的成本
        
        Args:
            layout: 待评估的布局
            
        Returns:
            float: 布局成本
        """
        if not self.constraint_manager.is_valid_layout(layout):
            return float('inf')
        
        return self.cost_calculator.calculate_total_cost(layout)
    
    def generate_initial_layout(self) -> List[str]:
        """
        生成符合约束的初始布局
        
        Returns:
            List[str]: 初始布局
        """
        return self.constraint_manager.generate_valid_layout()
    
    def update_best_solution(self, layout: List[str], cost: float):
        """
        更新最优解
        
        Args:
            layout: 候选布局
            cost: 布局成本
        """
        if cost < self.best_cost:
            self.best_cost = cost
            self.best_layout = layout.copy()
            self.logger.info(f"发现更优解: 成本 = {cost:.2f}, 迭代 = {self.current_iteration}")
    
    def start_optimization(self):
        """开始优化过程"""
        self.start_time = time.time()
        self.current_iteration = 0
        self.best_cost = float('inf')
        self.best_layout = None
        self.convergence_history = []
        self.logger.info(f"开始 {self.name} 优化")
    
    def finish_optimization(self) -> OptimizationResult:
        """
        结束优化过程并返回结果
        
        Returns:
            OptimizationResult: 优化结果
        """
        execution_time = time.time() - self.start_time if self.start_time else 0
        
        result = OptimizationResult(
            algorithm_name=self.name,
            best_layout=self.best_layout,
            best_cost=self.best_cost,
            execution_time=execution_time,
            iterations=self.current_iteration,
            convergence_history=self.convergence_history.copy(),
            additional_metrics=self.get_additional_metrics(),
            original_layout=self.original_layout,
            original_cost=self.original_cost
        )
        
        self.logger.info(f"{self.name} 优化完成:")
        self.logger.info(f"  最优成本: {self.best_cost:.2f}")
        if self.original_cost is not None:
            improvement = ((self.original_cost - self.best_cost) / self.original_cost) * 100
            self.logger.info(f"  原始成本: {self.original_cost:.2f}")
            self.logger.info(f"  改进率: {improvement:.1f}%")
        self.logger.info(f"  执行时间: {execution_time:.2f}s")
        self.logger.info(f"  迭代次数: {self.current_iteration}")
        
        return result
    
    def get_additional_metrics(self) -> Dict[str, Any]:
        """
        获取算法特定的额外指标
        
        Returns:
            Dict[str, Any]: 额外指标字典
        """
        return {}
    
    def log_iteration(self, cost: float, additional_info: str = ""):
        """
        记录迭代信息
        
        Args:
            cost: 当前成本
            additional_info: 额外信息
        """
        self.convergence_history.append(cost)
        
        if self.current_iteration % 100 == 0:  # 每100次迭代记录一次
            self.logger.debug(f"迭代 {self.current_iteration}: 成本 = {cost:.2f} {additional_info}")
</file>

<file path="src/core/network_generator.py">
"""
网络生成器 - 整合医院网络生成功能
"""

from src.rl_optimizer.utils.setup import setup_logger
import pathlib
from typing import Dict, List, Optional

from src.config import NetworkConfig, COLOR_MAP
from src.network.super_network import SuperNetwork
from src.plotting.plotter import PlotlyPlotter
from src.analysis.travel_time import calculate_room_travel_times

logger = setup_logger(__name__)


class NetworkGenerator:
    """
    网络生成器类
    
    整合原有的医院网络生成功能，从楼层平面图生成多层医院网络图，
    包括图像处理、网络构建和行程时间计算。
    """
    
    def __init__(self, config: Optional[NetworkConfig] = None):
        """
        初始化网络生成器
        
        Args:
            config: 网络配置，如果为None则使用默认配置
        """
        self.config = config if config is not None else NetworkConfig(color_map_data=COLOR_MAP)
        self.color_map_data = COLOR_MAP
        self.super_network = None
        self.super_graph = None
        
        logger.info("网络生成器初始化完成")
        logger.info(f"结果将保存在: {self.config.RESULT_PATH}")
        logger.info(f"调试图像将保存在: {self.config.DEBUG_PATH}")
    
    def generate_network(self, 
                        image_dir: str = "./data/label/",
                        base_floor: int = 0,
                        num_processes: Optional[int] = None) -> bool:
        """
        生成多层医院网络
        
        Args:
            image_dir: 楼层标注图像目录
            base_floor: 基准楼层
            num_processes: 并行处理进程数
            
        Returns:
            bool: 生成是否成功
        """
        logger.info("=== 开始生成多层医院网络 ===")
        
        # 检查图像目录
        label_dir = pathlib.Path(image_dir)
        if not label_dir.is_dir():
            logger.error(f"标注图像目录不存在: {label_dir}")
            return False
        
        # 收集图像文件
        image_file_paths = [
            str(p) for p in sorted(label_dir.glob('*.png')) if p.is_file()
        ]
        
        if not image_file_paths:
            logger.warning(f"在 {label_dir} 中未找到图像文件")
            return False
        
        logger.info(f"找到 {len(image_file_paths)} 个图像文件: {image_file_paths}")
        
        try:
            # 初始化SuperNetwork构建器
            self.super_network = SuperNetwork(
                config=self.config,
                color_map_data=self.color_map_data,
                base_floor=base_floor,
                num_processes=num_processes
            )
            
            # 生成SuperNetwork
            self.super_graph = self.super_network.run(
                image_file_paths=image_file_paths
            )
            
            logger.info(f"SuperNetwork生成成功:")
            logger.info(f"  节点数: {self.super_graph.number_of_nodes()}")
            logger.info(f"  边数: {self.super_graph.number_of_edges()}")
            logger.info(f"  图像尺寸: 宽度={self.super_network.width}, 高度={self.super_network.height}")
            
            return True
            
        except Exception as e:
            logger.error(f"生成网络时发生错误: {e}", exc_info=True)
            return False
    
    def visualize_network(self, output_filename: str = "hospital_network_3d.html") -> bool:
        """
        可视化生成的网络
        
        Args:
            output_filename: 输出文件名
            
        Returns:
            bool: 可视化是否成功
        """
        if self.super_graph is None or self.super_network is None:
            logger.error("未生成网络，无法进行可视化")
            return False
        
        try:
            logger.info("正在生成网络可视化...")
            
            plotter = PlotlyPlotter(
                config=self.config, 
                color_map_data=self.color_map_data
            )
            
            plot_output_path = self.config.RESULT_PATH / output_filename
            plotter.plot(
                graph=self.super_graph,
                output_path=plot_output_path,
                title="多层医院网络",
                graph_width=self.super_network.width,
                graph_height=self.super_network.height,
                floor_z_map=self.super_network.floor_z_map
            )
            
            logger.info(f"网络可视化已保存到: {plot_output_path}")
            return True
            
        except Exception as e:
            logger.error(f"网络可视化时发生错误: {e}", exc_info=True)
            return False
    
    def calculate_travel_times(self, output_filename: str = "hospital_travel_times.csv") -> bool:
        """
        计算并保存房间间行程时间
        
        Args:
            output_filename: 输出文件名
            
        Returns:
            bool: 计算是否成功
        """
        if self.super_graph is None or self.super_network is None:
            logger.error("未生成网络，无法计算行程时间")
            return False
        
        try:
            logger.info("正在计算房间间行程时间...")
            
            # 获取地面楼层Z值用于过滤
            ground_floor_z = self.super_network.designated_ground_floor_z
            if ground_floor_z is not None:
                logger.info(f"使用地面楼层 Z={ground_floor_z:.2f} 进行外部区域过滤")
            else:
                logger.warning("无法确定地面楼层Z值，外部区域过滤可能受影响")
            
            # 计算并保存行程时间
            travel_times_output_dir = self.config.RESULT_PATH
            calculate_room_travel_times(
                graph=self.super_graph,
                config=self.config,
                output_dir=travel_times_output_dir,
                output_filename=output_filename,
                ground_floor_z=ground_floor_z
            )
            
            travel_times_path = travel_times_output_dir / output_filename
            logger.info(f"行程时间矩阵已保存到: {travel_times_path}")
            return True
            
        except Exception as e:
            logger.error(f"计算行程时间时发生错误: {e}", exc_info=True)
            return False
    
    def get_network_info(self) -> Dict[str, any]:
        """
        获取网络信息
        
        Returns:
            Dict: 网络信息字典
        """
        if self.super_graph is None or self.super_network is None:
            return {}
        
        return {
            'nodes_count': self.super_graph.number_of_nodes(),
            'edges_count': self.super_graph.number_of_edges(),
            'width': self.super_network.width,
            'height': self.super_network.height,
            'floor_z_map': self.super_network.floor_z_map,
            'ground_floor_z': self.super_network.designated_ground_floor_z
        }
    
    def run_complete_generation(self, 
                               image_dir: str = "./data/label/",
                               visualization_filename: str = "hospital_network_3d.html",
                               travel_times_filename: str = "hospital_travel_times.csv") -> bool:
        """
        运行完整的网络生成流程
        
        Args:
            image_dir: 楼层标注图像目录
            visualization_filename: 可视化输出文件名
            travel_times_filename: 行程时间输出文件名
            
        Returns:
            bool: 完整流程是否成功
        """
        logger.info("=== 开始完整网络生成流程 ===")
        
        # 1. 生成网络
        if not self.generate_network(image_dir):
            logger.error("网络生成失败，中止流程")
            return False
        
        # 2. 可视化网络
        if not self.visualize_network(visualization_filename):
            logger.error("网络可视化失败，但继续后续步骤")
        
        # 3. 计算行程时间
        if not self.calculate_travel_times(travel_times_filename):
            logger.error("行程时间计算失败，但网络生成成功")
            return False
        
        logger.info("=== 完整网络生成流程成功完成 ===")
        network_info = self.get_network_info()
        logger.info(f"网络统计信息: {network_info}")
        
        return True
</file>

<file path="src/image_processing/processor.py">
"""Handles image loading, preprocessing, and basic morphological operations."""

import cv2
from src.rl_optimizer.utils.setup import setup_logger
import numpy as np
from PIL import Image
from scipy.spatial import KDTree
from typing import Tuple, Dict, Any, Optional

from src.config import NetworkConfig

logger = setup_logger(__name__)


class ImageProcessor:
    """
    Provides functionalities for image loading, color quantization,
    and morphological operations.
    """

    def __init__(self, config: NetworkConfig, color_map_data: Dict[Tuple[int, int, int], Dict[str, Any]]):
        """
        Initializes the ImageProcessor.

        Args:
            config: The NetworkConfig object.
            color_map_data: The color map dictionary.
        """
        self.config = config
        self.color_map_data = color_map_data
        self._current_image_data: Optional[np.ndarray] = None
        self._image_height: Optional[int] = None
        self._image_width: Optional[int] = None

    def load_and_prepare_image(self, image_path: str) -> np.ndarray:
        """
        Loads an image, rotates it, and stores its dimensions.

        The processed image is stored internally and also returned.

        Args:
            image_path: Path to the image file.

        Returns:
            A NumPy array representing the processed image (RGB).

        Raises:
            FileNotFoundError: If the image_path does not exist.
            IOError: If the image cannot be opened or read.
        """
        try:
            img = Image.open(image_path).convert('RGB')  # Ensure RGB
            if self.config.IMAGE_ROTATE != 0:
                img = img.rotate(self.config.IMAGE_ROTATE)
        except FileNotFoundError:
            raise FileNotFoundError(f"Image file not found: {image_path}")
        except IOError:
            raise IOError(f"Could not open or read image file: {image_path}")

        self._current_image_data = np.asarray(img, dtype=np.uint8)
        if self._current_image_data is None:
            raise ValueError(
                f"Failed to convert image to numpy array: {image_path}")

        self._image_height, self._image_width = self._current_image_data.shape[:2]
        return self._current_image_data.copy()  # 返回副本避免意外修改

    def get_image_dimensions(self) -> Tuple[int, int]:
        """
        Returns the dimensions of the last loaded image.

        Returns:
            A tuple (height, width).

        Raises:
            ValueError: If no image has been loaded yet.
        """
        if self._image_height is None or self._image_width is None:
            raise ValueError(
                "Image not loaded. Call load_and_prepare_image() first.")
        return self._image_height, self._image_width

    def quantize_colors(self, image_data: np.ndarray) -> np.ndarray:
        """
        Replaces each pixel's color in the image with the nearest color
        from the provided color_map using a KDTree for efficiency.

        Args:
            image_data: A NumPy array representing the image (H, W, 3) in RGB.

        Returns:
            A NumPy array of the same shape with colors replaced.
        """
        if not self.color_map_data:
            # 如果颜色映射为空，返回原始图像以避免错误
            logger.warning(
                "Warning: Color map is empty. Returning original image from quantize_colors.")
            return image_data.copy()

        pixels = image_data.reshape(-1, 3)
        map_colors_rgb = list(self.color_map_data.keys())

        if not map_colors_rgb:  # 如果self.color_map_data不为空，这种情况不应该发生
            logger.warning(
                "Warning: No colors in color_map_data keys. Returning original image.")
            return image_data.copy()

        kdtree = KDTree(map_colors_rgb)
        _, closest_indices = kdtree.query(pixels)

        new_pixels = np.array(map_colors_rgb, dtype=np.uint8)[closest_indices]
        new_image = new_pixels.reshape(image_data.shape).astype(np.uint8)
        return new_image

    def apply_morphology(self, mask: np.ndarray, operation: str = 'close_open',
                         kernel_size: Optional[Tuple[int, int]] = None) -> np.ndarray:
        """
        Applies morphological operations to a binary mask.

        Args:
            mask: The input binary mask (NumPy array, dtype=uint8).
            operation: The type of operation.
                       'close_open': MORPH_CLOSE then MORPH_OPEN (default)
                       'open': MORPH_OPEN
                       'close': MORPH_CLOSE
                       'dilate': MORPH_DILATE
                       'erode': MORPH_ERODE
            kernel_size: Tuple (k_height, k_width) for the morphological kernel.
                         Defaults to `config.MORPHOLOGY_KERNEL_SIZE`.

        Returns:
            The processed binary mask.
        """
        if kernel_size is None:
            k_size = self.config.MORPHOLOGY_KERNEL_SIZE
        else:
            k_size = kernel_size

        kernel = np.ones(k_size, np.uint8)
        processed_mask = mask.copy()

        if operation == 'close_open':
            processed_mask = cv2.morphologyEx(
                processed_mask, cv2.MORPH_CLOSE, kernel)
            processed_mask = cv2.morphologyEx(
                processed_mask, cv2.MORPH_OPEN, kernel)
        elif operation == 'open':
            processed_mask = cv2.morphologyEx(
                processed_mask, cv2.MORPH_OPEN, kernel)
        elif operation == 'close':
            processed_mask = cv2.morphologyEx(
                processed_mask, cv2.MORPH_CLOSE, kernel)
        elif operation == 'dilate':
            processed_mask = cv2.dilate(processed_mask, kernel, iterations=1)
        elif operation == 'erode':
            processed_mask = cv2.erode(processed_mask, kernel, iterations=1)
        else:
            raise ValueError(
                f"Unsupported morphological operation: {operation}")

        return processed_mask
    

class DebugImage:
    """Helper class for saving and displaying debug images."""
    count = 0

    def __init__(self, image_data: np.ndarray, save: bool = False,
                 show_napari: bool = False, suffix: str = '',
                 config: NetworkConfig = NetworkConfig()):
        """
        Initializes DebugImage.

        Args:
            image_data: NumPy array of the image to debug.
            save: If True, saves the image.
            show_napari: If True, shows the image using napari (requires napari installed).
            suffix: Suffix for the saved filename.
            debug_path_base: Base directory for saving debug images.
        """
        self.image_to_debug = image_data.copy()  # 使用副本工作
        self.debug_path = config.DEBUG_PATH
        self.debug_path.mkdir(parents=True, exist_ok=True)

        if save:
            self._save_image(suffix)
        if show_napari:
            self._show_with_napari()

    def _save_image(self, suffix: str = ''):
        """Saves the debug image."""
        filename = f'debug_{DebugImage.count}_{suffix}.png'
        save_path = self.debug_path / filename
        try:
            # 如果是RGB格式，转换为BGR以供Pillow保存，或处理灰度图
            if self.image_to_debug.ndim == 3 and self.image_to_debug.shape[2] == 3:
                # Assume RGB from PIL, convert to BGR for OpenCV-style saving or save as is with PIL
                img_to_save_pil = Image.fromarray(self.image_to_debug)
            elif self.image_to_debug.ndim == 2:  # 灰度图像
                 img_to_save_pil = Image.fromarray(self.image_to_debug, mode='L')
            else:
                logger.warning(f"Warning: Unsupported image format for saving: {self.image_to_debug.shape}")
                return

            img_to_save_pil.save(save_path)
            DebugImage.count += 1
            logger.info(f"Debug image saved to {save_path}")
        except Exception as e:
            logger.error(f"Error saving debug image {save_path}: {e}")


    def _show_with_napari(self):
        """Shows the image using napari."""
        try:
            import napari
            viewer = napari.Viewer()
            viewer.add_image(self.image_to_debug)
            napari.run()
        except ImportError:
            logger.warning("Napari is not installed. Skipping napari display.")
        except Exception as e:
            logger.error(f"Error showing image with napari: {e}")
</file>

<file path="src/network/floor_manager.py">
"""
Manages floor detection from filenames and Z-level calculations for SuperNetwork.
"""

import os
import re
import pathlib
from typing import List, Dict, Tuple, Optional
from src.rl_optimizer.utils.setup import setup_logger

logger = setup_logger(__name__)

class FloorManager:
    """
    Handles detection of floor numbers from image filenames and calculation
    of their corresponding Z-coordinate levels.
    """

    def __init__(self, base_floor_default: int = 0, default_floor_height: float = 10.0):
        """
        Initializes the FloorManager.

        Args:
            base_floor_default: The default starting floor number if none can be detected.
            default_floor_height: The default height difference between adjacent floors.
        """
        self.base_floor_default = base_floor_default
        self.default_floor_height = default_floor_height
        self._floor_patterns = {
            # Order matters: more specific patterns first
            # B-1, B1, b-1, b1 -> negative
            r'([Bb])-?(\d+)': lambda m: -int(m.group(2)),
            # -1F, -2f -> negative
            r'-(\d+)[Ff]': lambda m: -int(m.group(1)),
            r'([Ll])(\d+)': lambda m: int(m.group(2)),    # L1, l2 -> positive
            r'(\d+)[Ff]': lambda m: int(m.group(1)),      # 1F, 2f -> positive
            # Add more general patterns if needed, like just a number if no prefix/suffix
            # r'_(\d+)_': lambda m: int(m.group(1)) # Example: floor_1_plan.png
        }

    def detect_floor_from_filename(self, file_path: pathlib.Path) -> Optional[int]:
        """
        Detects the floor number from a filename.

        Args:
            file_path: Path object of the image file.

        Returns:
            The detected floor number (integer) or None if not detected.
        """
        filename = file_path.stem  # Get filename without extension
        for pattern, converter in self._floor_patterns.items():
            match = re.search(pattern, filename)
            if match:
                try:
                    return converter(match)
                except ValueError:
                    continue  # Conversion failed, try next pattern
        return None

    def auto_assign_floors(self, image_paths: List[pathlib.Path]) \
            -> Tuple[Dict[pathlib.Path, int], Dict[int, pathlib.Path]]:
        """
        Assigns floor numbers to a list of image paths.

        Attempts to detect from filenames first. If unsuccessful for some or all,
        assigns sequentially based on sort order or a defined starting floor.

        Args:
            image_paths: A list of Path objects for the images.

        Returns:
            A tuple containing:
                - path_to_floor_map (Dict[pathlib.Path, int]): Maps image path to floor number.
                - floor_to_path_map (Dict[int, pathlib.Path]): Maps floor number to image path.
                                                               (Assumes one image per floor for this map)
        """
        path_to_floor_map: Dict[pathlib.Path, int] = {}
        detected_floors: Dict[pathlib.Path, int] = {}
        undetected_paths: List[pathlib.Path] = []

        for p_path in image_paths:
            floor = self.detect_floor_from_filename(p_path)
            if floor is not None:
                if floor in path_to_floor_map.values():
                    # Handle duplicate floor detection if necessary (e.g., error or rename)
                    # For now, we might overwrite or just take the first one.
                    # Let's assume for now floor numbers detected are unique or take the first.
                    # A more robust solution would collect all paths per floor number.
                    logger.warning(
                        f"Warning: Duplicate floor number {floor} detected. Check filenames.")
                detected_floors[p_path] = floor
            else:
                undetected_paths.append(p_path)

        path_to_floor_map.update(detected_floors)

        # Assign floors to undetected paths sequentially
        if undetected_paths:
            # Sort undetected paths to ensure consistent assignment order
            # (e.g., alphabetically or by modification time if relevant)
            undetected_paths.sort()

            # Determine starting floor for sequential assignment
            if detected_floors:
                # Start from one above the highest detected floor, or one below the lowest if all are negative
                all_detected_nos = list(detected_floors.values())
                if all(f < 0 for f in all_detected_nos):  # if all are basement floors
                    start_floor = min(all_detected_nos) - 1 if min(all_detected_nos) - \
                        1 not in all_detected_nos else max(all_detected_nos) + 1
                else:
                    start_floor = max(all_detected_nos) + 1

                # Ensure start_floor is not already taken
                while start_floor in path_to_floor_map.values():
                    start_floor += 1  # simple increment, could be smarter
            else:
                start_floor = self.base_floor_default

            for i, p_path in enumerate(undetected_paths):
                current_assigned_floor = start_floor + i
                while current_assigned_floor in path_to_floor_map.values():  # Avoid collision
                    current_assigned_floor += 1
                path_to_floor_map[p_path] = current_assigned_floor

        # Create the reverse map (floor_to_path_map)
        # This assumes one unique image per floor for this specific map.
        # If multiple images could correspond to the same floor, this needs adjustment.
        floor_to_path_map: Dict[int, pathlib.Path] = {
            v: k for k, v in path_to_floor_map.items()}

        # Verify uniqueness for floor_to_path_map
        if len(floor_to_path_map) != len(path_to_floor_map):
            logger.warning("Non-unique floor numbers assigned or detected, "
                  "floor_to_path_map may not represent all images.")
            # Potentially rebuild floor_to_path_map to store List[pathlib.Path] per floor
            # For now, this structure is kept simple as per original design.

        return path_to_floor_map, floor_to_path_map

    def calculate_z_levels(self, floor_to_path_map: Dict[int, pathlib.Path]) \
            -> Dict[int, float]:
        """
        Calculates the Z-coordinate for each floor.

        Args:
            floor_to_path_map: A map from floor number to image path (used to get sorted floors).

        Returns:
            A dictionary mapping floor number to its Z-coordinate.
        """
        if not floor_to_path_map:
            return {}

        sorted_floor_numbers = sorted(floor_to_path_map.keys())
        
        revision_num = 0
        
        if 0 not in sorted_floor_numbers:
            revision_num = 1


        # Simple Z level calculation: floor_number * default_floor_height
        # This assumes a consistent floor height and that floor numbers represent relative positions.
        # For example, Floor 0 is at Z=0, Floor 1 at Z=10, Floor -1 at Z=-10.
        z_levels: Dict[int, float] = {
            floor_num: float((floor_num - revision_num) * self.default_floor_height if floor_num > 0 else floor_num * self.default_floor_height)
            for floor_num in sorted_floor_numbers
        }
        return z_levels
</file>

<file path="src/rl_optimizer/data/cache/node_variants.json">
{
  "ICU": [
    "ICU_30013"
  ],
  "NICU": [
    "NICU_30012"
  ],
  "中医科": [
    "中医科_10007"
  ],
  "中心供应室": [
    "中心供应室_10003"
  ],
  "产前诊断门诊": [
    "产前诊断门诊_30009"
  ],
  "产房": [
    "产房_30007"
  ],
  "产科": [
    "产科_30006"
  ],
  "介入科": [
    "介入科_20013"
  ],
  "体检科": [
    "体检科_10009"
  ],
  "儿科": [
    "儿科_10006"
  ],
  "全科": [
    "全科_10004"
  ],
  "内诊药房": [
    "内诊药房_10001"
  ],
  "内镜中心": [
    "内镜中心_20002"
  ],
  "口腔一区": [
    "口腔一区_40004"
  ],
  "口腔科二区": [
    "口腔科二区_40008"
  ],
  "呼吸内科": [
    "呼吸内科_20011"
  ],
  "妇科": [
    "妇科_30005"
  ],
  "心血管内科": [
    "心血管内科_20006"
  ],
  "急诊科": [
    "急诊科_1"
  ],
  "手术室": [
    "手术室_40006"
  ],
  "挂号收费": [
    "挂号收费_10002",
    "挂号收费_20001",
    "挂号收费_30001",
    "挂号收费_40001"
  ],
  "放射科": [
    "放射科_10005"
  ],
  "检验中心": [
    "检验中心_20003"
  ],
  "泌尿外科": [
    "泌尿外科_30003"
  ],
  "消化内科": [
    "消化内科_20004"
  ],
  "烧伤整形科": [
    "烧伤整形科_30011"
  ],
  "生殖医学科": [
    "生殖医学科_30010"
  ],
  "甲状腺外科": [
    "甲状腺外科_30002"
  ],
  "病理科": [
    "病理科_20008"
  ],
  "皮肤科": [
    "皮肤科_40005"
  ],
  "眼科": [
    "眼科_40002"
  ],
  "神经内科": [
    "神经内科_20010"
  ],
  "综合激光科": [
    "综合激光科_40009"
  ],
  "耳鼻喉科": [
    "耳鼻喉科_40003"
  ],
  "肝胆胰外科": [
    "肝胆胰外科_30004"
  ],
  "肾内科": [
    "肾内科_20005"
  ],
  "肿瘤科": [
    "肿瘤科_20012"
  ],
  "超声科": [
    "超声科_10008"
  ],
  "透析中心": [
    "透析中心_30008"
  ],
  "采血处": [
    "采血处_20007"
  ],
  "门": [
    "门_11072",
    "门_11083",
    "门_11086",
    "门_11087",
    "门_11093",
    "门_11094",
    "门_11112",
    "门_11113",
    "门_11116",
    "门_11118",
    "门_11119",
    "门_11122",
    "门_11145",
    "门_11146",
    "门_11147",
    "门_11149",
    "门_11153",
    "门_11154",
    "门_11155",
    "门_11165"
  ],
  "门诊手术室": [
    "门诊手术室_40007"
  ],
  "静配中心": [
    "静配中心_40010"
  ],
  "骨科": [
    "骨科_20009"
  ]
}
</file>

<file path="src/rl_optimizer/data/cache_manager.py">
import pandas as pd
from pathlib import Path
from typing import Dict, List, Any
from collections import defaultdict
import itertools

from src.config import RLConfig
from src.rl_optimizer.utils.setup import setup_logger, save_json, load_json, save_pickle, load_pickle

logger = setup_logger(__name__)

class CacheManager:

    def __init__(self, config: RLConfig):
        """
        初始化缓存管理器，并处理所有数据的加载和预计算。

        Args:
            config (RLConfig): RL优化器的配置对象。
        """
        self.config = config

        # --- 1. 加载和处理基础数据 ---
        self.raw_travel_times_df = self._load_raw_travel_times()
        self.node_data = self._load_and_process_node_data(self.raw_travel_times_df)
        self.travel_times_matrix = self.raw_travel_times_df.drop('面积', errors='ignore')

        # --- 2. 节点分类与封装 ---
        # 核心逻辑：所有节点分类都在此处完成并封装
        fixed_mask = self.node_data['generic_name'].isin(config.FIXED_NODE_TYPES)
        self.all_nodes_list: List[str] = self.node_data['node_id'].tolist()
        self.placeable_nodes_df: pd.DataFrame = self.node_data[~fixed_mask].copy()
        self.fixed_nodes_df: pd.DataFrame = self.node_data[fixed_mask].copy()

        # 公开给外部使用的、清晰的属性
        self.placeable_departments: List[str] = self.placeable_nodes_df['node_id'].tolist()
        self.placeable_slots: List[str] = self.placeable_nodes_df['node_id'].tolist()

        # --- 3. 解析流程与流线 ---
        self.variants = self.get_node_variants()
        self.traffic = self.get_traffic_distribution()
        self.resolved_pathways = self.get_resolved_pathways()

    def _load_raw_travel_times(self) -> pd.DataFrame:
        """加载原始CSV文件。"""
        if not self.config.TRAVEL_TIMES_CSV.exists():
            logger.error(f"致命错误: 原始通行时间文件未找到: {self.config.TRAVEL_TIMES_CSV}")
            raise FileNotFoundError(f"原始通行时间文件未找到: {self.config.TRAVEL_TIMES_CSV}")
        return pd.read_csv(self.config.TRAVEL_TIMES_CSV, index_col=0)

    def _load_and_process_node_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """从已加载的DataFrame中解析节点和面积。"""
        logger.info("正在处理节点数据...")
        node_ids = df.columns.tolist()
        
        if '面积' not in df.index:
            raise ValueError("CSV文件中必须包含名为'面积'的最后一行。")
        area_series = df.loc['面积']

        if not all(node_id in area_series.index for node_id in node_ids):
             raise ValueError("CSV文件的列标题与'面积'行的索引不完全匹配。请检查文件格式。")

        nodes_list = [{'node_id': node_id, 'area': area_series[node_id]} for node_id in node_ids]
        all_nodes_df = pd.DataFrame(nodes_list)
        all_nodes_df['generic_name'] = all_nodes_df['node_id'].apply(lambda x: str(x).split('_')[0])
        
        logger.info(f"成功处理了 {len(all_nodes_df)} 个唯一节点及其面积。")
        return all_nodes_df

    def get_node_variants(self) -> Dict[str, List[str]]:
        """
        获取节点变体映射。如果缓存存在则加载，否则从节点数据生成。
        """
        if self.config.NODE_VARIANTS_JSON.exists():
            logger.info(f"从缓存加载节点变体: {self.config.NODE_VARIANTS_JSON}")
            return load_json(self.config.NODE_VARIANTS_JSON)
        
        logger.info("缓存未找到，正在生成节点变体...")
        variants = self.node_data.groupby('generic_name')['node_id'].apply(list).to_dict()
        save_json(variants, self.config.NODE_VARIANTS_JSON)
        logger.info(f"节点变体已生成并缓存至: {self.config.NODE_VARIANTS_JSON}")
        return variants
    
    def get_traffic_distribution(self) -> Dict[str, Dict[str, float]]:
        """
        获取流量分布。如果缓存存在则加载，否则基于变体生成初始权重。
        """
        if self.config.TRAFFIC_DISTRIBUTION_JSON.exists():
            logger.info(f"从缓存加载流量分布: {self.config.TRAFFIC_DISTRIBUTION_JSON}")
            return load_json(self.config.TRAFFIC_DISTRIBUTION_JSON)
            
        logger.info("缓存未找到，正在生成初始流量分布...")
        traffic_distribution = {
            generic_name: {node: 1.0 for node in specific_nodes}
            for generic_name, specific_nodes in self.variants.items()
        }
        save_json(traffic_distribution, self.config.TRAFFIC_DISTRIBUTION_JSON)
        logger.info(f"初始流量分布已生成并缓存至: {self.config.TRAFFIC_DISTRIBUTION_JSON}")
        return traffic_distribution
    
    def get_resolved_pathways(self) -> List[Dict[str, Any]]:
        """
        获取最终解析出的流线列表。这是调度核心，如果缓存不存在则触发解析。
        """
        if self.config.RESOLVED_PATHWAYS_PKL.exists():
            logger.info(f"从缓存加载已解析的流线: {self.config.RESOLVED_PATHWAYS_PKL}")
            return load_pickle(self.config.RESOLVED_PATHWAYS_PKL)

        logger.info("缓存未找到，正在解析就医流程以生成具体流线...")
        
        try:
            templates = load_json(self.config.PROCESS_TEMPLATES_JSON)
        except FileNotFoundError:
            logger.error(f"致命错误: 就医流程模板文件未找到: {self.config.PROCESS_TEMPLATES_JSON}")
            raise
        
        resolved_pathways = []
        for template in templates:
            resolved_pathways.extend(self._resolve_single_template(template))
        
        save_pickle(resolved_pathways, self.config.RESOLVED_PATHWAYS_PKL)
        logger.info(f"所有流线已解析并缓存 ({len(resolved_pathways)}条): {self.config.RESOLVED_PATHWAYS_PKL}")
        return resolved_pathways
    
    def _resolve_single_template(self, template: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        根据单个流程模板，高效地解析出所有可能的具体流线及其权重。
        改进版：使用生成器和批处理来避免内存爆炸。
        
        Args:
            template (Dict[str, Any]): 单个就医流程的模板字典。

        Returns:
            List[Dict[str, Any]]: 一个包含流线字典的列表，每个字典包含process_id、path和weight字段。
        """
        base_weight = template.get('base_weight', 1.0)
        max_combinations = getattr(self.config, 'MAX_PATHWAY_COMBINATIONS', 10000)

        # 1. 准备所有部分的选项列表
        start_nodes_options = [self.variants.get(gn, [gn]) for gn in template['start_nodes']]
        core_sequences_options = [self.variants.get(gn, [gn]) for gn in template['core_sequence']]
        end_nodes_options = [self.variants.get(gn, [gn]) for gn in template['end_nodes']]

        # 2. 计算总组合数，如果太大则采样
        total_combinations = 1
        for options in start_nodes_options + core_sequences_options + end_nodes_options:
            total_combinations *= len(options)
        
        # 如果组合数太大，使用采样策略
        if total_combinations > max_combinations:
            logger.warning(f"流程 {template['process_id']} 的组合数 {total_combinations} 超过限制 {max_combinations}，将进行采样")
            return self._resolve_template_sampled(template, max_combinations)
        
        # 3. 使用生成器来减少内存占用
        pathways = []
        start_combinations = itertools.product(*start_nodes_options)
        
        for start_combo in start_combinations:
            core_combinations = itertools.product(*core_sequences_options)
            for core_combo in core_combinations:
                end_combinations = itertools.product(*end_nodes_options)
                for end_combo in end_combinations:
                    
                    # --- 权重计算开始 ---
                    final_weight = base_weight
                    
                    # a. 计算起点的权重 (总是计算)
                    for node in start_combo:
                        final_weight *= self._get_normalized_weight(node)
                        
                    # b. 计算核心路径的权重 (每个通用名只计算一次)
                    processed_core_generics = set()
                    for node in core_combo:
                        generic_name = str(node).split('_')[0]
                        if generic_name not in processed_core_generics:
                            final_weight *= self._get_normalized_weight(node)
                            processed_core_generics.add(generic_name)

                    # c. 计算终点的权重 (总是计算)
                    for node in end_combo:
                        final_weight *= self._get_normalized_weight(node)

                    # --- 权重计算结束 ---

                    # 拼接成完整流线
                    full_path = list(start_combo) + list(core_combo) + list(end_combo)
                    pathways.append({
                        "process_id": template['process_id'],
                        "path": full_path,
                        "weight": final_weight
                    })
            
        return pathways
    
    def _resolve_template_sampled(self, template: Dict[str, Any], max_samples: int) -> List[Dict[str, Any]]:
        """
        使用采样策略解析模板，避免组合爆炸。
        """
        import random
        
        base_weight = template.get('base_weight', 1.0)
        pathways = []
        
        # 准备选项
        start_nodes_options = [self.variants.get(gn, [gn]) for gn in template['start_nodes']]
        core_sequences_options = [self.variants.get(gn, [gn]) for gn in template['core_sequence']]
        end_nodes_options = [self.variants.get(gn, [gn]) for gn in template['end_nodes']]
        
        # 随机采样
        for _ in range(max_samples):
            start_combo = tuple(random.choice(opts) for opts in start_nodes_options)
            core_combo = tuple(random.choice(opts) for opts in core_sequences_options)
            end_combo = tuple(random.choice(opts) for opts in end_nodes_options)
            
            # 计算权重
            final_weight = base_weight
            for node in start_combo:
                final_weight *= self._get_normalized_weight(node)
            
            processed_core_generics = set()
            for node in core_combo:
                generic_name = str(node).split('_')[0]
                if generic_name not in processed_core_generics:
                    final_weight *= self._get_normalized_weight(node)
                    processed_core_generics.add(generic_name)
            
            for node in end_combo:
                final_weight *= self._get_normalized_weight(node)
            
            # 组合路径
            full_path = list(start_combo) + list(core_combo) + list(end_combo)
            pathways.append({
                "process_id": template['process_id'],
                "path": full_path,
                "weight": final_weight
            })
        
        return pathways
    
    def _get_normalized_weight(self, node_name: str) -> float:
        """
        计算单个节点的归一化流量权重。
        如果一个通用名下只有一个变体，其权重因子为1.0。
        """
        generic_name = str(node_name).split('_')[0]
        # 使用 self.traffic，它是在 __init__ 中加载或生成的
        distribution_map = self.traffic.get(generic_name)
        
        # 如果找不到分布或只有一个变体，则权重因子为1
        if not distribution_map or len(distribution_map) <= 1:
            return 1.0

        total_weight = sum(distribution_map.values())
        if total_weight == 0:
            logger.warning(f"通用名 '{generic_name}' 的总权重为0，无法进行归一化。")
            return 0.0

        raw_weight = distribution_map.get(node_name, 0.0)
        return raw_weight / total_weight
</file>

<file path="src/rl_optimizer/data/process_templates.json">
[
    {
        "process_id": "GENERAL_PRACTICE",
        "description": "全科流程",
        "core_sequence": ["全科", "采血处", "心血管内科", "全科", "药房"],
        "start_nodes": ["门"],
        "end_nodes": ["门"],
        "base_weight": 1.0
    },
    {
        "process_id": "MEDICAL_CHECKUP_PROCESS",
        "description": "体检流程",
        "core_sequence": ["体检科", "采血处", "检验中心", "放射科", "心血管内科", "超声科", "体检科"],
        "start_nodes": ["门"],
        "end_nodes": ["门"],
        "base_weight": 1.0
    },
    {
        "process_id": "PEDIATRIC_PROCESS",
        "description": "儿科",
        "core_sequence": ["儿科", "药房"],
        "start_nodes": ["门"],
        "end_nodes": ["门"],
        "base_weight": 1.0
    },
    {
        "process_id": "TRADITIONAL_CHINESE_MEDICINE_PROCESS",
        "description": "中医科",
        "core_sequence": ["中医科", "药房"],
        "start_nodes": ["门"],
        "end_nodes": ["门"],
        "base_weight": 1.0
    },
    {
        "process_id": "RESPIRATORY_PROCESS",
        "description": "呼吸科",
        "core_sequence": ["呼吸科", "采血处", "放射科", "呼吸科", "药房"],
        "start_nodes": ["门"],
        "end_nodes": ["门"],
        "base_weight": 1.0
    },
    {
        "process_id": "KIDNEY_PROCESS",
        "description": "肾内科",
        "core_sequence": ["肾内科", "采血处", "检验中心", "超声科", "肾内科", "药房"],
        "start_nodes": ["门"],
        "end_nodes": ["门"],
        "base_weight": 1.0
    },
    {
        "process_id": "KIDNEY_DIALYSIS_PROCESS",
        "description": "肾透析",
        "core_sequence": ["肾内科", "透析中心", "药房"],
        "start_nodes": ["门"],
        "end_nodes": ["门"],
        "base_weight": 1.0
    },
    {
        "process_id": "ORTHOPEDIC_PROCESS",
        "description": "骨科",
        "core_sequence": ["骨科", "采血处", "放射科", "骨科", "药房"],
        "start_nodes": ["门"],
        "end_nodes": ["门"],
        "base_weight": 1.0
    },
    {
        "process_id": "CARDIOVASCULAR_PROCESS",
        "description": "心血管内科",
        "core_sequence": ["心血管内科", "放射科", "心血管内科", "药房"],
        "start_nodes": ["门"],
        "end_nodes": ["门"],
        "base_weight": 1.0
    },
    {
        "process_id": "NEUROLOGY_PROCESS",
        "description": "神经科",
        "core_sequence": ["神经科", "采血处", "放射科", "神经科", "药房"],
        "start_nodes": ["门"],
        "end_nodes": ["门"],
        "base_weight": 1.0
    },
    {
        "process_id": "DIGESTIVE_PROCESS",
        "description": "消化内科",
        "core_sequence": ["消化内科", "采血处", "检验中心", "消化内科", "内镜中心", "药房"],
        "start_nodes": ["门"],
        "end_nodes": ["门"],
        "base_weight": 1.0
    },
    {
        "process_id": "ENDOSCOPY_PROCESS",
        "description": "内镜中心",
        "core_sequence": ["内镜中心", "消化内科", "药房"],
        "start_nodes": ["门"],
        "end_nodes": ["门"],
        "base_weight": 1.0
    },
    {
        "process_id": "ONCOLOGY_PROCESS",
        "description": "肿瘤科",
        "core_sequence": ["肿瘤科", "采血处", "放射科", "病理科", "肿瘤科", "药房"],
        "start_nodes": ["门"],
        "end_nodes": ["门"],
        "base_weight": 1.0
    },
    {
        "process_id": "UROLOGY_PROCESS",
        "description": "泌尿外科",
        "core_sequence": ["泌尿外科", "采血处", "检验中心", "超声科", "泌尿外科", "药房"],
        "start_nodes": ["门"],
        "end_nodes": ["门"],
        "base_weight": 1.0
    },
    {
        "process_id": "BURN_SURGERY_PROCESS",
        "description": "烧伤整形科",
        "core_sequence": ["烧伤整形科", "药房"],
        "start_nodes": ["门"],
        "end_nodes": ["门"],
        "base_weight": 1.0
    },
    {
        "process_id": "THYROID_SURGERY_PROCESS",
        "description": "甲状腺外科",
        "core_sequence": ["甲状腺外科", "采血处", "超声科", "甲状腺外科", "药房"],
        "start_nodes": ["门"],
        "end_nodes": ["门"],
        "base_weight": 1.0
    },
    {
        "process_id": "LIVER_BILIARY_PANCREAS_SURGERY_PROCESS",
        "description": "肝胆胰外科",
        "core_sequence": ["肝胆胰外科", "放射科", "肝胆胰外科", "内镜中心", "药房"],
        "start_nodes": ["门"],
        "end_nodes": ["门"],
        "base_weight": 1.0
    },
    {
        "process_id": "LIVER_BILIARY_PANCREAS_ENDOSCOPY_PROCESS",
        "description": "肝胆内镜",
        "core_sequence": ["内镜中心", "肝胆胰外科", "药房"],
        "start_nodes": ["门"],
        "end_nodes": ["门"],
        "base_weight": 1.0
    },
    {
        "process_id": "REPRODUCTIVE_MEDICINE_PROCESS",
        "description": "生殖医学科",
        "core_sequence": ["生殖医学科", "检验中心", "生殖医学科", "药房"],
        "start_nodes": ["门"],
        "end_nodes": ["门"],
        "base_weight": 1.0
    },
    {
        "process_id": "PRENATAL_DIAGNOSIS_PROCESS",
        "description": "产前诊断门诊",
        "core_sequence": ["产前诊断门诊", "检验中心", "超声科", "产前诊断门诊", "药房"],
        "start_nodes": ["门"],
        "end_nodes": ["门"],
        "base_weight": 1.0
    },
    {
        "process_id": "OBSTETRICS_PROCESS",
        "description": "产科",
        "core_sequence": ["产科", "采血处", "检验中心", "超声科", "产科", "药房"],
        "start_nodes": ["门"],
        "end_nodes": ["门"],
        "base_weight": 1.0
    },
    {
        "process_id": "GYNECOLOGY_PROCESS",
        "description": "妇科",
        "core_sequence": ["妇科", "采血处", "检验中心", "超声科", "妇科", "药房"],
        "start_nodes": ["门"],
        "end_nodes": ["门"],
        "base_weight": 1.0
    },
    {
        "process_id": "DERMATOLOGY_PROCESS",
        "description": "皮肤科",
        "core_sequence": ["皮肤科", "药房"],
        "start_nodes": ["门"],
        "end_nodes": ["门"],
        "base_weight": 1.0
    },
    {
        "process_id": "LASER_PROCESS",
        "description": "综合激光科",
        "core_sequence": ["综合激光科", "药房"],
        "start_nodes": ["门"],
        "end_nodes": ["门"],
        "base_weight": 1.0
    },
    {
        "process_id": "EYE_PROCESS",
        "description": "眼科",
        "core_sequence": ["眼科", "药房"],
        "start_nodes": ["门"],
        "end_nodes": ["门"],
        "base_weight": 1.0
    },
    {
        "process_id": "ENT_PROCESS",
        "description": "耳鼻喉科",
        "core_sequence": ["耳鼻喉科", "放射科", "耳鼻喉科", "药房"],
        "start_nodes": ["门"],
        "end_nodes": ["门"],
        "base_weight": 1.0
    },
    {
        "process_id": "ORAL_PROCESS_1",
        "description": "口腔一区",
        "core_sequence": ["口腔一区", "放射科", "口腔一区", "药房"],
        "start_nodes": ["门"],
        "end_nodes": ["门"],
        "base_weight": 1.0
    },
    {
        "process_id": "ORAL_PROCESS_2",
        "description": "口腔科二区",
        "core_sequence": ["口腔科二区", "放射科", "口腔科二区", "药房"],
        "start_nodes": ["门"],
        "end_nodes": ["门"],
        "base_weight": 1.0
    },
    {
        "process_id": "EMERGENCY_PROCESS",
        "description": "急诊科",
        "core_sequence": ["急诊科", "超声科", "放射科", "急诊科", "药房"],
        "start_nodes": ["门"],
        "end_nodes": ["门"],
        "base_weight": 1.0
    }
]
</file>

<file path="src/rl_optimizer/env/vec_env_wrapper.py">
# src/rl_optimizer/env/vec_env_wrapper.py

from typing import Any, Dict, List, Optional, Tuple
import numpy as np
from stable_baselines3.common.vec_env import VecEnvWrapper
from stable_baselines3.common.vec_env.base_vec_env import VecEnvStepReturn
import multiprocessing

from src.rl_optimizer.utils.setup import setup_logger

logger = setup_logger(__name__)


class EpisodeInfoVecEnvWrapper(VecEnvWrapper):
    """
    VecEnv包装器，确保episode信息能正确传递给回调函数。
    
    解决SB3中VecEnv环境episode信息传递的问题，特别是自定义info字典的传递。
    """
    
    def __init__(self, venv):
        """
        初始化包装器
        
        Args:
            venv: 要包装的VecEnv
        """
        super().__init__(venv)
        self.episode_rewards = np.zeros(self.num_envs)
        self.episode_lengths = np.zeros(self.num_envs, dtype=int)
        self.episode_count = 0
        
        # 检查是否为主进程，只有主进程才输出详细日志
        try:
            self.is_main_process = multiprocessing.current_process().name == 'MainProcess'
        except:
            self.is_main_process = True
        
    def step_wait(self) -> VecEnvStepReturn:
        """
        等待并处理环境步骤结果，确保episode信息正确传递
        """
        observations, rewards, dones, infos = self.venv.step_wait()
        
        # 更新episode统计
        self.episode_rewards += rewards
        self.episode_lengths += 1
        
        # 处理episode结束
        for i, (done, info) in enumerate(zip(dones, infos)):
            if done:
                self.episode_count += 1
                
                # 如果环境提供了episode信息，确保它能被正确传递
                if 'episode' in info:
                    episode_info = info['episode'].copy()
                    
                    # 添加标准的SB3 episode信息字段
                    episode_info['r'] = float(self.episode_rewards[i])
                    episode_info['l'] = int(self.episode_lengths[i])
                    
                    # 确保episode信息在正确的位置
                    info['episode'] = episode_info
                    
                    # 只有主进程输出调试日志，且仅在真正需要时输出
                    if self.is_main_process:  # DEBUG级别
                        logger.debug(f"环境{i} Episode {self.episode_count}结束")
                        if 'time_cost' in episode_info:
                            logger.debug(f"时间成本: {episode_info['time_cost']:.2f}")
                
                # 重置统计
                self.episode_rewards[i] = 0
                self.episode_lengths[i] = 0
        
        return observations, rewards, dones, infos
    
    def reset(self) -> np.ndarray:
        """重置环境"""
        self.episode_rewards.fill(0)
        self.episode_lengths.fill(0)
        return self.venv.reset()
</file>

<file path="src/rl_optimizer/utils/setup.py">
# src/rl_optimizer/utils/setup.py

import sys
from typing import Any, Optional
import json
import pathlib
import pickle
import numpy as np
import multiprocessing
from loguru import logger

class NpEncoder(json.JSONEncoder):
    """自定义JSON编码器，以处理Numpy数据类型和路径对象。"""
    def default(self, obj: Any) -> Any:
        if isinstance(obj, np.integer):
            return int(obj)
        if isinstance(obj, np.floating):
            return float(obj)
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        if isinstance(obj, (pathlib.Path, pathlib.PosixPath, pathlib.WindowsPath)):
            return str(obj)
        return super(NpEncoder, self).default(obj)
    
def setup_logger(name: str, log_file: Optional[pathlib.Path] = None, level: int = 20) -> "logger":
    """配置并返回loguru日志记录器。

    Args:
        name (str): 日志记录器的名称（用于上下文标识）。
        log_file (Optional[pathlib.Path]): 可选的日志文件路径。
        level (int): 日志级别，默认为20（INFO）。

    Returns:
        logger: 配置好的loguru日志记录器实例。
    """
    # 转换标准logging级别到loguru级别
    level_map = {10: "DEBUG", 20: "INFO", 30: "WARNING", 40: "ERROR", 50: "CRITICAL"}
    log_level = level_map.get(level, "INFO")
    
    # 检查是否为多进程环境中的子进程
    is_main_process = True
    try:
        current_process = multiprocessing.current_process()
        if current_process.name != 'MainProcess':
            is_main_process = False
    except:
        pass
    
    # 为每个模块创建唯一的logger标识符
    logger_id = f"module_{name}"
    
    # 移除现有的handlers以避免重复配置
    logger.remove()
    
    # 配置控制台输出（彩色日志）
    console_format = (
        "<green>{time:YYYY-MM-DD HH:mm:ss.SSS}</green> | "
        "<level>{level: <8}</level> | "
        "<cyan>{extra[module]}</cyan> | "
        "<level>{message}</level>"
    )
    
    # 为主进程和子进程设置不同的日志级别
    console_level = log_level if is_main_process else "WARNING"
    
    logger.add(
        sys.stdout,
        format=console_format,
        level=console_level,
        colorize=True,
        backtrace=True,
        diagnose=True
    )
    
    # 如果提供了日志文件路径且在主进程中，添加文件处理器
    if log_file and is_main_process:
        log_file.parent.mkdir(parents=True, exist_ok=True)
        
        file_format = (
            "{time:YYYY-MM-DD HH:mm:ss.SSS} | "
            "{level: <8} | "
            "{extra[module]} | "
            "{message}"
        )
        
        logger.add(
            str(log_file),
            format=file_format,
            level=log_level,
            rotation="10 MB",
            retention="30 days",
            compression="zip",
            encoding="utf-8"
        )
    
    # 绑定模块名称到logger上下文
    return logger.bind(module=name)

def save_json(data: dict, path: pathlib.Path):
    """使用自定义编码器将字典保存为JSON文件。"""
    with open(path, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2, cls=NpEncoder)

def load_json(path: pathlib.Path) -> dict:
    """从JSON文件加载字典。"""
    with open(path, 'r', encoding='utf-8') as f:
        return json.load(f)
    
def save_pickle(data: Any, path: pathlib.Path):
    """将任何Python对象序列化为pickle文件。"""
    with open(path, 'wb') as f:
        pickle.dump(data, f)

def load_pickle(path: pathlib.Path) -> Any:
    """从pickle文件加载Python对象。"""
    with open(path, 'rb') as f:
        return pickle.load(f)
</file>

<file path="src/algorithms/simulated_annealing.py">
"""
模拟退火优化器 - 基于模拟退火算法的布局优化
"""

import math
import random
import copy
from src.rl_optimizer.utils.setup import setup_logger
from typing import List, Optional, Dict, Any

from src.algorithms.base_optimizer import BaseOptimizer, OptimizationResult
from src.algorithms.constraint_manager import ConstraintManager
from src.rl_optimizer.env.cost_calculator import CostCalculator

logger = setup_logger(__name__)


class SimulatedAnnealingOptimizer(BaseOptimizer):
    """
    模拟退火优化器
    
    使用模拟退火算法进行布局优化。模拟退火是一种启发式全局优化算法，
    通过模拟固体退火过程来寻找全局最优解。
    """
    
    def __init__(self, 
                 cost_calculator: CostCalculator,
                 constraint_manager: ConstraintManager,
                 initial_temperature: float = 1000.0,
                 final_temperature: float = 0.1,
                 cooling_rate: float = 0.95,
                 temperature_length: int = 100):
        """
        初始化模拟退火优化器
        
        Args:
            cost_calculator: 成本计算器
            constraint_manager: 约束管理器
            initial_temperature: 初始温度
            final_temperature: 终止温度
            cooling_rate: 冷却速率
            temperature_length: 每个温度下的迭代次数
        """
        super().__init__(cost_calculator, constraint_manager, "SimulatedAnnealing")
        
        self.initial_temperature = initial_temperature
        self.final_temperature = final_temperature
        self.cooling_rate = cooling_rate
        self.temperature_length = temperature_length
        
        # 算法状态
        self.current_temperature = initial_temperature
        self.acceptance_count = 0
        self.rejection_count = 0
        self.improvement_count = 0
        
        logger.info(f"模拟退火优化器初始化完成:")
        logger.info(f"  初始温度: {initial_temperature}")
        logger.info(f"  终止温度: {final_temperature}") 
        logger.info(f"  冷却速率: {cooling_rate}")
        logger.info(f"  温度长度: {temperature_length}")
    
    def optimize(self, 
                 initial_layout: Optional[List[str]] = None,
                 max_iterations: int = 10000,
                 original_layout: Optional[List[str]] = None,
                 original_cost: Optional[float] = None,
                 **kwargs) -> OptimizationResult:
        """
        执行模拟退火优化
        
        Args:
            initial_layout: 初始布局
            max_iterations: 最大迭代次数
            original_layout: 原始布局（未经优化的基准）
            original_cost: 原始布局的成本
            **kwargs: 其他参数
            
        Returns:
            OptimizationResult: 优化结果
        """
        self.start_optimization()
        
        # 保存原始布局信息
        self.original_layout = original_layout
        self.original_cost = original_cost
        
        # 初始化当前解
        if initial_layout is None:
            current_layout = self.generate_initial_layout()
        else:
            current_layout = initial_layout.copy()
            
        current_cost = self.evaluate_layout(current_layout)
        self.update_best_solution(current_layout, current_cost)
        
        logger.info(f"模拟退火优化开始，初始成本: {current_cost:.2f}")
        
        # 重置算法状态
        self.current_temperature = self.initial_temperature
        self.acceptance_count = 0
        self.rejection_count = 0
        self.improvement_count = 0
        
        try:
            while (self.current_temperature > self.final_temperature and 
                   self.current_iteration < max_iterations):
                
                # 在当前温度下进行多次迭代
                for _ in range(self.temperature_length):
                    if self.current_iteration >= max_iterations:
                        break
                    
                    # 生成邻域解
                    neighbor_layout = self._generate_neighbor(current_layout)
                    neighbor_cost = self.evaluate_layout(neighbor_layout)
                    
                    # 计算成本差
                    delta_cost = neighbor_cost - current_cost
                    
                    # 决定是否接受新解
                    if self._accept_solution(delta_cost, self.current_temperature):
                        current_layout = neighbor_layout
                        current_cost = neighbor_cost
                        self.acceptance_count += 1
                        
                        # 检查是否为新的最优解
                        if neighbor_cost < self.best_cost:
                            self.update_best_solution(neighbor_layout, neighbor_cost)
                            self.improvement_count += 1
                    else:
                        self.rejection_count += 1
                    
                    self.current_iteration += 1
                    self.log_iteration(current_cost, f"T={self.current_temperature:.2f}")
                
                # 降温
                self.current_temperature *= self.cooling_rate
                
                if self.current_iteration % 1000 == 0:
                    logger.info(f"迭代 {self.current_iteration}: "
                              f"当前成本={current_cost:.2f}, "
                              f"最优成本={self.best_cost:.2f}, "
                              f"温度={self.current_temperature:.2f}")
                              
        except KeyboardInterrupt:
            logger.warning("模拟退火优化被用户中断")
        
        logger.info(f"模拟退火优化完成:")
        logger.info(f"  最终温度: {self.current_temperature:.2f}")
        logger.info(f"  接受次数: {self.acceptance_count}")
        logger.info(f"  拒绝次数: {self.rejection_count}")
        logger.info(f"  改进次数: {self.improvement_count}")
        logger.info(f"  接受率: {self.acceptance_count/(self.acceptance_count + self.rejection_count)*100:.1f}%")
        
        return self.finish_optimization()
    
    def _generate_neighbor(self, current_layout: List[str]) -> List[str]:
        """
        生成邻域解
        
        Args:
            current_layout: 当前布局
            
        Returns:
            List[str]: 邻域布局
        """
        neighbor = current_layout.copy()
        
        # 选择邻域生成策略
        strategies = ['swap', 'relocate', 'multiple_swap']
        strategy = random.choice(strategies)
        
        if strategy == 'swap':
            # 交换两个科室的位置
            self._swap_departments(neighbor)
        elif strategy == 'relocate':
            # 重新定位一个科室
            self._relocate_department(neighbor)
        elif strategy == 'multiple_swap':
            # 多次小幅调整
            num_swaps = random.randint(2, min(5, len(neighbor)//2))
            for _ in range(num_swaps):
                if random.random() < 0.7:
                    self._swap_departments(neighbor)
                else:
                    self._relocate_department(neighbor)
        
        # 确保生成的邻域解满足约束
        if not self.constraint_manager.is_valid_layout(neighbor):
            # 如果不满足约束，尝试修复或返回原布局
            neighbor = self._repair_layout(neighbor) or current_layout.copy()
        
        return neighbor
    
    def _swap_departments(self, layout: List[str]):
        """随机交换两个科室的位置"""
        if len(layout) < 2:
            return
        
        # 过滤掉None值，只对有效的位置进行交换
        valid_positions = [i for i, dept in enumerate(layout) if dept is not None]
        if len(valid_positions) < 2:
            return
            
        # 随机选择两个不同的有效位置
        pos1, pos2 = random.sample(valid_positions, 2)
        
        # 检查是否可以交换（考虑固定位置约束）
        if self.constraint_manager.get_swap_candidates(layout, pos1, pos2):
            # 确保两个位置都不是None（额外的安全检查）
            if layout[pos1] is not None and layout[pos2] is not None:
                layout[pos1], layout[pos2] = layout[pos2], layout[pos1]
    
    def _relocate_department(self, layout: List[str]):
        """
        改进的重定位操作：安全地将一个科室移动到新位置
        使用更温和的方式保持布局结构完整性
        """
        if len(layout) < 2:
            return
        
        # 过滤掉None值，只对有效的位置进行操作
        valid_positions = [i for i, dept in enumerate(layout) if dept is not None]
        if len(valid_positions) < 2:
            return
            
        # 随机选择两个有效位置
        old_pos = random.choice(valid_positions)
        new_pos = random.choice(valid_positions)
        
        if old_pos != new_pos:
            # 保存要移动的科室
            dept = layout[old_pos]
            
            # 确保dept不是None
            if dept is None:
                return
            
            # 使用安全的移动策略：通过一系列移位来实现重定位
            # 这种方式保证不会丢失任何科室
            if old_pos < new_pos:
                # 向右移动：将中间的元素向左移一位
                temp = layout[old_pos]
                for i in range(old_pos, new_pos):
                    layout[i] = layout[i + 1]
                layout[new_pos] = temp
            else:
                # 向左移动：将中间的元素向右移一位
                temp = layout[old_pos]
                for i in range(old_pos, new_pos, -1):
                    layout[i] = layout[i - 1]
                layout[new_pos] = temp
    
    def _repair_layout(self, layout: List[str]) -> Optional[List[str]]:
        """
        尝试修复不满足约束的布局
        
        Args:
            layout: 待修复的布局
            
        Returns:
            Optional[List[str]]: 修复后的布局，如果无法修复则返回None
        """
        repaired = layout.copy()
        
        # 简单修复策略：随机重新分配违反约束的科室
        max_attempts = getattr(self.constraint_manager.config, 'SA_MAX_REPAIR_ATTEMPTS', 10)
        for attempt in range(max_attempts):  # 最多尝试次数从配置读取
            if self.constraint_manager.is_valid_layout(repaired):
                return repaired
                
            # 找到违反约束的位置并尝试修复
            for i, dept in enumerate(repaired):
                if dept is not None:
                    compatible_depts = self.constraint_manager.get_compatible_departments(i)
                    if dept not in compatible_depts and compatible_depts:
                        # 尝试找到一个兼容的科室进行交换
                        for j, other_dept in enumerate(repaired):
                            if other_dept in compatible_depts:
                                repaired[i], repaired[j] = repaired[j], repaired[i]
                                break
        
        return None  # 无法修复
    
    def _accept_solution(self, delta_cost: float, temperature: float) -> bool:
        """
        根据模拟退火接受准则决定是否接受新解
        
        Args:
            delta_cost: 成本差值
            temperature: 当前温度
            
        Returns:
            bool: 是否接受新解
        """
        if delta_cost <= 0:
            # 更优解总是接受
            return True
        
        if temperature <= 0:
            # 温度为0时只接受更优解
            return False
        
        # 按概率接受较差解
        probability = math.exp(-delta_cost / temperature)
        return random.random() < probability
    
    def get_additional_metrics(self) -> Dict[str, Any]:
        """获取模拟退火特定的额外指标"""
        total_attempts = self.acceptance_count + self.rejection_count
        acceptance_rate = self.acceptance_count / total_attempts if total_attempts > 0 else 0
        
        return {
            "initial_temperature": self.initial_temperature,
            "final_temperature": self.final_temperature,
            "current_temperature": self.current_temperature,
            "cooling_rate": self.cooling_rate,
            "temperature_length": self.temperature_length,
            "acceptance_count": self.acceptance_count,
            "rejection_count": self.rejection_count,
            "improvement_count": self.improvement_count,
            "acceptance_rate": acceptance_rate,
            "total_temperature_cycles": int((math.log(self.final_temperature/self.initial_temperature) / 
                                           math.log(self.cooling_rate)) if self.cooling_rate < 1 else 0)
        }
</file>

<file path="src/analysis/process_flow.py">
import pandas as pd
from src.rl_optimizer.utils.setup import setup_logger
from pathlib import Path
from typing import List, Dict, Optional, Tuple, Set, Any, Mapping
from itertools import product

from src.analysis.word_detect import WordDetect
from src.config import NetworkConfig

logger = setup_logger(__name__)


class PeopleFlow:
    """
    Represents a specific, resolved flow of an entity through a sequence of nodes.

    Each node in the flow is represented by its unique "Name_ID" string.
    This class primarily stores a fully determined path.

    Attributes:
        identify (Any): An identifier for this specific flow instance or the
            workflow it belongs to.
        actual_node_id_sequence (List[str]): The complete, ordered list of
            "Name_ID" strings representing the flow.
        _cached_hash (Optional[int]): Cached hash value for performance.
    """

    def __init__(self, identify: Any, actual_node_id_sequence: List[str]):
        """
        Initializes a PeopleFlow instance with a specific node ID sequence.

        Args:
            identify (Any): An identifier for this flow.
            actual_node_id_sequence (List[str]): The fully resolved sequence
                of "Name_ID" strings for this flow.
        """
        self.identify: Any = identify
        self.actual_node_id_sequence: List[str] = actual_node_id_sequence
        self.total_time: Optional[float] = None
        self._cached_hash: Optional[int] = None

    def update_total_time(self, total_time: float) -> None:
        """
        Updates the total travel time for this flow.
        """
        self.total_time = total_time

    @property
    def start_node_id(self) -> Optional[str]:
        """Optional[str]: The 'Name_ID' of the first node in the sequence, if any."""
        return self.actual_node_id_sequence[0] if self.actual_node_id_sequence else None

    @property
    def end_node_id(self) -> Optional[str]:
        """Optional[str]: The 'Name_ID' of the last node in the sequence, if any."""
        if len(self.actual_node_id_sequence) > 0:
            return self.actual_node_id_sequence[-1]
        return None

    @property
    def intermediate_node_ids(self) -> List[str]:
        """List[str]: A list of 'Name_ID's for intermediate nodes."""
        if len(self.actual_node_id_sequence) > 2:
            return self.actual_node_id_sequence[1:-1]
        return []

    def __eq__(self, other: object) -> bool:
        """
        Checks equality based on the `identify` and `actual_node_id_sequence`.
        """
        if not isinstance(other, PeopleFlow):
            return NotImplemented
        return (self.identify == other.identify and
                self.actual_node_id_sequence == other.actual_node_id_sequence)

    def __hash__(self) -> int:
        """
        Computes hash based on `identify` and the tuple of `actual_node_id_sequence`.
        """
        if self._cached_hash is None:
            # Making sequence a tuple makes it hashable
            self._cached_hash = hash(
                (self.identify, tuple(self.actual_node_id_sequence)))
        return self._cached_hash

    def __repr__(self) -> str:
        """
        Returns a string representation of the PeopleFlow instance.
        """
        flow_str = " -> ".join(
            self.actual_node_id_sequence) if self.actual_node_id_sequence else "Empty"
        return (
            f"PeopleFlow(identify={self.identify}, "
            f"path=[{flow_str}], "
            f"total_time={self.total_time})"
        )


class PathFinder:
    """
    Finds all possible PeopleFlow paths based on a sequence of node names
    and a CSV file defining available "Name_ID"s and their connections/travel times.
    """

    def __init__(self, csv_filepath: str | None  = None, config: NetworkConfig | None = None):
        """
        Initializes the PathFinder by loading and processing the CSV file.

        Args:
            csv_filepath (str): Path to the CSV file. The CSV should have
                "Name_ID" formatted strings as column headers and row index.
        """
        if config:
            self.config: NetworkConfig = config
        else:
            self.config: NetworkConfig = NetworkConfig()

        if csv_filepath:
            self.csv_filepath: Path = Path(csv_filepath)
        else:
            self.csv_filepath: Path = self.config.RESULT_PATH / 'super_network_travel_times.csv'

        self.name_to_ids_map: Dict[str, List[str]] = {}
        self.all_name_ids: Set[str] = set()
        self.travel_times_df: Optional[pd.DataFrame] = None
        self._load_and_process_csv()

    def _parse_name_id(self, name_id_str: str) -> Tuple[str, str]:
        """
        Parses a "Name_ID" string into its name and ID components.

        Args:
            name_id_str (str): The string in "Name_ID" format (e.g., "Door_11072").

        Returns:
            Tuple[str, str]: (name, id)
        """
        parts = name_id_str.split('_', 1)
        name = parts[0]
        # if no '_', id is same as name
        node_id = parts[1] if len(parts) > 1 else name
        return name, node_id

    def _load_and_process_csv(self) -> None:
        """
        Loads the CSV, extracts "Name_ID"s, and populates the name_to_ids_map.
        """
        try:
            # Assuming the first column is the index and also contains Name_ID
            df = pd.read_csv(self.csv_filepath, index_col=0)
            self.travel_times_df = df
        except FileNotFoundError:
            logger.error(f"Error: CSV file not found at {self.csv_filepath}")
            return
        except Exception as e:
            logger.error(f"Error loading CSV {self.csv_filepath}: {e}")
            return

        # Process column headers (assuming they are the primary source of Name_IDs)
        # and also the index if it's different or more comprehensive
        all_headers = list(df.columns)
        if df.index.name is not None or df.index.dtype == 'object':  # Check if index is meaningful
            all_headers.extend(list(df.index))

        unique_name_ids = sorted(list(set(all_headers)))  # Get unique Name_IDs

        for name_id_str in unique_name_ids:
            if not isinstance(name_id_str, str) or '_' not in name_id_str:
                # Skip non-string headers or headers not in expected format, like "面积"
                # print(f"Skipping header/index: {name_id_str} as it's not in Name_ID format.")
                continue

            self.all_name_ids.add(name_id_str)
            name, _ = self._parse_name_id(name_id_str)
            if name not in self.name_to_ids_map:
                self.name_to_ids_map[name] = []
            # Ensure no duplicates if a Name_ID appears in both columns and index
            if name_id_str not in self.name_to_ids_map[name]:
                self.name_to_ids_map[name].append(name_id_str)

    def transform_workflow_name(self, workflow_names: List[str]) -> str:
        """
        Transforms a workflow name by detecting the nearest word in the CSV.
        """
        word_detect = WordDetect(config=self.config)
        all_type_names = self.config.ALL_TYPES
        return word_detect.detect_nearest_word(workflow_names, all_type_names)

    def generate_flows(self,
                       workflow_names: List[str],
                       workflow_identifier: Any = 0,
                       # Changed Dict to Mapping for broader type hint
                       custom_assignment_map: Optional[Mapping[str,
                                                               List[str]]] = None
                       ) -> List[PeopleFlow]:
        """Generates all possible PeopleFlow objects for a given sequence of node names.

        Uses custom_assignment_map if provided for resolving functional names to
        physical Name_IDs, otherwise defaults to the instance's self.name_to_ids_map
        which is loaded from the CSV during initialization.

        Args:
            workflow_names: An ordered list of functional node names.
            workflow_identifier: An identifier for the generated PeopleFlow objects.
            custom_assignment_map: An optional mapping where keys are functional type
                names (e.g., '妇科') and values are lists of physical Name_ID strings
                (e.g., ['妇科_101', '妇科_102']) that currently fulfill that function.

        Returns:
            A list of PeopleFlow objects.
        """
        if not workflow_names:
            return []

        # Use the provided custom_assignment_map if available, otherwise use the default one
        assignment_to_use = custom_assignment_map if custom_assignment_map is not None else self.name_to_ids_map

        # Convert node names in `workflow_names` to the closest node types (e.g., from config.ALL_TYPES)
        # This step helps normalize user input if workflow_names might not exactly match official types.

        possible_ids_per_step: List[List[str]] = []
        for name in workflow_names:
            # Use the determined assignment map
            ids_for_name = assignment_to_use.get(name)
            if not ids_for_name:
                logger.warning(
                    f"Functional type '{name}' (from original '{workflow_names[workflow_names.index(name)] if name in workflow_names else 'N/A'}') "
                    f"not found in the current assignment map. Workflow '{workflow_identifier}' cannot be fully resolved."
                )
                return []  # Cannot generate flows if a step is unresolvable
            possible_ids_per_step.append(ids_for_name)

        # Use itertools.product to get all combinations of Name_IDs
        all_possible_id_sequences = product(*possible_ids_per_step)

        generated_flows: List[PeopleFlow] = []
        # Use a local counter for unique flow IDs within this specific generation call,
        # prefixed by the workflow_identifier.
        flow_counter_for_identifier = 0
        for id_sequence_tuple in all_possible_id_sequences:
            # Create a more unique identifier for the flow if multiple flows are generated for one workflow_identifier
            current_flow_id = f"{workflow_identifier}_{flow_counter_for_identifier}" \
                if isinstance(workflow_identifier, str) else (workflow_identifier, flow_counter_for_identifier)

            flow = PeopleFlow(identify=current_flow_id,
                              actual_node_id_sequence=list(id_sequence_tuple))
            flow_counter_for_identifier += 1
            generated_flows.append(flow)

        return generated_flows

    def get_travel_time(self, from_name_id: str, to_name_id: str) -> Optional[float]:
        """
        Gets the travel time between two specific "Name_ID" nodes.

        Args:
            from_name_id (str): The "Name_ID" of the source node.
            to_name_id (str): The "Name_ID" of the target node.

        Returns:
            Optional[float]: The travel time if connection exists, else None.
                           Returns None also if dataframe isn't loaded.
        """
        if self.travel_times_df is None:
            logger.warning("Warning: Travel times DataFrame not loaded.")
            return None
        try:
            # Ensure both from_name_id and to_name_id are in the DataFrame's
            # index and columns to avoid KeyError
            if from_name_id in self.travel_times_df.index and \
               to_name_id in self.travel_times_df.columns:
                time = self.travel_times_df.loc[from_name_id, to_name_id]
                return float(time)  # Ensure it's a float
            else:
                logger.warning(
                    f"Warning: Node {from_name_id} or {to_name_id} not in travel matrix.")
                return None  # Or handle as an error, e.g., raise ValueError
        except KeyError:
            logger.warning(
                f"KeyError: Node {from_name_id} or {to_name_id} not found in travel matrix.")
            return None
        except ValueError:  # If conversion to float fails for some reason
            logger.warning(
                f"ValueError: Travel time for {from_name_id} to {to_name_id} is not a valid number.")
            return None

    def calculate_flow_total_time(self, flow: PeopleFlow) -> Optional[float]:
        """
        Calculates the total travel time for a given PeopleFlow.

        Args:
            flow (PeopleFlow): The PeopleFlow object.

        Returns:
            Optional[float]: The total travel time for the flow.
                             Returns None if any segment has no travel time.
                             Returns 0.0 for single-node flows.
        """
        if not flow.actual_node_id_sequence:
            return 0.0
        if len(flow.actual_node_id_sequence) == 1:
            return 0.0  # No travel for a single point

        total_time = 0.0
        for i in range(len(flow.actual_node_id_sequence) - 1):
            from_node = flow.actual_node_id_sequence[i]
            to_node = flow.actual_node_id_sequence[i+1]
            segment_time = self.get_travel_time(from_node, to_node)
            if segment_time is None:
                logger.warning(
                    f"Warning: No travel time for segment {from_node} -> {to_node} in flow {flow.identify}.")
                return None  # Or handle this case differently, e.g. infinite time
            total_time += segment_time
        flow.update_total_time(total_time)
        return total_time
</file>

<file path="src/analysis/word_detect.py">
import torch
from src.rl_optimizer.utils.setup import setup_logger
from sentence_transformers import SentenceTransformer, util
from src.config import NetworkConfig

logger = setup_logger(__name__)

class WordDetect:
    def __init__(self, model: SentenceTransformer = None, config: NetworkConfig = None):
        self.model = model
        self.config = config
        self._initialize_model()

    def _initialize_model(self):
        if not self.model:
            model_name = 'paraphrase-multilingual-MiniLM-L12-v2'
            self.model = SentenceTransformer(model_name)
            logger.info(f'Loaded model: {model_name}')

    def _detect_nearest_word(self, query_word: str, word_list: list[str] = None):
        if word_list is None:
            word_list = self.config.ALL_TYPES
        query_embedding = self.model.encode(query_word, convert_to_tensor=True)
        list_embedding = self.model.encode(word_list, convert_to_tensor=True)
        cosine_scores = util.pytorch_cos_sim(query_embedding, list_embedding)
        max_score_index = torch.argmax(cosine_scores)
        return word_list[max_score_index]

    def detect_nearest_word(self, query_word: str | list[str], word_list: list[str] | None = None):
        if isinstance(query_word, str):
            return self._detect_nearest_word(query_word, word_list)
        elif isinstance(query_word, list):
            return [self._detect_nearest_word(word, word_list) for word in query_word]
        else:
            raise ValueError(f"Invalid query_word type: {type(query_word)}")
</file>

<file path="src/comparison/results_comparator.py">
"""
结果对比分析器 - 提供详细的算法对比分析和可视化功能
"""

from src.rl_optimizer.utils.setup import setup_logger
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from pathlib import Path
from typing import Dict, List, Optional, Any
import json
import seaborn as sns
from datetime import datetime

from src.algorithms.base_optimizer import OptimizationResult
from src.rl_optimizer.data.cache_manager import CacheManager

logger = setup_logger(__name__)


class ResultsComparator:
    """
    结果对比分析器
    
    提供算法结果的详细对比分析，包括统计分析、可视化图表、
    性能指标计算和报告生成功能。
    """
    
    def __init__(self, results: Dict[str, OptimizationResult], cache_manager: Optional[CacheManager] = None):
        """
        初始化结果对比分析器
        
        Args:
            results: 算法名到结果的映射
            cache_manager: 缓存管理器（用于获取面积信息）
        """
        self.results = results
        self.algorithm_names = list(results.keys())
        self.comparison_df = None
        self.cache_manager = cache_manager
        
        logger.info(f"结果对比分析器初始化完成，包含 {len(results)} 个算法结果")
    
    def generate_comparison_table(self) -> pd.DataFrame:
        """
        生成详细的对比表格
        
        Returns:
            pd.DataFrame: 对比表格
        """
        comparison_data = []
        
        for algorithm_name, result in self.results.items():
            # 基础指标
            row = {
                'Algorithm': result.algorithm_name,
                'Best_Cost': result.best_cost,
                'Runtime_Sec': result.execution_time,
                'Iterations': result.iterations,
                'Iter_Per_Sec': result.iterations / result.execution_time if result.execution_time > 0 else 0,
                'Improvement_Rate': self._calculate_improvement_rate(result),
                'Convergence_Stability': self._calculate_convergence_stability(result),
                'Search_Efficiency': self._calculate_search_efficiency(result)
            }
            
            # 算法特定指标
            metrics = result.additional_metrics
            if algorithm_name == 'simulated_annealing':
                row.update({
                    'SA_Acceptance_Rate': metrics.get('acceptance_rate', 0) * 100,
                    'SA_Improvement_Count': metrics.get('improvement_count', 0),
                    'SA_Initial_Temp': metrics.get('initial_temperature', 0),
                    'SA_Final_Temp': metrics.get('current_temperature', 0)
                })
            elif algorithm_name == 'genetic_algorithm':
                row.update({
                    'GA_Final_Generation': metrics.get('final_generation', 0),
                    'GA_Population_Diversity': metrics.get('population_diversity', 0),
                    'GA_Convergence_Rate': metrics.get('convergence_rate', 0) * 100,
                    'GA_Stagnation_Count': metrics.get('stagnation_count', 0)
                })
            elif algorithm_name == 'ppo':
                row.update({
                    'PPO_Training_Steps': metrics.get('total_timesteps', 0),
                    'PPO_Num_Envs': metrics.get('num_envs', 0),
                    'PPO_LR_Schedule': metrics.get('learning_rate_schedule', 'N/A')
                })
            
            comparison_data.append(row)
        
        self.comparison_df = pd.DataFrame(comparison_data)
        
        # 计算排名
        self.comparison_df['Cost_Rank'] = self.comparison_df['Best_Cost'].rank()
        self.comparison_df['Time_Rank'] = self.comparison_df['Runtime_Sec'].rank()
        self.comparison_df['Efficiency_Rank'] = self.comparison_df['Search_Efficiency'].rank(ascending=False)
        
        # 计算综合得分
        self.comparison_df['Composite_Score'] = self._calculate_composite_score()
        
        # 按最优成本排序
        self.comparison_df = self.comparison_df.sort_values('Best_Cost').reset_index(drop=True)
        
        return self.comparison_df
    
    def _calculate_improvement_rate(self, result: OptimizationResult) -> float:
        """计算改进率"""
        if not result.convergence_history or len(result.convergence_history) < 2:
            return 0.0
        
        initial_cost = result.original_cost
        final_cost = result.best_cost
        
        if initial_cost > 0:
            return ((initial_cost - final_cost) / initial_cost) * 100
        return 0.0
    
    def _calculate_convergence_stability(self, result: OptimizationResult) -> float:
        """计算收敛稳定性"""
        if not result.convergence_history or len(result.convergence_history) < 10:
            return 0.0
        
        # 取最后20%的迭代历史计算稳定性
        history = result.convergence_history
        tail_length = max(10, len(history) // 5)
        tail_history = history[-tail_length:]
        
        if len(tail_history) < 2:
            return 0.0
        
        # 使用变异系数衡量稳定性
        mean_cost = np.mean(tail_history)
        std_cost = np.std(tail_history)
        
        if mean_cost > 0:
            cv = std_cost / mean_cost
            stability = 1.0 / (1.0 + cv)  # 变异系数越小，稳定性越高
            return stability
        
        return 0.0
    
    def _calculate_search_efficiency(self, result: OptimizationResult) -> float:
        """计算搜索效率"""
        if result.execution_time <= 0 or not result.convergence_history:
            return 0.0
        
        improvement = self._calculate_improvement_rate(result)
        time_penalty = 1.0 / (1.0 + np.log10(result.execution_time + 1))
        
        efficiency = (improvement / 100.0) * time_penalty
        return efficiency
    
    def _calculate_composite_score(self) -> pd.Series:
        """计算综合得分"""
        if self.comparison_df is None:
            return pd.Series()
        
        # 归一化各项指标（越小越好的指标需要取倒数）
        cost_norm = 1.0 / (self.comparison_df['Best_Cost'] / self.comparison_df['Best_Cost'].min())
        time_norm = 1.0 / (self.comparison_df['Runtime_Sec'] / self.comparison_df['Runtime_Sec'].min())
        improvement_norm = self.comparison_df['Improvement_Rate'] / self.comparison_df['Improvement_Rate'].max()
        stability_norm = self.comparison_df['Convergence_Stability'] / self.comparison_df['Convergence_Stability'].max()
        efficiency_norm = self.comparison_df['Search_Efficiency'] / self.comparison_df['Search_Efficiency'].max()
        
        # 加权计算综合得分
        weights = {
            'cost': 0.4,      # 成本权重最高
            'time': 0.2,      # 时间权重
            'improvement': 0.2, # 改进率权重
            'stability': 0.1,  # 稳定性权重
            'efficiency': 0.1  # 效率权重
        }
        
        composite_score = (
            weights['cost'] * cost_norm +
            weights['time'] * time_norm + 
            weights['improvement'] * improvement_norm +
            weights['stability'] * stability_norm +
            weights['efficiency'] * efficiency_norm
        )
        
        return composite_score
    
    def create_comparison_plots(self, output_dir: str = "./results/plots"):
        """
        创建对比图表
        
        Args:
            output_dir: 输出目录
        """
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
        
        # 1. 最优成本对比图
        self._plot_cost_comparison(output_path, timestamp)
        
        # 2. 执行时间对比图
        self._plot_time_comparison(output_path, timestamp)
        
        # 3. 收敛曲线图
        self._plot_convergence_curves(output_path, timestamp)
        
        # 4. 综合性能雷达图
        self._plot_performance_radar(output_path, timestamp)
        
        # 5. 算法特性热力图
        self._plot_algorithm_heatmap(output_path, timestamp)
        
        logger.info(f"所有对比图表已保存到: {output_path}")
    
    def _plot_cost_comparison(self, output_path: Path, timestamp: str):
        """绘制成本对比图"""
        plt.figure(figsize=(10, 6))
        
        costs = [result.best_cost for result in self.results.values()]
        colors = plt.cm.Set3(np.linspace(0, 1, len(self.algorithm_names)))
        
        bars = plt.bar(self.algorithm_names, costs, color=colors)
        plt.title('Algorithm Cost Comparison', fontsize=16, fontweight='bold')
        plt.xlabel('Algorithm', fontsize=12)
        plt.ylabel('Optimal Cost', fontsize=12)
        plt.xticks(rotation=45)
        
        # 添加数值标签
        for bar, cost in zip(bars, costs):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(costs)*0.01,
                    f'{cost:.2f}', ha='center', va='bottom', fontweight='bold')
        
        plt.grid(axis='y', alpha=0.3)
        plt.tight_layout()
        plt.savefig(output_path / f"cost_comparison_{timestamp}.png", dpi=300, bbox_inches='tight')
        plt.close()
    
    def _plot_time_comparison(self, output_path: Path, timestamp: str):
        """绘制执行时间对比图"""
        plt.figure(figsize=(10, 6))
        
        times = [result.execution_time for result in self.results.values()]
        colors = plt.cm.Set2(np.linspace(0, 1, len(self.algorithm_names)))
        
        bars = plt.bar(self.algorithm_names, times, color=colors)
        plt.title('Algorithm Runtime Comparison', fontsize=16, fontweight='bold')
        plt.xlabel('Algorithm', fontsize=12)
        plt.ylabel('Runtime (seconds)', fontsize=12)
        plt.xticks(rotation=45)
        plt.yscale('log')  # 使用对数刻度，因为PPO可能时间很长
        
        # 添加数值标签
        for bar, time in zip(bars, times):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() * 1.1,
                    f'{time:.1f}s', ha='center', va='bottom', fontweight='bold')
        
        plt.grid(axis='y', alpha=0.3)
        plt.tight_layout()
        plt.savefig(output_path / f"time_comparison_{timestamp}.png", dpi=300, bbox_inches='tight')
        plt.close()
    
    def _plot_convergence_curves(self, output_path: Path, timestamp: str):
        """绘制收敛曲线图"""
        plt.figure(figsize=(12, 8))
        
        colors = plt.cm.tab10(np.linspace(0, 1, len(self.algorithm_names)))
        
        for i, (algorithm_name, result) in enumerate(self.results.items()):
            if result.convergence_history:
                history = result.convergence_history
                # 对于长度不同的历史，进行采样以便比较
                if len(history) > 1000:
                    indices = np.linspace(0, len(history)-1, 1000, dtype=int)
                    sampled_history = [history[idx] for idx in indices]
                    x_vals = np.linspace(0, len(history), 1000)
                else:
                    sampled_history = history
                    x_vals = range(len(history))
                
                plt.plot(x_vals, sampled_history, label=result.algorithm_name, 
                        color=colors[i], linewidth=2)
        
        plt.title('Algorithm Convergence Curves', fontsize=16, fontweight='bold')
        plt.xlabel('Iterations', fontsize=12)
        plt.ylabel('Cost', fontsize=12)
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.savefig(output_path / f"convergence_curves_{timestamp}.png", dpi=300, bbox_inches='tight')
        plt.close()
    
    def _plot_performance_radar(self, output_path: Path, timestamp: str):
        """绘制性能雷达图"""
        if self.comparison_df is None:
            self.generate_comparison_table()
        
        # 检查DataFrame是否为空
        if self.comparison_df.empty:
            logger.warning("对比数据为空，跳过雷达图生成")
            return
        
        # 选择关键指标
        metrics = ['Best_Cost', 'Runtime_Sec', 'Improvement_Rate', 'Convergence_Stability', 'Search_Efficiency']
        
        # 数据归一化（转换为0-1范围，越大越好）
        normalized_data = {}
        # 使用DataFrame中实际的算法名称而不是self.algorithm_names
        actual_algorithm_names = self.comparison_df['Algorithm'].tolist()
        
        for algorithm_name in actual_algorithm_names:
            try:
                row = self.comparison_df[self.comparison_df['Algorithm'] == algorithm_name].iloc[0]
            except IndexError:
                logger.warning(f"未找到算法 {algorithm_name} 的数据，跳过")
                continue
            
            values = []
            try:
                # 成本：越小越好，取倒数后归一化
                cost_val = 1.0 / row['Best_Cost'] if pd.notna(row['Best_Cost']) and row['Best_Cost'] > 0 else 0
                values.append(cost_val)
                
                # 时间：越小越好，取倒数后归一化
                time_val = 1.0 / row['Runtime_Sec'] if pd.notna(row['Runtime_Sec']) and row['Runtime_Sec'] > 0 else 0
                values.append(time_val)
                
                # 其他指标：越大越好，直接使用
                improvement_rate = row['Improvement_Rate'] / 100.0 if pd.notna(row['Improvement_Rate']) else 0
                convergence = row['Convergence_Stability'] if pd.notna(row['Convergence_Stability']) else 0
                efficiency = row['Search_Efficiency'] if pd.notna(row['Search_Efficiency']) else 0
                
                values.extend([improvement_rate, convergence, efficiency])
                
                normalized_data[algorithm_name] = values
                
            except Exception as e:
                logger.warning(f"处理算法 {algorithm_name} 的雷达图数据时出错: {e}")
                continue
        
        # 检查是否有有效数据
        if not normalized_data:
            logger.warning("没有有效的雷达图数据，跳过雷达图生成")
            return
        
        # 创建雷达图
        fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))
        
        angles = np.linspace(0, 2*np.pi, len(metrics), endpoint=False).tolist()
        angles += angles[:1]  # 闭合图形
        
        colors = plt.cm.Set1(np.linspace(0, 1, len(actual_algorithm_names)))
        
        for i, (algorithm_name, values) in enumerate(normalized_data.items()):
            values += values[:1]  # 闭合图形
            ax.plot(angles, values, 'o-', linewidth=2, label=algorithm_name, color=colors[i])
            ax.fill(angles, values, alpha=0.25, color=colors[i])
        
        ax.set_xticks(angles[:-1])
        ax.set_xticklabels(['Cost', 'Time', 'Improvement', 'Stability', 'Efficiency'])
        ax.set_ylim(0, 1)
        ax.set_title('Algorithm Performance Radar Chart', fontsize=16, fontweight='bold', pad=20)
        ax.legend(loc='upper right', bbox_to_anchor=(1.2, 1.0))
        ax.grid(True)
        
        plt.tight_layout()
        plt.savefig(output_path / f"performance_radar_{timestamp}.png", dpi=300, bbox_inches='tight')
        plt.close()
    
    def _plot_algorithm_heatmap(self, output_path: Path, timestamp: str):
        """绘制算法特性热力图"""
        if self.comparison_df is None:
            self.generate_comparison_table()
        
        # 选择数值列进行热力图展示
        numeric_cols = self.comparison_df.select_dtypes(include=[np.number]).columns
        display_cols = [col for col in numeric_cols if not col.endswith('排名')]
        
        # 准备数据
        heatmap_data = self.comparison_df[['Algorithm'] + display_cols].set_index('Algorithm')
        
        # 数据标准化
        from sklearn.preprocessing import StandardScaler
        scaler = StandardScaler()
        heatmap_data_scaled = pd.DataFrame(
            scaler.fit_transform(heatmap_data),
            index=heatmap_data.index,
            columns=heatmap_data.columns
        )
        
        # 创建热力图
        plt.figure(figsize=(12, 6))
        sns.heatmap(heatmap_data_scaled.T, annot=True, cmap='RdYlBu_r', center=0,
                   fmt='.2f', cbar_kws={'label': '标准化值'})
        plt.title('Algorithm Characteristics Heatmap (Normalized)', fontsize=16, fontweight='bold')
        plt.xlabel('Algorithm', fontsize=12)
        plt.ylabel('Performance Metrics', fontsize=12)
        plt.tight_layout()
        plt.savefig(output_path / f"algorithm_heatmap_{timestamp}.png", dpi=300, bbox_inches='tight')
        plt.close()
    
    def generate_detailed_report(self, output_dir: str = "./results/reports") -> str:
        """
        生成详细的对比分析报告
        
        Args:
            output_dir: 输出目录
            
        Returns:
            str: 报告文件路径
        """
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
        report_path = output_path / f"algorithm_comparison_report_{timestamp}.md"
        
        if self.comparison_df is None:
            self.generate_comparison_table()
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("# 医院布局优化算法对比分析报告\n\n")
            f.write(f"生成时间: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            # 执行摘要
            f.write("## 执行摘要\n\n")
            best_cost_algo = self.comparison_df.iloc[0]['Algorithm']
            best_cost = self.comparison_df.iloc[0]['Best_Cost']
            f.write(f"- **最优成本算法**: {best_cost_algo} (成本: {best_cost:.2f})\n")
            
            fastest_algo = self.comparison_df.loc[self.comparison_df['Runtime_Sec'].idxmin(), 'Algorithm']
            fastest_time = self.comparison_df['Runtime_Sec'].min()
            f.write(f"- **最快算法**: {fastest_algo} (时间: {fastest_time:.2f}秒)\n")
            
            best_efficiency_algo = self.comparison_df.loc[self.comparison_df['Search_Efficiency'].idxmax(), 'Algorithm']
            best_efficiency = self.comparison_df['Search_Efficiency'].max()
            f.write(f"- **最高效率算法**: {best_efficiency_algo} (效率: {best_efficiency:.4f})\n\n")
            
            # 详细对比表格
            f.write("## 详细对比结果\n\n")
            f.write(self.comparison_df.to_markdown(index=False))
            f.write("\n\n")
            
            # 算法分析
            f.write("## 算法详细分析\n\n")
            for algorithm_name, result in self.results.items():
                f.write(f"### {result.algorithm_name}\n\n")
                f.write(f"- **最优成本**: {result.best_cost:.2f}\n")
                f.write(f"- **执行时间**: {result.execution_time:.2f}秒\n")
                f.write(f"- **迭代次数**: {result.iterations}\n")
                
                # 算法特定分析
                metrics = result.additional_metrics
                if algorithm_name == 'simulated_annealing':
                    f.write(f"- **接受率**: {metrics.get('acceptance_rate', 0)*100:.1f}%\n")
                    f.write(f"- **改进次数**: {metrics.get('improvement_count', 0)}\n")
                elif algorithm_name == 'genetic_algorithm':
                    f.write(f"- **最终代数**: {metrics.get('final_generation', 0)}\n")
                    f.write(f"- **种群多样性**: {metrics.get('population_diversity', 0):.3f}\n")
                elif algorithm_name == 'ppo':
                    f.write(f"- **训练步数**: {metrics.get('total_timesteps', 0)}\n")
                    f.write(f"- **环境数量**: {metrics.get('num_envs', 0)}\n")
                
                f.write("\n")
            
            # 结论和建议
            f.write("## 结论和建议\n\n")
            f.write("基于上述分析结果，我们得出以下结论：\n\n")
            
            # 根据结果生成智能建议
            if len(self.results) >= 2:
                cost_range = self.comparison_df['Best_Cost'].max() - self.comparison_df['Best_Cost'].min()
                time_range = self.comparison_df['Runtime_Sec'].max() - self.comparison_df['Runtime_Sec'].min()
                
                if cost_range / self.comparison_df['Best_Cost'].min() > 0.1:
                    f.write("1. **成本差异显著**: 不同算法在优化质量上存在明显差异，建议优先使用成本最低的算法。\n")
                
                if time_range > 60:
                    f.write("2. **时间效率考虑**: 算法执行时间差异较大，在实际应用中需要平衡优化质量和时间成本。\n")
                
                f.write("3. **算法选择建议**:\n")
                f.write(f"   - 追求最优解质量: 推荐使用 **{best_cost_algo}**\n")
                f.write(f"   - 追求快速求解: 推荐使用 **{fastest_algo}**\n")
                f.write(f"   - 平衡质量和效率: 推荐使用 **{best_efficiency_algo}**\n")
        
        logger.info(f"详细对比报告已生成: {report_path}")
        return str(report_path)
    
    def _get_node_area(self, node_name: str) -> float:
        """
        获取节点（科室/槽位）的面积
        
        Args:
            node_name: 节点名称
            
        Returns:
            float: 节点面积
        """
        if self.cache_manager is None:
            return 0.0
        
        # 从cache_manager获取面积信息
        placeable_df = self.cache_manager.placeable_nodes_df
        area_data = placeable_df[placeable_df['node_id'] == node_name]['area']
        
        if not area_data.empty:
            return float(area_data.iloc[0])
        else:
            logger.warning(f"未找到节点 {node_name} 的面积信息")
            return 0.0
    
    def _get_layout_details(self, layout: List[str]) -> List[Dict[str, Any]]:
        """
        获取布局的详细信息，包括每个位置的面积信息
        
        Args:
            layout: 布局列表
            
        Returns:
            List[Dict]: 布局详细信息列表
        """
        details = []
        
        # 获取槽位列表（在这个系统中，槽位和科室是相同的）
        slots = self.cache_manager.placeable_slots if self.cache_manager else layout
        
        for i, department in enumerate(layout):
            if i < len(slots):
                slot = slots[i]
                slot_area = self._get_node_area(slot)
                dept_area = self._get_node_area(department)
                
                # 计算面积匹配情况
                area_diff = abs(slot_area - dept_area)
                if dept_area > 0:
                    match_ratio = min(slot_area, dept_area) / max(slot_area, dept_area)
                else:
                    match_ratio = 0.0
                
                detail = {
                    'position': i,
                    'slot': slot,
                    'slot_area': slot_area,
                    'department': department,
                    'department_area': dept_area,
                    'area_difference': area_diff,
                    'area_match_ratio': match_ratio
                }
                details.append(detail)
        
        return details
    
    def _calculate_area_statistics(self, layout_details: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        计算面积匹配的统计信息
        
        Args:
            layout_details: 布局详细信息
            
        Returns:
            Dict: 面积统计信息
        """
        if not layout_details:
            return {}
        
        perfect_matches = 0  # 完全匹配（比率=1.0）
        good_matches = 0     # 良好匹配（比率>=0.9）
        poor_matches = 0     # 较差匹配（比率<0.9）
        total_ratio = 0.0
        
        for detail in layout_details:
            ratio = detail['area_match_ratio']
            total_ratio += ratio
            
            if ratio >= 0.99:  # 考虑浮点误差
                perfect_matches += 1
            elif ratio >= 0.9:
                good_matches += 1
            else:
                poor_matches += 1
        
        avg_ratio = total_ratio / len(layout_details) if layout_details else 0.0
        
        return {
            'perfect_matches': perfect_matches,
            'good_matches': good_matches,
            'poor_matches': poor_matches,
            'average_match_ratio': avg_ratio,
            'total_positions': len(layout_details)
        }
    
    def export_layouts_comparison(self, output_dir: str = "./results/layouts"):
        """
        导出所有算法的最优布局进行可视化对比，包含面积信息
        
        Args:
            output_dir: 输出目录
        """
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
        
        layouts_data = {}
        
        # 添加原始布局（所有算法应该有相同的原始布局）
        first_result = next(iter(self.results.values()))
        if first_result.original_layout is not None:
            original_details = self._get_layout_details(first_result.original_layout)
            original_stats = self._calculate_area_statistics(original_details)
            
            layouts_data['original'] = {
                'layout': first_result.original_layout,
                'cost': first_result.original_cost,
                'description': '原始布局（未经优化的基准）',
                'layout_details': original_details,
                'area_statistics': original_stats
            }
        
        # 添加各算法的最优布局
        for algorithm_name, result in self.results.items():
            improvement = None
            if result.original_cost is not None and result.original_cost > 0:
                improvement = ((result.original_cost - result.best_cost) / result.original_cost) * 100
            
            # 获取布局详细信息
            layout_details = self._get_layout_details(result.best_layout)
            area_stats = self._calculate_area_statistics(layout_details)
            
            layouts_data[algorithm_name] = {
                'initial_layout': result.original_layout,  # 原始布局（用于对比）
                'best_layout': result.best_layout,
                'initial_cost': result.original_cost,
                'best_cost': result.best_cost,
                'improvement': improvement,  # 相对于原始布局的改进百分比
                'algorithm': result.algorithm_name,
                'iterations': result.iterations,
                'execution_time': result.execution_time,
                'layout_details': layout_details,  # 新增：布局详细信息
                'area_statistics': area_stats      # 新增：面积统计信息
            }
        
        # 保存为JSON文件
        layouts_path = output_path / f"best_layouts_{timestamp}.json"
        with open(layouts_path, 'w', encoding='utf-8') as f:
            json.dump(layouts_data, f, indent=2, ensure_ascii=False)
        
        logger.info(f"最优布局对比数据（含面积信息）已保存到: {layouts_path}")
        
        return str(layouts_path)
</file>

<file path="src/network/graph_manager.py">
"""简化的图管理器，用于存储和管理网络图中的节点和边"""

import itertools
import networkx as nx
from src.rl_optimizer.utils.setup import setup_logger
from typing import Dict, Optional, Iterator, List, Tuple, Any

from .node import Node

logger = setup_logger(__name__)


class GraphManager:
    """
    管理网络图结构、节点存储和ID生成
    
    提供统一的接口来创建、存储、查询节点，并维护NetworkX图结构
    """
    
    def __init__(self, id_start: int = 1):
        """
        初始化图管理器
        
        Args:
            id_start: 节点ID的起始值，默认为1
        """
        self._nodes: Dict[int, Node] = {}
        self._graph = nx.Graph()
        self._id_counter = itertools.count(start=id_start)
        
        logger.debug(f"GraphManager 初始化完成，起始ID: {id_start}")
    
    def get_next_id(self) -> int:
        """获取下一个可用的节点ID"""
        return next(self._id_counter)
    
    def generate_node_id(self) -> int:
        """
        生成新的节点ID（get_next_id的别名）
        
        Returns:
            新的唯一节点ID
        """
        return self.get_next_id()
    
    def get_current_id_value(self) -> int:
        """
        获取当前ID计数器的值（不消耗ID）
        
        Returns:
            当前的ID值（下一个将被分配的ID）
        """
        # 创建计数器的副本以获取当前值
        temp_counter = itertools.tee(self._id_counter, 1)[0]
        current_value = next(temp_counter)
        return current_value
    
    def add_node(self, node: Node) -> None:
        """
        添加节点到图中
        
        Args:
            node: 要添加的节点对象
        """
        self._nodes[node.id] = node
        # 添加节点到NetworkX图中，使用节点ID作为图节点
        # 将完整的Node对象作为属性存储
        self._graph.add_node(
            node.id,
            node_obj=node,  # 存储完整的Node对象
            # 冗余存储一些常用属性便于快速访问
            pos=(node.x, node.y, node.z),
            type=node.node_type,
            name=node.name,
            time=node.time
        )
        
        logger.debug(f"添加节点: {node}")
    
    def add_edge(self, node1_id: int, node2_id: int, weight: float = 1.0) -> None:
        """
        在两个节点之间添加边
        
        Args:
            node1_id: 第一个节点ID
            node2_id: 第二个节点ID
            weight: 边的权重（通常是距离或时间）
        """
        if node1_id in self._nodes and node2_id in self._nodes:
            self._graph.add_edge(node1_id, node2_id, weight=weight)
            logger.debug(f"添加边: {node1_id} -> {node2_id} (权重: {weight})")
        else:
            logger.warning(f"尝试连接不存在的节点: {node1_id}, {node2_id}")
    
    def connect_nodes_by_ids(self, node1_id: int, node2_id: int, weight: float = 1.0) -> None:
        """
        通过ID连接两个节点（add_edge的别名）
        
        Args:
            node1_id: 第一个节点ID
            node2_id: 第二个节点ID
            weight: 边的权重
        """
        self.add_edge(node1_id, node2_id, weight)
    
    def get_node(self, node_id: int) -> Optional[Node]:
        """
        根据ID获取节点
        
        Args:
            node_id: 节点ID
            
        Returns:
            节点对象，如果不存在则返回None
        """
        return self._nodes.get(node_id)
    
    def get_node_by_id(self, node_id: int) -> Optional[Node]:
        """
        根据ID获取节点（get_node的别名）
        
        Args:
            node_id: 节点ID
            
        Returns:
            节点对象，如果不存在则返回None
        """
        return self.get_node(node_id)
    
    def get_all_nodes(self) -> Dict[int, Node]:
        """获取所有节点"""
        return self._nodes.copy()
    
    def get_nodes_by_type(self, node_type: str) -> List[Node]:
        """
        根据类型获取节点列表
        
        Args:
            node_type: 节点类型
            
        Returns:
            指定类型的节点列表
        """
        return [node for node in self._nodes.values() if node.node_type == node_type]
    
    def get_graph(self) -> nx.Graph:
        """获取NetworkX图对象"""
        return self._graph
    
    def get_graph_copy(self) -> nx.Graph:
        """获取NetworkX图对象的副本"""
        return self._graph.copy()
    
    def get_next_available_node_id_estimate(self) -> int:
        """
        获取下一个可用节点ID的估计值
        
        Returns:
            下一个将被分配的节点ID
        """
        return self.get_current_id_value()
    
    def node_count(self) -> int:
        """获取节点总数"""
        return len(self._nodes)
    
    def edge_count(self) -> int:
        """获取边总数"""
        return self._graph.number_of_edges()
    
    def get_node_positions(self) -> Dict[int, Tuple[float, float, float]]:
        """获取所有节点的位置坐标"""
        return {node_id: (node.x, node.y, node.z) for node_id, node in self._nodes.items()}
    
    def clear(self) -> None:
        """清空所有节点和边"""
        self._nodes.clear()
        self._graph.clear()
        logger.debug("GraphManager 已清空")
    
    def __len__(self) -> int:
        """返回节点数量"""
        return len(self._nodes)
    
    def __str__(self) -> str:
        """图管理器的字符串表示"""
        return f"GraphManager(nodes={self.node_count()}, edges={self.edge_count()})"
</file>

<file path="src/rl_optimizer/data/cache/traffic_distribution.json">
{
    "ICU": {
        "ICU_30013": 1.0
    },
    "NICU": {
        "NICU_30012": 1.0
    },
    "中医科": {
        "中医科_10007": 1.0
    },
    "中心供应室": {
        "中心供应室_10003": 1.0
    },
    "产前诊断门诊": {
        "产前诊断门诊_30009": 1.0
    },
    "产房": {
        "产房_30007": 1.0
    },
    "产科": {
        "产科_30006": 1.0
    },
    "介入科": {
        "介入科_20013": 1.0
    },
    "体检科": {
        "体检科_10009": 1.0
    },
    "儿科": {
        "儿科_10006": 1.0
    },
    "全科": {
        "全科_10004": 1.0
    },
    "内诊药房": {
        "内诊药房_10001": 1.0
    },
    "内镜中心": {
        "内镜中心_20002": 1.0
    },
    "口腔一区": {
        "口腔一区_40004": 1.0
    },
    "口腔科二区": {
        "口腔科二区_40008": 1.0
    },
    "呼吸内科": {
        "呼吸内科_20011": 1.0
    },
    "妇科": {
        "妇科_30005": 1.0
    },
    "心血管内科": {
        "心血管内科_20006": 1.0
    },
    "急诊科": {
        "急诊科_1": 1.0
    },
    "手术室": {
        "手术室_40006": 1.0
    },
    "挂号收费": {
        "挂号收费_10002": 1.0,
        "挂号收费_20001": 1.0,
        "挂号收费_30001": 1.0,
        "挂号收费_40001": 1.0
    },
    "放射科": {
        "放射科_10005": 1.0
    },
    "检验中心": {
        "检验中心_20003": 1.0
    },
    "泌尿外科": {
        "泌尿外科_30003": 1.0
    },
    "消化内科": {
        "消化内科_20004": 1.0
    },
    "烧伤整形科": {
        "烧伤整形科_30011": 1.0
    },
    "生殖医学科": {
        "生殖医学科_30010": 1.0
    },
    "甲状腺外科": {
        "甲状腺外科_30002": 1.0
    },
    "病理科": {
        "病理科_20008": 1.0
    },
    "皮肤科": {
        "皮肤科_40005": 1.0
    },
    "眼科": {
        "眼科_40002": 1.0
    },
    "神经内科": {
        "神经内科_20010": 1.0
    },
    "综合激光科": {
        "综合激光科_40009": 1.0
    },
    "耳鼻喉科": {
        "耳鼻喉科_40003": 1.0
    },
    "肝胆胰外科": {
        "肝胆胰外科_30004": 1.0
    },
    "肾内科": {
        "肾内科_20005": 1.0
    },
    "肿瘤科": {
        "肿瘤科_20012": 1.0
    },
    "超声科": {
        "超声科_10008": 1.0
    },
    "透析中心": {
        "透析中心_30008": 1.0
    },
    "采血处": {
        "采血处_20007": 1.0
    },
    "门": {
        "门_11072": 0.046019042,
        "门_11083": 0.182080284,
        "门_11086": 0.046019042,
        "门_11087": 0.046019042,
        "门_11093": 0.182080284,
        "门_11094": 0.046019042,
        "门_11112": 0.037939511,
        "门_11113": 0.081614043,
        "门_11116": 0.081614043,
        "门_11118": 0.037939511,
        "门_11119": 0.081614043,
        "门_11122": 0.037939511,
        "门_11145": 0.063429585,
        "门_11146": 0.053365481,
        "门_11147": 0.063429585,
        "门_11149": 0.053365481,
        "门_11153": 0.063429585,
        "门_11154": 0.053365481,
        "门_11155": 0.063429585,
        "门_11165": 0.063429585
    },
    "门诊手术室": {
        "门诊手术室_40007": 1.0
    },
    "静配中心": {
        "静配中心_40010": 1.0
    },
    "骨科": {
        "骨科_20009": 1.0
    }
}
</file>

<file path="src/rl_optimizer/env/cost_calculator.py">
# src/rl_optimizer/env/cost_calculator.py

import numpy as np
import pandas as pd
from scipy.sparse import dok_matrix, csr_matrix
from typing import List, Dict, Tuple, Any
from collections import defaultdict, OrderedDict
import itertools

from src.config import RLConfig
from src.rl_optimizer.utils.setup import setup_logger, load_pickle, save_pickle

logger = setup_logger(__name__)

class LRUCache:
    """
    简单的LRU缓存实现，用于限制缓存大小
    """
    def __init__(self, max_size: int = 1000):
        self.cache = OrderedDict()
        self.max_size = max_size
    
    def get(self, key: tuple) -> Any:
        """获取缓存值，并将其移到最近使用位置"""
        if key not in self.cache:
            return None
        # 移到最近使用位置
        self.cache.move_to_end(key)
        return self.cache[key]
    
    def put(self, key: tuple, value: Any):
        """设置缓存值"""
        if key in self.cache:
            # 如果已存在，移到最近使用位置
            self.cache.move_to_end(key)
        else:
            # 如果缓存已满，删除最久未使用的项
            if len(self.cache) >= self.max_size:
                self.cache.popitem(last=False)
            self.cache[key] = value
    
    def clear(self):
        """清空缓存"""
        self.cache.clear()
    
    def size(self) -> int:
        """返回缓存大小"""
        return len(self.cache)


class CostCalculator:
    """
    一个高效的成本计算器，用于评估给定布局下的加权总通行时间。
    它通过预计算一个稀疏矩阵来实现高性能计算。
    """

    def __init__(self, 
                 config: RLConfig, 
                 resolved_pathways: List[Dict[str, Any]], 
                 travel_times: pd.DataFrame, 
                 placeable_slots: List[str],
                 placeable_departments: List[str]):
        """
        初始化成本计算器。

        Args:
            config (RLConfig): RL优化器的配置对象。
            resolved_pathways (List[Dict]): 已解析的流线字典列表。
            travel_times (pd.DataFrame): 纯通行时间矩阵，其行列为原始节点名。
            placeable_slots (List[str]): 可用物理槽位的原始节点名称列表。
            placeable_departments (List[str]): 所有需要被布局的科室的完整列表。
        """
        self.config = config
        self.travel_times = travel_times
        self.max_taravel_time = travel_times.values.max()
        self.placeable_slots = placeable_slots
        self.num_slots = len(placeable_slots)
        
        self.dept_list = sorted(placeable_departments)
        self.pair_to_col, self.num_dept_pairs = self._create_dept_pair_mapping()
        
        self.M, self.W, self.pathway_to_process_id = self._build_cost_matrices(resolved_pathways)
        
        # 初始化LRU缓存，限制缓存大小以防止内存泄漏
        cache_size = getattr(config, 'COST_CACHE_SIZE', 1000)
        self._cost_cache = LRUCache(max_size=cache_size)
        self._time_vector_cache = LRUCache(max_size=cache_size // 2)
        
        logger.info(f"成本计算器初始化完成，LRU缓存大小: {cache_size}")

    def _create_dept_pair_mapping(self) -> Tuple[Dict[Tuple[str, str], int], int]:
        """创建科室对到列索引的映射。"""
        dept_pairs = list(itertools.permutations(self.dept_list, 2))
        return {pair: i for i, pair in enumerate(dept_pairs)}, len(dept_pairs)
    
    def _build_cost_matrices(self, pathways: List[Dict[str, Any]]) -> Tuple[csr_matrix, np.ndarray, List[str]]:
        """构建成本矩阵。"""
        num_pathways = len(pathways)
        pathway_to_process_id = [p['process_id'] for p in pathways]
        
        M_dok = dok_matrix((num_pathways, self.num_dept_pairs), dtype=np.float32)
        W_arr = np.zeros(num_pathways, dtype=np.float32)

        for i, p_data in enumerate(pathways):
            path = p_data['path']
            W_arr[i] = p_data['weight']
            for j in range(len(path) - 1):
                pair = (path[j], path[j+1])
                if pair in self.pair_to_col:
                    M_dok[i, self.pair_to_col[pair]] += 1
        
        return M_dok.tocsr(), W_arr, pathway_to_process_id
    
    def calculate_total_cost(self, layout: List[str]) -> float:
        """
        给定一个布局，高效计算总加权通行时间（使用LRU缓存优化）。

        Args:
            layout (List[str]): 一个表示当前布局的科室名称列表。
                               其索引对应于 self.placeable_nodes 的索引。

        Returns:
            float: 计算出的总加权成本。
        """
        # 创建布局的元组作为缓存键
        layout_key = tuple(layout)
        
        # 检查缓存
        cached_cost = self._cost_cache.get(layout_key)
        if cached_cost is not None:
            return cached_cost
        
        # 计算成本
        time_vector = self._get_time_vector(layout)
        time_per_pathway = self.M.dot(time_vector)
        total_cost = time_per_pathway.dot(self.W)
        total_cost = float(total_cost)
        
        # 存入缓存
        self._cost_cache.put(layout_key, total_cost)
        
        return total_cost
    
    def calculate_per_process_cost(self, layout: List[str]) -> Dict[str, float]:
        """
        评估每个就医流程模板的单独通行时间（未加权）。
        """
        time_vector = self._get_time_vector(layout)
        time_per_pathway = self.M.dot(time_vector)
        
        process_costs = defaultdict(float)
        # 遍历所有流线，将它们的时间累加到对应的流程模板上
        for i, process_id in enumerate(self.pathway_to_process_id):
            process_costs[process_id] += time_per_pathway[i]
            
        return dict(process_costs)
    
    def _get_time_vector(self, layout: List[str]) -> np.ndarray:
        """
        内部辅助函数，根据布局计算所有科室对的通行时间向量（使用LRU缓存优化）。
        
        Args:
            layout: 科室名称列表。可以是：
                   1. 完整布局（长度=槽位数，包含None）
                   2. 仅已放置科室的列表（不包含None）
        """
        # 创建布局的元组作为缓存键
        layout_key = tuple(layout)
        
        # 检查缓存
        cached_vector = self._time_vector_cache.get(layout_key)
        if cached_vector is not None:
            return cached_vector
        
        time_vector = np.zeros(self.num_dept_pairs, dtype=np.float32)

        # 判断layout是完整布局还是仅已放置科室
        if len(layout) == self.num_slots:
            # 完整布局：索引对应槽位
            dept_to_slot_node = {dept: self.placeable_slots[i] for i, dept in enumerate(layout) if dept is not None}
        else:
            # 仅已放置科室：需要为每个科室分配一个虚拟槽位
            # 使用科室名本身作为节点名（简化处理）
            dept_to_slot_node = {dept: dept for dept in layout if dept is not None}

        # 遍历所有需要计算时间的科室对
        for pair, col_idx in self.pair_to_col.items():
            dept_from, dept_to = pair
            
            # 查找科室所在的原始节点名
            node_from = dept_to_slot_node.get(dept_from)
            node_to = dept_to_slot_node.get(dept_to)
            
            try:
                time_vector[col_idx] = self.travel_times.loc[node_from, node_to]
            except KeyError as e:
                # 错误处理：如果找不到节点对，使用默认值
                default_penalty = self.max_taravel_time
                logger.debug(f"找不到节点对 {node_from}->{node_to} 的行程时间，使用最大通行值{default_penalty}")
                time_vector[col_idx] = default_penalty
        
        # 存入缓存
        self._time_vector_cache.put(layout_key, time_vector)
        
        return time_vector
    
    def clear_cache(self):
        """清空所有缓存"""
        self._cost_cache.clear()
        self._time_vector_cache.clear()
        logger.info("成本计算器缓存已清空")
    
    def get_cache_stats(self) -> Dict[str, int]:
        """获取缓存统计信息"""
        return {
            'cost_cache_size': self._cost_cache.size(),
            'time_vector_cache_size': self._time_vector_cache.size()
        }
</file>

<file path="src/algorithms/genetic_algorithm.py">
"""
遗传算法优化器 - 基于遗传算法的布局优化
"""

import random
import copy
import numpy as np
from src.rl_optimizer.utils.setup import setup_logger
from typing import List, Optional, Dict, Any, Tuple, Set
from dataclasses import dataclass

from src.algorithms.base_optimizer import BaseOptimizer, OptimizationResult
from src.algorithms.constraint_manager import ConstraintManager, SmartConstraintRepairer
from src.rl_optimizer.env.cost_calculator import CostCalculator

logger = setup_logger(__name__)


@dataclass
class Individual:
    """遗传算法中的个体"""
    layout: List[str]
    fitness: float = float('inf')
    age: int = 0
    
    def __post_init__(self):
        if self.fitness == float('inf'):
            # 如果没有设置适应度，则需要外部计算
            pass


class GeneticAlgorithmOptimizer(BaseOptimizer):
    """
    约束感知遗传算法优化器
    
    使用约束感知的遗传算法进行布局优化。集成了智能约束修复器、
    FPX交叉算子、约束导向变异等先进技术，大幅提升约束满足度和优化效果。
    
    主要特性：
    1. 智能初始种群生成（贪心面积匹配 + 随机多样性）
    2. FPX（Fixed Position Crossover）约束感知交叉算子
    3. 约束导向的智能变异策略
    4. 多策略约束修复机制
    5. 动态参数调整和种群多样性维护
    """
    
    def __init__(self, 
                 cost_calculator: CostCalculator,
                 constraint_manager: ConstraintManager,
                 population_size: int = 100,
                 elite_size: int = 20,
                 mutation_rate: float = 0.15,
                 crossover_rate: float = 0.85,
                 tournament_size: int = 5,
                 max_age: int = 50,
                 constraint_repair_strategy: str = 'greedy_area_matching',
                 adaptive_parameters: bool = True):
        """
        初始化约束感知遗传算法优化器
        
        Args:
            cost_calculator: 成本计算器
            constraint_manager: 约束管理器
            population_size: 种群大小
            elite_size: 精英个体数量
            mutation_rate: 初始变异率
            crossover_rate: 初始交叉率
            tournament_size: 锦标赛选择的参与者数量
            max_age: 个体最大年龄
            constraint_repair_strategy: 约束修复策略
            adaptive_parameters: 是否启用自适应参数调整
        """
        super().__init__(cost_calculator, constraint_manager, "ConstraintAwareGeneticAlgorithm")
        
        self.population_size = population_size
        self.elite_size = elite_size
        self.mutation_rate = mutation_rate
        self.crossover_rate = crossover_rate
        self.tournament_size = tournament_size
        self.max_age = max_age
        self.constraint_repair_strategy = constraint_repair_strategy
        self.adaptive_parameters = adaptive_parameters
        
        # 初始化智能约束修复器
        self.constraint_repairer = SmartConstraintRepairer(constraint_manager)
        
        # 算法状态
        self.population = []
        self.generation = 0
        self.stagnation_count = 0
        self.best_fitness_history = []
        self.diversity_history = []
        
        # 自适应参数历史
        self.mutation_rate_history = []
        self.crossover_rate_history = []
        
        # 约束违反统计
        self.constraint_violation_count = 0
        self.successful_repair_count = 0
        
        # 性能统计
        self.fpx_crossover_count = 0
        self.smart_mutation_count = 0
        self.greedy_initialization_count = 0
        
        logger.info(f"约束感知遗传算法优化器初始化完成:")
        logger.info(f"  种群大小: {population_size}")
        logger.info(f"  精英数量: {elite_size}")
        logger.info(f"  初始变异率: {mutation_rate}")
        logger.info(f"  初始交叉率: {crossover_rate}")
        logger.info(f"  约束修复策略: {constraint_repair_strategy}")
        logger.info(f"  自适应参数: {adaptive_parameters}")
    
    def optimize(self, 
                 initial_layout: Optional[List[str]] = None,
                 max_iterations: int = 1000,
                 convergence_threshold: int = 50,
                 original_layout: Optional[List[str]] = None,
                 original_cost: Optional[float] = None,
                 **kwargs) -> OptimizationResult:
        """
        执行遗传算法优化
        
        Args:
            initial_layout: 初始布局（用于种群初始化的种子）
            max_iterations: 最大代数
            convergence_threshold: 收敛阈值（连续多少代无改进则停止）
            original_layout: 原始布局（未经优化的基准）
            original_cost: 原始布局的成本
            **kwargs: 其他参数
            
        Returns:
            OptimizationResult: 优化结果
        """
        self.start_optimization()
        
        # 保存原始布局信息
        self.original_layout = original_layout
        self.original_cost = original_cost
        
        logger.info(f"遗传算法优化开始，最大代数: {max_iterations}")
        
        # 初始化种群
        self._initialize_population(initial_layout)
        
        # 评估初始种群
        self._evaluate_population()
        
        # 更新最优解
        best_individual = min(self.population, key=lambda x: x.fitness)
        self.update_best_solution(best_individual.layout, best_individual.fitness)
        self.best_fitness_history.append(best_individual.fitness)
        
        logger.info(f"初始种群最优适应度: {best_individual.fitness:.2f}")
        
        # 重置算法状态
        self.generation = 0
        self.stagnation_count = 0
        
        try:
            while (self.generation < max_iterations and 
                   self.stagnation_count < convergence_threshold):
                
                # 自适应参数调整
                if self.generation > 0:
                    self._adapt_parameters()
                
                # 执行一代进化
                self._evolve_generation()
                
                # 评估种群
                self._evaluate_population()
                
                # 更新最优解
                current_best = min(self.population, key=lambda x: x.fitness)
                previous_best = self.best_cost
                
                if current_best.fitness < self.best_cost:
                    self.update_best_solution(current_best.layout, current_best.fitness)
                    self.stagnation_count = 0
                    logger.info(f"第{self.generation}代发现更优解: {current_best.fitness:.2f}")
                else:
                    self.stagnation_count += 1
                
                self.best_fitness_history.append(current_best.fitness)
                self.generation += 1
                self.current_iteration = self.generation
                
                # 记录进化信息
                avg_fitness = sum(ind.fitness for ind in self.population) / len(self.population)
                self.log_iteration(current_best.fitness, 
                                 f"avg={avg_fitness:.2f}, stagnation={self.stagnation_count}, "
                                 f"diversity={self._calculate_diversity():.3f}")
                
                if self.generation % 50 == 0:
                    logger.info(f"第{self.generation}代: "
                              f"最优={current_best.fitness:.2f}, "
                              f"平均={avg_fitness:.2f}, "
                              f"停滞={self.stagnation_count}, "
                              f"多样性={self._calculate_diversity():.3f}")
                
                # 种群多样性维护
                if self.generation % 100 == 0:
                    self._maintain_diversity()
                
                # 更新约束统计信息
                if self.generation % 20 == 0:
                    self._update_constraint_statistics()
                    
        except KeyboardInterrupt:
            logger.warning("约束感知遗传算法优化被用户中断")
        
        logger.info(f"遗传算法优化完成:")
        logger.info(f"  总代数: {self.generation}")
        logger.info(f"  停滞代数: {self.stagnation_count}")
        logger.info(f"  种群多样性: {self._calculate_diversity():.3f}")
        
        return self.finish_optimization()
    
    def _initialize_population(self, seed_layout: Optional[List[str]] = None):
        """
        智能初始化种群
        
        结合贪心面积匹配和随机多样性策略生成高质量的初始种群。
        
        Args:
            seed_layout: 种子布局，用于生成部分个体
        """
        self.population = []
        
        # 1. 如果有种子布局，将其作为第一个个体
        if seed_layout is not None:
            if self.constraint_manager.is_valid_layout(seed_layout):
                self.population.append(Individual(layout=seed_layout.copy()))
                logger.debug("添加有效种子布局到种群")
            else:
                # 修复种子布局
                repaired_seed = self.constraint_repairer.repair_layout(seed_layout, self.constraint_repair_strategy)
                self.population.append(Individual(layout=repaired_seed))
                logger.debug("修复并添加种子布局到种群")
        
        # 2. 生成贪心面积匹配个体（种群的30%）
        greedy_count = min(int(self.population_size * 0.3), self.population_size - len(self.population))
        for _ in range(greedy_count):
            greedy_layout = self._generate_greedy_area_matched_layout()
            self.population.append(Individual(layout=greedy_layout))
            self.greedy_initialization_count += 1
        
        # 3. 基于种子布局生成变异个体（如果有种子布局，占种群的20%）
        if seed_layout is not None and len(self.population) < self.population_size:
            variant_count = min(int(self.population_size * 0.2), self.population_size - len(self.population))
            for _ in range(variant_count):
                variant_layout = seed_layout.copy()
                self._smart_mutate_layout(variant_layout)
                variant_layout = self.constraint_repairer.repair_layout(variant_layout, self.constraint_repair_strategy)
                self.population.append(Individual(layout=variant_layout))
        
        # 4. 生成约束感知的随机个体（填充剩余位置）
        while len(self.population) < self.population_size:
            random_layout = self._generate_constraint_aware_random_layout()
            self.population.append(Individual(layout=random_layout))
        
        logger.info(f"智能初始化种群完成:")
        logger.info(f"  总大小: {len(self.population)}")
        logger.info(f"  贪心个体: {self.greedy_initialization_count}")
        logger.info(f"  约束修复次数: {self.constraint_repairer.repair_attempts}")
    
    def _generate_greedy_area_matched_layout(self) -> List[str]:
        """
        生成基于贪心面积匹配的布局
        
        使用贪心算法优先分配面积匹配度最高的科室-槽位对。
        
        Returns:
            List[str]: 贪心面积匹配布局
        """
        import numpy as np
        
        # 获取所有科室和槽位
        departments = list(self.constraint_manager.placeable_departments)
        slots = list(range(len(self.constraint_manager.slots_info)))
        
        # 计算面积匹配度矩阵
        match_scores = np.zeros((len(slots), len(departments)))
        
        for slot_idx in slots:
            slot_area = self.constraint_manager.slots_info[slot_idx].area
            for dept_idx, dept_name in enumerate(departments):
                dept_area = self.constraint_manager.departments_info[dept_idx].area_requirement
                
                # 计算面积匹配度（值越大表示匹配度越高）
                if max(slot_area, dept_area) > 0:
                    area_ratio = min(slot_area, dept_area) / max(slot_area, dept_area)
                    # 结合约束兼容性
                    if self.constraint_manager.area_compatibility_matrix[slot_idx, dept_idx]:
                        match_scores[slot_idx, dept_idx] = area_ratio
                    else:
                        match_scores[slot_idx, dept_idx] = 0.0
                else:
                    match_scores[slot_idx, dept_idx] = 0.0
        
        # 贪心分配
        layout = [''] * len(slots)
        used_departments = set()
        used_slots = set()
        
        # 处理固定位置约束
        for dept_name, fixed_slot_idx in self.constraint_manager.fixed_assignments.items():
            if dept_name in departments and fixed_slot_idx in slots:
                layout[fixed_slot_idx] = dept_name
                used_departments.add(dept_name)
                used_slots.add(fixed_slot_idx)
        
        # 创建候选列表并按匹配度排序
        candidates = []
        for slot_idx in slots:
            if slot_idx not in used_slots:
                for dept_idx, dept_name in enumerate(departments):
                    if dept_name not in used_departments:
                        score = match_scores[slot_idx, dept_idx]
                        candidates.append((score, slot_idx, dept_name))
        
        # 按匹配度降序排序
        candidates.sort(key=lambda x: x[0], reverse=True)
        
        # 贪心分配
        for score, slot_idx, dept_name in candidates:
            if slot_idx not in used_slots and dept_name not in used_departments:
                layout[slot_idx] = dept_name
                used_departments.add(dept_name)
                used_slots.add(slot_idx)
        
        return layout
    
    def _generate_constraint_aware_random_layout(self) -> List[str]:
        """
        生成约束感知的随机布局
        
        在随机分配的基础上，考虑约束兼容性，提高初始布局的有效性。
        
        Returns:
            List[str]: 约束感知的随机布局
        """
        # 首先生成一个基本的随机布局
        departments = list(self.constraint_manager.placeable_departments)
        random.shuffle(departments)
        
        layout = departments.copy()
        
        # 尝试改善约束违反
        max_improvement_attempts = 10
        for _ in range(max_improvement_attempts):
            violations = []
            
            # 检查面积约束违反
            for slot_idx, dept_name in enumerate(layout):
                dept_idx = self.constraint_manager.dept_name_to_index.get(dept_name)
                if (dept_idx is not None and 
                    not self.constraint_manager.area_compatibility_matrix[slot_idx, dept_idx]):
                    violations.append(slot_idx)
            
            if not violations:
                break
            
            # 随机选择一个违反位置进行改善
            violation_slot = random.choice(violations)
            compatible_depts = self.constraint_manager.get_compatible_departments(violation_slot)
            
            if compatible_depts:
                # 寻找可以交换的兼容科室
                for target_dept in compatible_depts:
                    if target_dept in layout:
                        target_slot = layout.index(target_dept)
                        # 检查交换是否改善整体约束满足度
                        if self.constraint_manager.get_swap_candidates(layout, violation_slot, target_slot):
                            layout[violation_slot], layout[target_slot] = layout[target_slot], layout[violation_slot]
                            break
        
        # 如果仍有约束违反，使用约束修复器
        if not self.constraint_manager.is_valid_layout(layout):
            repair_strategy = np.random.choice(['greedy_area_matching', 'swap_optimization', 'random_repair'])
            layout = self.constraint_repairer.repair_layout(layout, repair_strategy, max_attempts=5)
        
        return layout
    
    def _evaluate_population(self):
        """评估种群中所有个体的适应度"""
        for individual in self.population:
            if individual.fitness == float('inf'):  # 只计算未评估的个体
                individual.fitness = self.evaluate_layout(individual.layout)
    
    def _evolve_generation(self):
        """执行一代进化"""
        new_population = []
        
        # 1. 精英保留
        elite_individuals = self._select_elite()
        new_population.extend(elite_individuals)
        
        # 2. 生成新个体
        while len(new_population) < self.population_size:
            if random.random() < self.crossover_rate:
                # 交叉生成后代
                parent1 = self._tournament_selection()
                parent2 = self._tournament_selection()
                child1, child2 = self._crossover(parent1, parent2)
                
                # 变异
                if random.random() < self.mutation_rate:
                    self._mutate_individual(child1)
                if random.random() < self.mutation_rate:
                    self._mutate_individual(child2)
                
                new_population.extend([child1, child2])
            else:
                # 复制并变异
                parent = self._tournament_selection()
                child = Individual(layout=parent.layout.copy())
                if random.random() < self.mutation_rate:
                    self._mutate_individual(child)
                new_population.append(child)
        
        # 3. 更新年龄并移除过老个体
        new_population = self._age_and_filter_population(new_population)
        
        # 4. 裁剪到目标大小
        if len(new_population) > self.population_size:
            new_population = new_population[:self.population_size]
        
        self.population = new_population
    
    def _select_elite(self) -> List[Individual]:
        """选择精英个体"""
        # 按适应度排序并选择最优的
        sorted_pop = sorted(self.population, key=lambda x: x.fitness)
        elite = []
        
        for individual in sorted_pop[:self.elite_size]:
            elite_copy = Individual(
                layout=individual.layout.copy(),
                fitness=individual.fitness,
                age=individual.age + 1
            )
            elite.append(elite_copy)
        
        return elite
    
    def _tournament_selection(self) -> Individual:
        """锦标赛选择"""
        tournament = random.sample(self.population, 
                                 min(self.tournament_size, len(self.population)))
        return min(tournament, key=lambda x: x.fitness)
    
    def _crossover(self, parent1: Individual, parent2: Individual) -> Tuple[Individual, Individual]:
        """
        FPX (Fixed Position Crossover) 约束感知交叉操作
        
        这是一种专门为约束布局问题设计的交叉算子，能够：
        1. 保持固定位置约束不变
        2. 在非固定位置进行智能信息交换
        3. 自动修复可能的约束违反
        
        Args:
            parent1: 父代1
            parent2: 父代2
            
        Returns:
            Tuple[Individual, Individual]: 两个子代
        """
        self.fpx_crossover_count += 1
        
        layout1, layout2 = parent1.layout.copy(), parent2.layout.copy()
        
        # 使用FPX算法进行约束感知交叉
        child1_layout, child2_layout = self._fpx_crossover(layout1, layout2)
        
        # 修复可能的约束违反
        child1_layout = self.constraint_repairer.repair_layout(child1_layout, self.constraint_repair_strategy, max_attempts=3)
        child2_layout = self.constraint_repairer.repair_layout(child2_layout, self.constraint_repair_strategy, max_attempts=3)
        
        child1 = Individual(layout=child1_layout)
        child2 = Individual(layout=child2_layout)
        
        return child1, child2
    
    def _fpx_crossover(self, parent1: List[str], parent2: List[str]) -> Tuple[List[str], List[str]]:
        """
        Fixed Position Crossover (FPX) 算法实现
        
        FPX是专门为带固定位置约束的布局问题设计的交叉算子：
        1. 固定位置的科室保持不变
        2. 在非固定位置间进行有序交叉
        3. 使用启发式修复策略处理冲突
        
        Args:
            parent1: 父代1布局
            parent2: 父代2布局
            
        Returns:
            Tuple[List[str], List[str]]: 交叉后的两个子代布局
        """
        length = len(parent1)
        
        # 初始化子代
        child1 = [''] * length
        child2 = [''] * length
        
        # 1. 保持固定位置约束
        fixed_positions = set()
        for dept_name, slot_idx in self.constraint_manager.fixed_assignments.items():
            if slot_idx < length:
                child1[slot_idx] = dept_name
                child2[slot_idx] = dept_name
                fixed_positions.add(slot_idx)
        
        # 2. 获取非固定位置
        non_fixed_positions = [i for i in range(length) if i not in fixed_positions]
        
        if len(non_fixed_positions) <= 1:
            # 如果非固定位置太少，直接复制父代
            return parent1.copy(), parent2.copy()
        
        # 3. 在非固定位置执行改进的顺序交叉
        # 随机选择交叉区间
        start_idx = random.randint(0, len(non_fixed_positions) - 1)
        end_idx = random.randint(start_idx, len(non_fixed_positions) - 1)
        
        # 交叉区间的实际位置
        crossover_positions = non_fixed_positions[start_idx:end_idx + 1]
        
        # 复制交叉区间
        for pos in crossover_positions:
            child1[pos] = parent1[pos]
            child2[pos] = parent2[pos]
        
        # 4. 填充剩余位置（使用改进的填充策略）
        self._fill_remaining_positions_fpx(child1, parent2, crossover_positions, non_fixed_positions, fixed_positions)
        self._fill_remaining_positions_fpx(child2, parent1, crossover_positions, non_fixed_positions, fixed_positions)
        
        return child1, child2
    
    def _fill_remaining_positions_fpx(self, 
                                     child: List[str], 
                                     parent: List[str], 
                                     crossover_positions: List[int],
                                     non_fixed_positions: List[int],
                                     fixed_positions: Set[int]):
        """
        FPX交叉算子的智能填充策略
        
        使用约束感知的填充方法，优先考虑面积兼容性。
        
        Args:
            child: 待填充的子代
            parent: 参考的父代
            crossover_positions: 已交叉的位置
            non_fixed_positions: 所有非固定位置
            fixed_positions: 所有固定位置
        """
        # 获取已在子代中的科室
        child_departments = set(dept for dept in child if dept and dept != '')
        
        # 获取需要填充的位置
        positions_to_fill = [pos for pos in non_fixed_positions if pos not in crossover_positions]
        
        # 从父代中按顺序提取未在子代中的科室
        available_departments = []
        for dept in parent:
            if dept and dept != '' and dept not in child_departments:
                available_departments.append(dept)
                child_departments.add(dept)  # 立即添加以避免重复
        
        # 如果可用科室不足，从所有科室中补充
        all_departments = set(self.constraint_manager.placeable_departments)
        missing_departments = all_departments - child_departments
        available_departments.extend(list(missing_departments))
        
        # 智能分配：优先考虑面积兼容性
        for i, pos in enumerate(positions_to_fill):
            if i < len(available_departments):
                dept_to_assign = available_departments[i]
                
                # 检查是否有更好的面积匹配选择
                if i + 1 < len(available_departments):
                    current_dept = available_departments[i]
                    next_dept = available_departments[i + 1]
                    
                    # 比较面积匹配度
                    current_match = self._calculate_area_match_score(pos, current_dept)
                    next_match = self._calculate_area_match_score(pos, next_dept)
                    
                    # 如果下一个科室的匹配度明显更好，进行交换
                    if next_match > current_match + 0.1:  # 0.1是匹配度改善阈值
                        available_departments[i], available_departments[i + 1] = \
                            available_departments[i + 1], available_departments[i]
                        dept_to_assign = available_departments[i]
                
                child[pos] = dept_to_assign
    
    def _calculate_area_match_score(self, slot_idx: int, dept_name: str) -> float:
        """
        计算科室与槽位的面积匹配度
        
        Args:
            slot_idx: 槽位索引
            dept_name: 科室名称
            
        Returns:
            float: 匹配度得分 (0-1之间，越高越好)
        """
        dept_idx = self.constraint_manager.dept_name_to_index.get(dept_name)
        if dept_idx is None:
            return 0.0
        
        slot_area = self.constraint_manager.slots_info[slot_idx].area
        dept_area = self.constraint_manager.departments_info[dept_idx].area_requirement
        
        if max(slot_area, dept_area) == 0:
            return 0.0
        
        # 面积匹配度：越接近1表示匹配度越高
        area_ratio = min(slot_area, dept_area) / max(slot_area, dept_area)
        
        # 结合约束兼容性
        if self.constraint_manager.area_compatibility_matrix[slot_idx, dept_idx]:
            return area_ratio
        else:
            return 0.0  # 不兼容则匹配度为0
    
    def _smart_mutate_layout(self, layout: List[str]):
        """
        约束导向的智能变异操作
        
        这个变异算子考虑约束条件，优先进行有益的变异：
        1. 识别约束违反位置
        2. 优先修复违反的约束
        3. 在不违反约束的前提下进行多样性变异
        
        Args:
            layout: 待变异的布局
        """
        self.smart_mutation_count += 1
        
        # 1. 检查是否有约束违反
        violations = self.constraint_repairer._find_constraint_violations(layout)
        
        if violations:
            # 优先修复约束违反
            self._constraint_repair_mutation(layout, violations)
        else:
            # 进行多样性导向的变异
            self._diversity_oriented_mutation(layout)
    
    def _constraint_repair_mutation(self, layout: List[str], violations: List[int]):
        """
        约束修复导向的变异
        
        专门针对约束违反位置进行修复性变异。
        
        Args:
            layout: 布局
            violations: 违反约束的位置列表
        """
        # 随机选择一个违反位置进行修复
        violation_pos = random.choice(violations)
        
        # 获取该位置兼容的科室
        compatible_depts = self.constraint_manager.get_compatible_departments(violation_pos)
        
        if compatible_depts:
            # 寻找可以交换的科室
            current_dept = layout[violation_pos]
            
            for candidate_dept in compatible_depts:
                if candidate_dept in layout and candidate_dept != current_dept:
                    candidate_pos = layout.index(candidate_dept)
                    
                    # 检查交换是否有益
                    if self.constraint_manager.get_swap_candidates(layout, violation_pos, candidate_pos):
                        layout[violation_pos], layout[candidate_pos] = layout[candidate_pos], layout[violation_pos]
                        break
        
        # 如果交换修复失败，尝试其他变异策略
        if violation_pos in self.constraint_repairer._find_constraint_violations(layout):
            self._diversity_oriented_mutation(layout)
    
    def _diversity_oriented_mutation(self, layout: List[str]):
        """
        多样性导向的变异
        
        在不违反约束的前提下，进行多种变异操作以增加种群多样性。
        
        Args:
            layout: 布局
        """
        mutation_type = random.choice(['constraint_aware_swap', 'smart_insertion', 'local_scramble'])
        
        if mutation_type == 'constraint_aware_swap':
            self._constraint_aware_swap_mutation(layout)
        elif mutation_type == 'smart_insertion':
            self._smart_insertion_mutation(layout)
        elif mutation_type == 'local_scramble':
            self._local_scramble_mutation(layout)
    
    def _constraint_aware_swap_mutation(self, layout: List[str]):
        """约束感知的交换变异"""
        # 排除固定位置
        non_fixed_positions = [i for i in range(len(layout)) 
                              if i not in self.constraint_manager.fixed_assignments.values()]
        
        if len(non_fixed_positions) >= 2:
            pos1, pos2 = random.sample(non_fixed_positions, 2)
            
            # 只有当交换不违反约束时才执行
            if self.constraint_manager.get_swap_candidates(layout, pos1, pos2):
                layout[pos1], layout[pos2] = layout[pos2], layout[pos1]
    
    def _smart_insertion_mutation(self, layout: List[str]):
        """智能插入变异"""
        non_fixed_positions = [i for i in range(len(layout)) 
                              if i not in self.constraint_manager.fixed_assignments.values()]
        
        if len(non_fixed_positions) >= 2:
            # 选择源位置和目标位置
            source_pos = random.choice(non_fixed_positions)
            target_pos = random.choice(non_fixed_positions)
            
            if source_pos != target_pos:
                # 执行插入操作
                dept = layout.pop(source_pos)
                layout.insert(target_pos, dept)
                
                # 如果插入后违反约束，回滚操作
                if not self.constraint_manager.is_valid_layout(layout):
                    layout.pop(target_pos)
                    layout.insert(source_pos, dept)
    
    def _local_scramble_mutation(self, layout: List[str]):
        """局部乱序变异"""
        non_fixed_positions = [i for i in range(len(layout)) 
                              if i not in self.constraint_manager.fixed_assignments.values()]
        
        if len(non_fixed_positions) >= 3:
            # 随机选择一个局部区域
            start_idx = random.randint(0, len(non_fixed_positions) - 3)
            end_idx = min(start_idx + random.randint(2, 5), len(non_fixed_positions))
            
            # 获取区域内的实际位置
            region_positions = non_fixed_positions[start_idx:end_idx]
            
            # 提取该区域的科室
            region_depts = [layout[pos] for pos in region_positions]
            
            # 乱序并重新分配
            random.shuffle(region_depts)
            
            # 临时保存原始布局以备回滚
            original_layout = layout.copy()
            
            for pos, dept in zip(region_positions, region_depts):
                layout[pos] = dept
            
            # 如果乱序后违反约束，回滚操作
            if not self.constraint_manager.is_valid_layout(layout):
                for i, pos in enumerate(region_positions):
                    layout[pos] = original_layout[pos]
    
    def _mutate_individual(self, individual: Individual):
        """个体变异"""
        self._smart_mutate_layout(individual.layout)
        individual.fitness = float('inf')  # 重置适应度，需要重新计算
    
    def _adapt_parameters(self):
        """
        自适应参数调整
        
        根据算法的进化状态动态调整变异率和交叉率：
        1. 如果停滞时间过长，增加变异率以增强探索
        2. 如果种群多样性过低，调整参数促进多样性
        3. 记录参数变化历史用于分析
        """
        if not self.adaptive_parameters:
            return
        
        # 计算当前种群多样性
        current_diversity = self._calculate_diversity()
        self.diversity_history.append(current_diversity)
        
        # 根据停滞情况调整变异率
        if self.stagnation_count > 20:
            # 长期停滞，增加变异率
            self.mutation_rate = min(0.3, self.mutation_rate * 1.1)
        elif self.stagnation_count < 5:
            # 快速改进，适当降低变异率
            self.mutation_rate = max(0.05, self.mutation_rate * 0.95)
        
        # 根据多样性调整交叉率
        if current_diversity < 0.1:
            # 多样性过低，降低交叉率，增加随机性
            self.crossover_rate = max(0.6, self.crossover_rate * 0.9)
            self.mutation_rate = min(0.4, self.mutation_rate * 1.2)
        elif current_diversity > 0.8:
            # 多样性过高，增加交叉率，促进信息交换
            self.crossover_rate = min(0.95, self.crossover_rate * 1.05)
        
        # 记录参数变化
        self.mutation_rate_history.append(self.mutation_rate)
        self.crossover_rate_history.append(self.crossover_rate)
        
        if self.generation % 50 == 0:
            logger.debug(f"参数调整: 变异率={self.mutation_rate:.3f}, "
                        f"交叉率={self.crossover_rate:.3f}, "
                        f"多样性={current_diversity:.3f}")
    
    def _maintain_diversity(self):
        """维护种群多样性（增强版）"""
        # 计算多样性
        diversity = self._calculate_diversity()
        
        if diversity < 0.05:  # 多样性极低
            logger.info(f"种群多样性极低({diversity:.3f})，执行大规模重新初始化")
            
            # 保留最佳的20%个体
            num_to_keep = max(1, self.population_size // 5)
            best_individuals = sorted(self.population, key=lambda x: x.fitness)[:num_to_keep]
            
            # 重新生成剩余个体
            self.population = best_individuals.copy()
            while len(self.population) < self.population_size:
                new_layout = self._generate_constraint_aware_random_layout()
                self.population.append(Individual(layout=new_layout))
            
        elif diversity < 0.15:  # 多样性较低
            logger.info(f"种群多样性较低({diversity:.3f})，注入新个体")
            
            # 替换最差的25%个体
            num_to_replace = self.population_size // 4
            worst_individuals = sorted(self.population, key=lambda x: x.fitness, reverse=True)
            
            for i in range(min(num_to_replace, len(worst_individuals))):
                # 使用不同的生成策略
                if i % 2 == 0:
                    new_layout = self._generate_greedy_area_matched_layout()
                else:
                    new_layout = self._generate_constraint_aware_random_layout()
                
                worst_individuals[i].layout = new_layout
                worst_individuals[i].fitness = float('inf')
                worst_individuals[i].age = 0
    
    def get_additional_metrics(self) -> Dict[str, Any]:
        """获取约束感知遗传算法的增强指标"""
        base_metrics = super().get_additional_metrics() if hasattr(super(), 'get_additional_metrics') else {}
        
        diversity = self._calculate_diversity() if self.population else 0
        
        # 计算约束违反率
        violation_rate = (self.constraint_violation_count / max(1, self.constraint_repairer.repair_attempts))
        repair_success_rate = (self.successful_repair_count / max(1, self.constraint_violation_count))
        
        # 算法特定指标
        constraint_aware_metrics = {
            # 基础参数
            "population_size": self.population_size,
            "elite_size": self.elite_size,
            "initial_mutation_rate": self.mutation_rate_history[0] if self.mutation_rate_history else self.mutation_rate,
            "initial_crossover_rate": self.crossover_rate_history[0] if self.crossover_rate_history else self.crossover_rate,
            "final_mutation_rate": self.mutation_rate,
            "final_crossover_rate": self.crossover_rate,
            "tournament_size": self.tournament_size,
            "max_age": self.max_age,
            "constraint_repair_strategy": self.constraint_repair_strategy,
            "adaptive_parameters": self.adaptive_parameters,
            
            # 算法状态
            "final_generation": self.generation,
            "stagnation_count": self.stagnation_count,
            "population_diversity": diversity,
            
            # 约束处理统计
            "constraint_violation_count": self.constraint_violation_count,
            "successful_repair_count": self.successful_repair_count,
            "constraint_violation_rate": violation_rate,
            "constraint_repair_success_rate": repair_success_rate,
            
            # 算法性能统计
            "fpx_crossover_count": self.fpx_crossover_count,
            "smart_mutation_count": self.smart_mutation_count,
            "greedy_initialization_count": self.greedy_initialization_count,
            
            # 历史数据
            "best_fitness_history": self.best_fitness_history.copy(),
            "diversity_history": self.diversity_history.copy(),
            "mutation_rate_history": self.mutation_rate_history.copy(),
            "crossover_rate_history": self.crossover_rate_history.copy(),
            
            # 收敛性分析
            "convergence_rate": (self.best_fitness_history[0] - self.best_cost) / self.best_fitness_history[0] 
                               if self.best_fitness_history and self.best_fitness_history[0] > 0 else 0,
            
            # 约束修复器统计
            "constraint_repairer_stats": self.constraint_repairer.get_repair_statistics()
        }
        
        # 合并基础指标和约束感知指标
        base_metrics.update(constraint_aware_metrics)
        
        return base_metrics
    
    def reset_algorithm_state(self):
        """重置算法状态（用于多次运行）"""
        self.population = []
        self.generation = 0
        self.stagnation_count = 0
        self.best_fitness_history = []
        self.diversity_history = []
        self.mutation_rate_history = []
        self.crossover_rate_history = []
        
        # 重置统计计数器
        self.constraint_violation_count = 0
        self.successful_repair_count = 0
        self.fpx_crossover_count = 0
        self.smart_mutation_count = 0
        self.greedy_initialization_count = 0
        
        # 重置约束修复器统计
        self.constraint_repairer.reset_statistics()
        
        # 重置父类状态
        if hasattr(super(), 'reset_algorithm_state'):
            super().reset_algorithm_state()
    
    def _update_constraint_statistics(self):
        """更新约束相关统计信息"""
        # 检查当前种群中的约束违反情况
        for individual in self.population:
            if not self.constraint_manager.is_valid_layout(individual.layout):
                self.constraint_violation_count += 1
        
        # 更新修复成功计数
        repair_stats = self.constraint_repairer.get_repair_statistics()
        self.successful_repair_count = repair_stats['successful_repairs']
    
    def _calculate_diversity(self) -> float:
        """计算种群多样性（使用汉明距离）"""
        if len(self.population) < 2:
            return 1.0
        
        total_distance = 0
        comparisons = 0
        
        for i in range(len(self.population)):
            for j in range(i + 1, len(self.population)):
                distance = self._layout_distance(self.population[i].layout, self.population[j].layout)
                total_distance += distance
                comparisons += 1
        
        if comparisons == 0:
            return 1.0
        
        avg_distance = total_distance / comparisons
        max_distance = 1.0  # 归一化的最大距离
        
        return avg_distance / max_distance if max_distance > 0 else 0
    
    def _layout_distance(self, layout1: List[str], layout2: List[str]) -> float:
        """计算两个布局之间的归一化汉明距离"""
        if len(layout1) != len(layout2):
            return 1.0  # 最大距离
        
        # 使用汉明距离
        distance = sum(1 for a, b in zip(layout1, layout2) if a != b)
        return distance / len(layout1) if len(layout1) > 0 else 0
    
    def _age_and_filter_population(self, population: List[Individual]) -> List[Individual]:
        """更新年龄并过滤过老的个体"""
        filtered = []
        for individual in population:
            individual.age += 1
            if individual.age <= self.max_age:
                filtered.append(individual)
            else:
                # 替换过老的个体
                new_layout = self._generate_constraint_aware_random_layout()
                new_individual = Individual(layout=new_layout)
                filtered.append(new_individual)
        
        return filtered
</file>

<file path="src/analysis/travel_time.py">
"""
Calculates travel times between specified room-like nodes in the graph.
"""
import csv
import pathlib
import networkx as nx
import logging  # Added for logging
from typing import Dict, List, Union, Optional

from src.config import NetworkConfig
from src.network.node import Node

# Get a logger for this module
logger = logging.getLogger(__name__)


def calculate_room_travel_times(
    graph: nx.Graph,
    config: NetworkConfig,
    output_dir: pathlib.Path,
    output_filename: str = "room_travel_times.csv",
    ground_floor_z: Optional[float] = None
) -> Dict[str, Dict[str, Union[float, str]]]:
    """
    Calculates the shortest travel times between all pairs of individual "room" instances
    and designated "outward-facing door" nodes in the graph.

    Each room instance and each relevant outward-facing door is treated as a unique location.
    Room instance names will be 'NodeType_NodeID'.
    Outward-facing door names will be 'OutDoor_NodeID'.
    The output CSV will also include a final row with the area of each unique location.

    Args:
        graph: The input NetworkX graph. Nodes are expected to be `Node` objects
               or have a `node_obj` attribute pointing to a `Node` object.
        config: The NetworkConfig object.
        output_dir: The directory to save the resulting CSV file.
        output_filename: The name of the output CSV file.
        ground_floor_z: The Z-coordinate of the designated ground floor.
                        Only 'out' doors on this floor will be considered.

    Returns:
        A dictionary where keys are source location names and values are
        dictionaries mapping target location names to travel times.
    """
    if not graph.nodes:
        logger.warning("Graph is empty. Cannot calculate travel times.")
        return {}

    room_nodes: List[Node] = []
    out_door_nodes: List[Node] = []

    if ground_floor_z is None:
        logger.warning(
            "ground_floor_z not provided to calculate_room_travel_times. "
            "No 'out' doors will be specifically included as distinct locations for travel time analysis "
            "unless this behavior is changed in the filtering logic below."
        )

    for G_node_id in graph.nodes():
        # 获取节点数据
        G_node_data = graph.nodes[G_node_id]
        node_obj = G_node_data.get('node_obj')
        
        if not isinstance(node_obj, Node):
            logger.warning(f"Node {G_node_id} does not have a valid node_obj attribute")
            continue

        if node_obj.node_type in config.ROOM_TYPES:
            room_nodes.append(node_obj)
        elif node_obj.node_type in config.CONNECTION_TYPES and getattr(node_obj, 'door_type', None) == 'out':
            if ground_floor_z is not None:
                # Tolerance for Z comparison
                if abs(node_obj.z - ground_floor_z) < 0.1:
                    out_door_nodes.append(node_obj)
            # If ground_floor_z is None, no out_door_nodes are added from this path based on current logic.
            # If you want a fallback, it would be here. For "only ground floor", this is correct.

    location_nodes: List[Node] = room_nodes + out_door_nodes
    # Map Node object to its unique name
    location_names_map: Dict[Node, str] = {}
    # Map unique location name to its area
    location_areas_map: Dict[str, float] = {}

    for node_obj in room_nodes:
        unique_name = f"{node_obj.node_type}_{node_obj.id}"
        location_names_map[node_obj] = unique_name
        location_areas_map[unique_name] = getattr(node_obj, 'area', 0)

    for node_obj in out_door_nodes:  # These are already filtered for ground floor
        unique_name = f"门_{node_obj.id}"
        location_names_map[node_obj] = unique_name
        location_areas_map[unique_name] = getattr(node_obj, 'area', 0)

    if not location_nodes:
        logger.warning(
            "No room instances or designated (ground floor) outward-facing door nodes found to calculate travel times.")
        return {}

    def weight_function(u_node_id, v_node_id, edge_data):  # u,v are node IDs
        # 从图中获取节点对象
        if v_node_id not in graph.nodes:
            logger.error(f"Node {v_node_id} not found in graph")
            return float('inf')
        
        v_node_data = graph.nodes[v_node_id]
        v_node_obj = v_node_data.get('node_obj')
        
        if not isinstance(v_node_obj, Node):
            logger.error(
                f"Target node {v_node_id} in edge is not a valid Node object for weight func.")
            return float('inf')  # 返回无穷大而不是抛出异常
        
        return v_node_obj.time

    travel_times_data: Dict[str, Dict[str, Union[float, str]]] = {}
    logger.info(
        f"Calculating travel times for {len(location_nodes)} unique locations...")

    for start_node_obj in location_nodes:
        start_location_name = location_names_map[start_node_obj]
        travel_times_data.setdefault(start_location_name, {})

        try:
            lengths = nx.single_source_dijkstra_path_length(
                graph,
                source=start_node_obj.id,  # 使用节点ID而不是节点对象
                weight=weight_function
            )
        except nx.NodeNotFound:
            logger.warning(
                f"Start node {start_location_name} (ID: {start_node_obj.id}) not in graph for Dijkstra. Skipping.")
            continue
        except Exception as e:
            logger.error(
                f"Error during Dijkstra for {start_location_name} (ID: {start_node_obj.id}): {e}")
            continue

        for target_node_obj in location_nodes:
            target_location_name = location_names_map[target_node_obj]

            if start_node_obj == target_node_obj:
                travel_times_data[start_location_name][target_location_name] = round(
                    start_node_obj.time, 2)
                continue

            if target_node_obj.id in lengths:
                total_time = start_node_obj.time + lengths[target_node_obj.id]
                travel_times_data[start_location_name][target_location_name] = round(
                    total_time, 2)
            else:
                travel_times_data[start_location_name][target_location_name] = '∞'

    logger.info("Travel time calculation complete.")

    output_dir.mkdir(parents=True, exist_ok=True)
    csv_file_path = output_dir / output_filename

    # all_location_names will now be unique identifiers like "RoomType_ID" or "OutDoor_ID"
    all_location_names = sorted(list(location_names_map.values()))

    if not all_location_names:
        logger.warning(
            "No unique location names generated to write to CSV for travel times and areas.")
        return travel_times_data

    try:
        with open(csv_file_path, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)

            header = ['来源/目标'] + all_location_names
            writer.writerow(header)

            for source_name in all_location_names:
                row_data = [source_name]
                for target_name in all_location_names:
                    time_val = travel_times_data.get(
                        source_name, {}).get(target_name, 'N/A')
                    row_data.append(time_val)
                writer.writerow(row_data)

            # "Area (px²)" or "面积 (m²)" depending on your unit
            area_row_label = "面积"
            area_row_data = [area_row_label]
            for loc_name in all_location_names:  # loc_name is now unique
                area_val = location_areas_map.get(
                    loc_name)  # Get area by unique name
                if area_val is not None:
                    area_row_data.append(f"{area_val:.2f}")
                else:
                    area_row_data.append("N/A")  # Should ideally not happen
            writer.writerow(area_row_data)

        logger.info(f"Travel times and areas saved to {csv_file_path}")
    except IOError as e:
        logger.error(
            f"Failed to write travel times and areas CSV to {csv_file_path}: {e}")

    return travel_times_data
</file>

<file path="src/core/algorithm_manager.py">
"""
算法管理器 - 统一管理和运行所有优化算法
"""

from src.rl_optimizer.utils.setup import setup_logger
import time
import pandas as pd
from pathlib import Path
from typing import Dict, List, Optional, Any, Type
from concurrent.futures import ThreadPoolExecutor, as_completed
import copy

from src.algorithms.base_optimizer import BaseOptimizer, OptimizationResult
from src.algorithms.constraint_manager import ConstraintManager
from src.algorithms.ppo_optimizer import PPOOptimizer
from src.algorithms.simulated_annealing import SimulatedAnnealingOptimizer
from src.algorithms.genetic_algorithm import GeneticAlgorithmOptimizer
from src.rl_optimizer.env.cost_calculator import CostCalculator
from src.rl_optimizer.data.cache_manager import CacheManager
from src.config import RLConfig, NetworkConfig

logger = setup_logger(__name__)


class AlgorithmManager:
    """
    算法管理器
    
    统一管理所有优化算法的运行，提供单一接口来执行不同的优化算法，
    支持算法对比、并行执行和结果管理。
    """
    
    def __init__(self, 
                 cost_calculator: CostCalculator,
                 constraint_manager: ConstraintManager,
                 config: RLConfig,
                 cache_manager: CacheManager):
        """
        初始化算法管理器
        
        Args:
            cost_calculator: 成本计算器
            constraint_manager: 约束管理器  
            config: RL配置
            cache_manager: 缓存管理器
        """
        self.cost_calculator = cost_calculator
        self.constraint_manager = constraint_manager
        self.config = config
        self.cache_manager = cache_manager
        
        # 算法注册表
        self.algorithm_registry = {
            'ppo': PPOOptimizer,
            'simulated_annealing': SimulatedAnnealingOptimizer,
            'genetic_algorithm': GeneticAlgorithmOptimizer
        }
        
        # 算法参数配置
        self.algorithm_configs = {
            'ppo': {
                'total_timesteps': config.TOTAL_TIMESTEPS,
            },
            'simulated_annealing': {
                'initial_temperature': config.SA_DEFAULT_INITIAL_TEMP,
                'final_temperature': config.SA_DEFAULT_FINAL_TEMP,
                'cooling_rate': config.SA_DEFAULT_COOLING_RATE,
                'temperature_length': config.SA_DEFAULT_TEMPERATURE_LENGTH,
                'max_iterations': config.SA_DEFAULT_MAX_ITERATIONS
            },
            'genetic_algorithm': {
                'population_size': config.GA_DEFAULT_POPULATION_SIZE,
                'elite_size': config.GA_DEFAULT_ELITE_SIZE,
                'mutation_rate': config.GA_DEFAULT_MUTATION_RATE,
                'crossover_rate': config.GA_DEFAULT_CROSSOVER_RATE,
                'tournament_size': config.GA_DEFAULT_TOURNAMENT_SIZE,
                'max_age': config.GA_DEFAULT_MAX_AGE,
                'max_iterations': config.GA_DEFAULT_MAX_ITERATIONS,
                'convergence_threshold': config.GA_DEFAULT_CONVERGENCE_THRESHOLD,
                'constraint_repair_strategy': config.GA_CONSTRAINT_REPAIR_STRATEGY,
                'adaptive_parameters': config.GA_ADAPTIVE_PARAMETERS
            }
        }
        
        # 结果存储
        self.results = {}
        
        logger.info("算法管理器初始化完成")
        logger.info(f"已注册算法: {list(self.algorithm_registry.keys())}")
    
    def run_single_algorithm(self, 
                           algorithm_name: str,
                           initial_layout: Optional[List[str]] = None,
                           custom_params: Optional[Dict[str, Any]] = None) -> OptimizationResult:
        """
        运行单个算法
        
        Args:
            algorithm_name: 算法名称
            initial_layout: 初始布局
            custom_params: 自定义参数
            
        Returns:
            OptimizationResult: 优化结果
        """
        if algorithm_name not in self.algorithm_registry:
            raise ValueError(f"未知算法: {algorithm_name}. 可用算法: {list(self.algorithm_registry.keys())}")
        
        logger.info(f"开始运行算法: {algorithm_name}")
        
        # 生成原始布局（未经优化的基准）
        original_layout = self.constraint_manager.generate_original_layout()
        original_cost = self.cost_calculator.calculate_total_cost(original_layout)
        logger.info(f"原始布局成本: {original_cost:.2f}")
        
        # 合并参数
        params = self.algorithm_configs[algorithm_name].copy()
        if custom_params:
            params.update(custom_params)
        
        # 创建算法实例（传递自定义参数用于构造函数）
        optimizer = self._create_optimizer(algorithm_name, params)
        
        # 准备运行时参数（移除构造函数参数）
        runtime_params = params.copy()
        if algorithm_name == 'simulated_annealing':
            # 移除SA的构造函数参数，只保留运行时参数
            for key in ['initial_temperature', 'final_temperature', 'cooling_rate', 'temperature_length']:
                runtime_params.pop(key, None)
        elif algorithm_name == 'genetic_algorithm':
            # 移除GA的构造函数参数，只保留运行时参数
            for key in ['population_size', 'elite_size', 'mutation_rate', 'crossover_rate', 'tournament_size', 'max_age']:
                runtime_params.pop(key, None)
        
        # 运行优化
        start_time = time.time()
        try:
            result = optimizer.optimize(
                initial_layout=initial_layout,
                original_layout=original_layout,
                original_cost=original_cost,
                **runtime_params
            )
            result.execution_time = time.time() - start_time
            
            # 存储结果
            self.results[algorithm_name] = result
            
            logger.info(f"算法 {algorithm_name} 完成:")
            logger.info(f"  最优成本: {result.best_cost:.2f}")
            logger.info(f"  执行时间: {result.execution_time:.2f}s")
            logger.info(f"  迭代次数: {result.iterations}")
            
            return result
            
        except Exception as e:
            logger.error(f"算法 {algorithm_name} 执行失败: {e}", exc_info=True)
            raise
    
    def run_multiple_algorithms(self, 
                              algorithm_names: List[str],
                              initial_layout: Optional[List[str]] = None,
                              custom_params: Optional[Dict[str, Dict[str, Any]]] = None,
                              parallel: bool = False) -> Dict[str, OptimizationResult]:
        """
        运行多个算法
        
        Args:
            algorithm_names: 算法名称列表
            initial_layout: 初始布局
            custom_params: 自定义参数字典，键为算法名
            parallel: 是否并行执行
            
        Returns:
            Dict[str, OptimizationResult]: 算法名到结果的映射
        """
        logger.info(f"开始运行多个算法: {algorithm_names}")
        logger.info(f"并行执行: {parallel}")
        
        # 生成共同的原始布局（所有算法使用相同的基准）
        original_layout = self.constraint_manager.generate_original_layout()
        original_cost = self.cost_calculator.calculate_total_cost(original_layout)
        logger.info(f"原始布局成本（所有算法共用）: {original_cost:.2f}")
        
        results = {}
        
        if parallel:
            # 并行执行（注意：PPO可能需要GPU资源，谨慎并行）
            results = self._run_algorithms_parallel(algorithm_names, initial_layout, custom_params, 
                                                   original_layout, original_cost)
        else:
            # 串行执行
            for algorithm_name in algorithm_names:
                params = self.algorithm_configs[algorithm_name].copy()
                if custom_params:
                    params.update(custom_params)
                try:
                    # 创建算法实例
                    optimizer = self._create_optimizer(algorithm_name, params)
                    
                    # 准备运行时参数
                    runtime_params = params.copy()

                    # 移除构造函数参数
                    if algorithm_name == 'simulated_annealing':
                        for key in ['initial_temperature', 'final_temperature', 'cooling_rate', 'temperature_length']:
                            runtime_params.pop(key, None)
                    elif algorithm_name == 'genetic_algorithm':
                        for key in ['population_size', 'elite_size', 'mutation_rate', 'crossover_rate', 'tournament_size', 'max_age']:
                            runtime_params.pop(key, None)
                    
                    # 运行优化
                    result = optimizer.optimize(
                        initial_layout=initial_layout,
                        original_layout=original_layout,
                        original_cost=original_cost,
                        **runtime_params
                    )
                    results[algorithm_name] = result
                except Exception as e:
                    logger.error(f"跳过算法 {algorithm_name}，原因: {e}")
        
        self.results.update(results)
        return results
    
    def _run_algorithms_parallel(self, 
                               algorithm_names: List[str],
                               initial_layout: Optional[List[str]],
                               custom_params: Optional[Dict[str, Dict[str, Any]]],
                               original_layout: List[str],
                               original_cost: float) -> Dict[str, OptimizationResult]:
        """
        并行运行算法（改进版：每个算法使用独立的资源实例）
        """
        results = {}
        
        def run_algorithm_independent(alg_name: str, init_layout: Optional[List[str]], 
                                     params: Dict[str, Any], orig_layout: List[str], orig_cost: float):
            """在独立的上下文中运行算法"""
            # 创建独立的优化器实例
            optimizer = self._create_optimizer(alg_name, params, create_independent=True)
            
            # 准备运行时参数
            runtime_params = params.copy()
            if alg_name == 'simulated_annealing':
                for key in ['initial_temperature', 'final_temperature', 'cooling_rate', 'temperature_length']:
                    runtime_params.pop(key, None)
            elif alg_name == 'genetic_algorithm':
                for key in ['population_size', 'elite_size', 'mutation_rate', 'crossover_rate', 'tournament_size', 'max_age']:
                    runtime_params.pop(key, None)
            
            # 运行优化
            start_time = time.time()
            result = optimizer.optimize(
                initial_layout=init_layout,
                original_layout=orig_layout,
                original_cost=orig_cost,
                **runtime_params
            )
            result.execution_time = time.time() - start_time
            
            return result
        
        with ThreadPoolExecutor(max_workers=min(len(algorithm_names), 3)) as executor:
            # 提交任务
            future_to_algorithm = {}
            for algorithm_name in algorithm_names:
                params = custom_params.get(algorithm_name, {}) if custom_params else {}
                future = executor.submit(run_algorithm_independent, algorithm_name, 
                                        initial_layout, params, original_layout, original_cost)
                future_to_algorithm[future] = algorithm_name
            
            # 收集结果
            for future in as_completed(future_to_algorithm):
                algorithm_name = future_to_algorithm[future]
                try:
                    result = future.result()
                    results[algorithm_name] = result
                except Exception as e:
                    logger.error(f"并行执行算法 {algorithm_name} 失败: {e}")
        
        return results
    
    def _create_optimizer(self, algorithm_name: str, custom_params: Optional[Dict[str, Any]] = None, 
                         create_independent: bool = False) -> BaseOptimizer:
        """
        创建优化器实例
        
        Args:
            algorithm_name: 算法名称
            custom_params: 自定义参数
            create_independent: 是否创建独立的calculator和manager实例（用于并发执行）
        """
        optimizer_class = self.algorithm_registry[algorithm_name]
        
        # 如果需要独立实例（并发执行），创建新的calculator和manager
        if create_independent:
            # 深拷贝以避免共享状态
            cost_calculator = copy.deepcopy(self.cost_calculator)
            constraint_manager = copy.deepcopy(self.constraint_manager)
        else:
            cost_calculator = self.cost_calculator
            constraint_manager = self.constraint_manager
        
        if algorithm_name == 'ppo':
            # 获取预训练模型路径（如果有）
            pretrained_model_path = None
            if custom_params and 'pretrained_model_path' in custom_params:
                pretrained_model_path = custom_params['pretrained_model_path']
            
            return optimizer_class(
                cost_calculator=cost_calculator,
                constraint_manager=constraint_manager,
                config=self.config,
                cache_manager=self.cache_manager,
                pretrained_model_path=pretrained_model_path
            )
        elif algorithm_name == 'simulated_annealing':
            # 获取SA特定的构造参数
            sa_params = {}
            if custom_params:
                if 'initial_temperature' in custom_params:
                    sa_params['initial_temperature'] = custom_params['initial_temperature']
                if 'final_temperature' in custom_params:
                    sa_params['final_temperature'] = custom_params['final_temperature']
                if 'cooling_rate' in custom_params:
                    sa_params['cooling_rate'] = custom_params['cooling_rate']
                if 'temperature_length' in custom_params:
                    sa_params['temperature_length'] = custom_params['temperature_length']
            
            return optimizer_class(
                cost_calculator=cost_calculator,
                constraint_manager=constraint_manager,
                **sa_params
            )
        elif algorithm_name == 'genetic_algorithm':
            # 获取GA特定的构造参数
            ga_params = {}
            if custom_params:
                if 'population_size' in custom_params:
                    ga_params['population_size'] = custom_params['population_size']
                if 'elite_size' in custom_params:
                    ga_params['elite_size'] = custom_params['elite_size']
                if 'mutation_rate' in custom_params:
                    ga_params['mutation_rate'] = custom_params['mutation_rate']
                if 'crossover_rate' in custom_params:
                    ga_params['crossover_rate'] = custom_params['crossover_rate']
                if 'tournament_size' in custom_params:
                    ga_params['tournament_size'] = custom_params['tournament_size']
                if 'max_age' in custom_params:
                    ga_params['max_age'] = custom_params['max_age']
            
            return optimizer_class(
                cost_calculator=cost_calculator,
                constraint_manager=constraint_manager,
                **ga_params
            )
        else:
            # 默认情况
            return optimizer_class(
                cost_calculator=self.cost_calculator,
                constraint_manager=self.constraint_manager
            )
    
    def get_algorithm_comparison(self) -> pd.DataFrame:
        """
        获取算法对比结果
        
        Returns:
            pd.DataFrame: 对比结果表格
        """
        if not self.results:
            logger.warning("没有算法执行结果可供对比")
            return pd.DataFrame()
        
        comparison_data = []
        
        for algorithm_name, result in self.results.items():
            row = {
                '算法名称': result.algorithm_name,
                '最优成本': result.best_cost,
                '执行时间(秒)': result.execution_time,
                '迭代次数': result.iterations,
                '收敛性': self._calculate_convergence_metric(result),
                '最终改进率(%)': self._calculate_improvement_rate(result)
            }
            
            # 添加算法特定指标
            if algorithm_name == 'simulated_annealing':
                metrics = result.additional_metrics
                row['接受率(%)'] = metrics.get('acceptance_rate', 0) * 100
                row['改进次数'] = metrics.get('improvement_count', 0)
            elif algorithm_name == 'genetic_algorithm':
                metrics = result.additional_metrics
                row['最终代数'] = metrics.get('final_generation', 0)
                row['种群多样性'] = metrics.get('population_diversity', 0)
                row['收敛率(%)'] = metrics.get('convergence_rate', 0) * 100
            elif algorithm_name == 'ppo':
                metrics = result.additional_metrics
                row['训练步数'] = metrics.get('total_timesteps', 0)
                row['环境数量'] = metrics.get('num_envs', 0)
        
            comparison_data.append(row)
        
        df = pd.DataFrame(comparison_data)
        
        # 按最优成本排序
        df = df.sort_values('最优成本')
        df = df.reset_index(drop=True)
        
        return df
    
    def _calculate_convergence_metric(self, result: OptimizationResult) -> float:
        """计算收敛性指标"""
        if not result.convergence_history or len(result.convergence_history) < 2:
            return 0.0
        
        # 计算后期收敛稳定性
        history = result.convergence_history
        if len(history) >= 100:
            # 取最后100次迭代的标准差作为收敛性指标
            recent_history = history[-100:]
            std_dev = pd.Series(recent_history).std()
            return 1.0 / (1.0 + std_dev)  # 标准差越小，收敛性越好
        else:
            # 对于较短的历史，计算总体收敛趋势
            initial_cost = history[0]
            final_cost = history[-1]
            if initial_cost > 0:
                return (initial_cost - final_cost) / initial_cost
            return 0.0
    
    def _calculate_improvement_rate(self, result: OptimizationResult) -> float:
        """计算改进率"""
        if not result.convergence_history or len(result.convergence_history) < 2:
            return 0.0
        
        initial_cost = result.original_cost
        final_cost = result.best_cost
        
        if initial_cost > 0:
            return ((initial_cost - final_cost) / initial_cost) * 100
        return 0.0
    
    def save_results(self, output_dir: str = "./results/comparison"):
        """
        保存算法对比结果
        
        Args:
            output_dir: 输出目录
        """
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        timestamp = time.strftime("%Y%m%d-%H%M%S")
        
        # 保存对比表格
        comparison_df = self.get_algorithm_comparison()
        if not comparison_df.empty:
            csv_path = output_path / f"algorithm_comparison_{timestamp}.csv"
            comparison_df.to_csv(csv_path, index=False, encoding='utf-8-sig')
            logger.info(f"算法对比结果已保存到: {csv_path}")
        
        # 保存详细结果
        for algorithm_name, result in self.results.items():
            result_dict = {
                'algorithm_name': result.algorithm_name,
                'best_cost': result.best_cost,
                'execution_time': result.execution_time,
                'iterations': result.iterations,
                'best_layout': result.best_layout,
                'original_layout': result.original_layout,  # 添加原始布局
                'original_cost': result.original_cost,      # 添加原始成本
                'convergence_history': result.convergence_history,
                'additional_metrics': result.additional_metrics
            }
            
            # 计算改进率
            if result.original_cost is not None and result.original_cost > 0:
                result_dict['improvement'] = ((result.original_cost - result.best_cost) / result.original_cost) * 100
            
            import json
            json_path = output_path / f"{algorithm_name}_result_{timestamp}.json"
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(result_dict, f, indent=2, ensure_ascii=False)
            
            logger.info(f"算法 {algorithm_name} 详细结果已保存到: {json_path}")
    
    def get_best_result(self) -> Optional[OptimizationResult]:
        """
        获取最佳结果
        
        Returns:
            Optional[OptimizationResult]: 最佳优化结果
        """
        if not self.results:
            return None
        
        best_result = min(self.results.values(), key=lambda x: x.best_cost)
        return best_result
    
    def clear_results(self):
        """清除所有结果"""
        self.results.clear()
        logger.info("已清除所有算法结果")
    
    def get_algorithm_names(self) -> List[str]:
        """获取所有可用的算法名称"""
        return list(self.algorithm_registry.keys())
    
    def register_algorithm(self, name: str, optimizer_class: Type[BaseOptimizer], config: Dict[str, Any]):
        """
        注册新算法
        
        Args:
            name: 算法名称
            optimizer_class: 优化器类
            config: 算法配置
        """
        self.algorithm_registry[name] = optimizer_class
        self.algorithm_configs[name] = config
        logger.info(f"已注册新算法: {name}")
    
    def update_algorithm_config(self, algorithm_name: str, config: Dict[str, Any]):
        """
        更新算法配置
        
        Args:
            algorithm_name: 算法名称
            config: 新配置
        """
        if algorithm_name in self.algorithm_configs:
            self.algorithm_configs[algorithm_name].update(config)
            logger.info(f"已更新算法 {algorithm_name} 的配置")
        else:
            logger.warning(f"算法 {algorithm_name} 不存在，无法更新配置")
</file>

<file path="src/network/super_network.py">
"""
Manages the construction of a multi-floor network by orchestrating
individual Network instances, potentially in parallel.
"""

import multiprocessing
import os  # For os.cpu_count()
import pathlib
import networkx as nx
import numpy as np
from src.rl_optimizer.utils.setup import setup_logger
from typing import List, Dict, Tuple, Optional, Any
from scipy.spatial import KDTree

from src.config import NetworkConfig
from .node import Node
from .network import Network  # The single-floor network builder
from .floor_manager import FloorManager

logger = setup_logger(__name__)

# Worker function for multiprocessing - must be defined at the top-level or picklable


def _process_floor_worker(task_args: Tuple[pathlib.Path, float, int, Dict[str, Any], Dict[Tuple[int, int, int], Dict[str, Any]], bool]) \
        -> Tuple[Optional[nx.Graph], Optional[int], Optional[int], int, pathlib.Path, float]:
    """
    Worker function to process a single floor's network generation.

    Args:
        task_args: A tuple containing:
            - image_path (pathlib.Path): Path to the floor image.
            - z_level (float): Z-coordinate for this floor.
            - id_start_value (int): Starting node ID for this floor.
            - config_dict (Dict): Dictionary representation of NetworkConfig.
            - color_map_data (Dict): The color map.
            - process_outside_nodes (bool): Flag to process outside nodes.

    Returns:
        A tuple containing:
            - graph (Optional[nx.Graph]): Generated graph for the floor, or None on error.
            - width (Optional[int]): Image width, or None on error.
            - height (Optional[int]): Image height, or None on error.
            - next_id_val_from_worker (int): The next available ID from this worker's GraphManager.
            - image_path (pathlib.Path): Original image path (for result matching).
            - z_level (float): Original z_level (for result matching).
    """
    image_path, z_level, id_start_value, config_dict, color_map_data, process_outside_nodes = task_args
    try:
        # Reconstruct config from dict for the worker process
        # Note: This assumes NetworkConfig can be reconstructed from its __dict__
        # and COLOR_MAP is passed directly.
        # A more robust way might be to pass necessary primitive types or use a dedicated
        # config serialization if NetworkConfig becomes very complex.
        # Initialize with color_map
        worker_config = NetworkConfig(color_map_data=color_map_data)
        # Update other attributes from the passed dictionary
        for key, value in config_dict.items():
            # Avoid re-assigning COLOR_MAP
            if key != "COLOR_MAP" and hasattr(worker_config, key):
                setattr(worker_config, key, value)

        network_builder = Network(
            config=worker_config,
            color_map_data=color_map_data,
            id_generator_start_value=id_start_value
        )
        graph, width, height, next_id = network_builder.run(
            image_path=str(image_path),  # network.run expects str path
            z_level=z_level,
            process_outside_nodes=process_outside_nodes
        )
        return graph, width, height, next_id, image_path, z_level
    except Exception as e:
        logger.error(
            f"Error processing floor {image_path.name} in worker: {e}")
        # Return next_id_val as id_start_value + config.ESTIMATED_MAX_NODES_PER_FLOOR
        # to ensure main process ID allocation remains consistent even on worker failure.
        # A more sophisticated error handling might be needed.
        est_next_id = id_start_value + \
            config_dict.get("ESTIMATED_MAX_NODES_PER_FLOOR", 10000)
        return None, None, None, est_next_id, image_path, z_level


class SuperNetwork:
    """
    Orchestrates the creation of a multi-floor network graph.

    It manages multiple Network instances, one for each floor, and combines
    their graphs. It supports parallel processing of floors.
    """

    def __init__(self,
                 config: NetworkConfig,
                 color_map_data: Dict[Tuple[int, int, int], Dict[str, Any]],
                 num_processes: Optional[int] = None,
                 base_floor: int = 0,
                 default_floor_height: Optional[float] = None,
                 vertical_connection_tolerance: Optional[int] = None):
        """
        Initializes the SuperNetwork.

        Args:
            config: The main configuration object.
            color_map_data: The RGB color to type mapping.
            num_processes: Number of processes to use for parallel floor processing.
                           Defaults to os.cpu_count().
            base_floor: Default base floor number if not detected from filename.
            default_floor_height: Default height between floors. Uses config value if None.
            vertical_connection_tolerance: Pixel distance tolerance for connecting
                                           vertical nodes between floors. Uses config if None.
        """
        self.config = config
        self.color_map_data = color_map_data
        self.super_graph: nx.Graph = nx.Graph()
        self.designated_ground_floor_number: Optional[int] = None
        self.designated_ground_floor_z: Optional[float] = None

        self.num_processes: int = num_processes if num_processes is not None else (
            os.cpu_count() or 1)

        _floor_height = default_floor_height if default_floor_height is not None else config.DEFAULT_FLOOR_HEIGHT
        self.floor_manager = FloorManager(
            base_floor_default=base_floor, default_floor_height=_floor_height)

        self.vertical_connection_tolerance: int = vertical_connection_tolerance \
            if vertical_connection_tolerance is not None else config.DEFAULT_VERTICAL_CONNECTION_TOLERANCE

        self.floor_z_map: Dict[int, float] = {}  # floor_number -> z_coordinate
        self.path_to_floor_map: Dict[pathlib.Path,
                                     int] = {}  # image_path -> floor_number

        self.width: Optional[int] = None
        self.height: Optional[int] = None

    def _prepare_floor_data(self, image_file_paths: List[pathlib.Path],
                           z_levels_override: Optional[List[float]] = None) \
            -> List[Tuple[pathlib.Path, float, bool]]:
        """
        Determines floor numbers and Z-levels for each image path.
        Also determines if outside nodes should be processed for that floor.
        Outside nodes are processed ONLY for the designated ground/first floor.

        Returns:
            A list of tuples: (image_path, z_level, process_outside_nodes_flag)
        """
        image_paths_as_pathlib = [pathlib.Path(p) for p in image_file_paths]
        
        self.path_to_floor_map, floor_to_path_map = self.floor_manager.auto_assign_floors(image_paths_as_pathlib)
        
        if z_levels_override and len(z_levels_override) == len(image_paths_as_pathlib):
            sorted_paths_by_floor = sorted(self.path_to_floor_map.keys(), key=lambda p: self.path_to_floor_map[p])
            temp_path_to_z = {path: z for path, z in zip(sorted_paths_by_floor, z_levels_override)} # Make sure this aligns correctly
            self.floor_z_map = {self.path_to_floor_map[p]: temp_path_to_z.get(p) for p in self.path_to_floor_map.keys() if temp_path_to_z.get(p) is not None}

        else:
            self.floor_z_map = self.floor_manager.calculate_z_levels(floor_to_path_map)

        if not self.floor_z_map:
            # logger.error("Could not determine Z-levels for floors.") # Ensure logger is available
            raise ValueError("Could not determine Z-levels for floors.")

        floor_tasks_data = []
        all_floor_nums = list(self.floor_z_map.keys()) # These are the actual floor numbers (e.g., -1, 0, 1, 2)

        if not all_floor_nums:
            return []
        
        designated_ground_floor_num: Optional[int] = self.config.GROUND_FLOOR_NUMBER_FOR_OUTSIDE
        
        if designated_ground_floor_num is None: # If not set in config, try auto-detection
            positive_or_zero_floors = sorted([fn for fn in all_floor_nums if fn >= 0])
            if 0 in all_floor_nums:
                designated_ground_floor_num = 0
            elif 1 in all_floor_nums and not positive_or_zero_floors : # Check if 1 is the only positive candidate
                 if not any(0 <= fn < 1 for fn in all_floor_nums): # Ensure no 0.x floors if 1 is chosen
                    designated_ground_floor_num = 1
            elif positive_or_zero_floors:
                designated_ground_floor_num = positive_or_zero_floors[0]
        
        # logger.info(f"Designated ground floor for outside nodes: {designated_ground_floor_num}")

        for p_path, floor_num in self.path_to_floor_map.items():
            z_level = self.floor_z_map.get(floor_num)
            if z_level is None:
                # logger.warning(f"Z-level for floor {floor_num} (path {p_path}) not found. Skipping task.")
                continue

            # Process outside nodes ONLY if the current floor is the designated ground floor
            # AND if there's at least one "OUTSIDE_TYPE" defined in config.
            process_outside = False
            if designated_ground_floor_num is not None and floor_num == designated_ground_floor_num \
               and self.config.OUTSIDE_TYPES:
                process_outside = True
            
            # Override via config if a global "always process outside" is set (though less likely now)
            if self.config.DEFAULT_OUTSIDE_PROCESSING_IN_SUPERNETWORK: # This flag might be re-purposed or removed
                # If this flag is true, it might override the ground-floor-only logic.
                # For "only on ground floor", this should typically be false.
                # Let's assume the ground_floor_num logic is primary.
                # So, if DEFAULT_OUTSIDE_PROCESSING_IN_SUPERNETWORK is true, it processes for all.
                # If false (typical for this new requirement), then only for designated_ground_floor_num.
                if self.config.DEFAULT_OUTSIDE_PROCESSING_IN_SUPERNETWORK:
                    process_outside = True 
                # else: it remains as determined by ground_floor_num logic

            # logger.debug(f"Floor task: Path={p_path.name}, FloorNum={floor_num}, Z={z_level:.2f}, ProcessOutside={process_outside}")
            floor_tasks_data.append((p_path, z_level, process_outside))
        
        floor_tasks_data.sort(key=lambda item: item[1]) # Sort by Z-level
        
        self.designated_ground_floor_number = designated_ground_floor_num
        if self.designated_ground_floor_number is not None:
            self.designated_ground_floor_z = self.floor_z_map.get(self.designated_ground_floor_number)

        return floor_tasks_data

    def run(self,
            image_file_paths: List[str],  # List of string paths from main
            z_levels_override: Optional[List[float]] = None,
            force_vertical_tolerance: Optional[int] = None) -> nx.Graph:
        """
        Builds the multi-floor network.

        Args:
            image_file_paths: List of string paths to floor images.
            z_levels_override: Optional list to manually set Z-levels for each image.
                               Order should correspond to sorted floor order or be a path-to-z map.
            force_vertical_tolerance: Optionally override the vertical connection tolerance.

        Returns:
            The combined multi-floor NetworkX graph.
        """
        self.super_graph.clear()  # Clear previous graph if any
        image_paths_pl = [pathlib.Path(p) for p in image_file_paths]

        floor_run_data = self._prepare_floor_data(
            image_paths_pl, z_levels_override)
        if not floor_run_data:
            logger.warning("Warning: No floor data to process.")
            return self.super_graph

        tasks_for_pool = []
        current_id_start = 1
        config_dict_serializable = self.config.__dict__.copy()
        # COLOR_MAP is already part of config_dict_serializable if NetworkConfig init stores it.
        # If COLOR_MAP is global, it's fine for multiprocessing on systems where memory is copied (fork).
        # For spawn, it needs to be picklable or passed. Here, color_map_data is passed.

        for p_path, z_level, process_outside_flag in floor_run_data:
            tasks_for_pool.append((
                p_path, z_level, current_id_start,
                config_dict_serializable, self.color_map_data, process_outside_flag
            ))
            current_id_start += self.config.ESTIMATED_MAX_NODES_PER_FLOOR

        logging.info(
            f"Starting parallel processing of {len(tasks_for_pool)} floors using {self.num_processes} processes...")

        results = []
        # Use with statement for Pool to ensure proper cleanup
        # Only use pool if multiple tasks and processes
        if self.num_processes > 1 and len(tasks_for_pool) > 1:
            with multiprocessing.Pool(processes=self.num_processes) as pool:
                results = pool.map(_process_floor_worker, tasks_for_pool)
        else:  # Run sequentially for single process or single task
            logging.info("Running floor processing sequentially...")
            for task in tasks_for_pool:
                results.append(_process_floor_worker(task))

        first_floor_processed = True
        # To store (graph, width, height) for valid results
        processed_graphs_data = []

        for graph_result, width_res, height_res, _next_id, res_path, res_z in results:
            if graph_result is None or width_res is None or height_res is None:
                logger.warning(
                    f"Warning: Failed to process floor image {res_path.name} (z={res_z}). Skipping.")
                continue

            if first_floor_processed:
                self.width = width_res
                self.height = height_res
                first_floor_processed = False
            elif self.width != width_res or self.height != height_res:
                raise ValueError(
                    f"Image dimensions mismatch for {res_path.name}. "
                    f"Expected ({self.width},{self.height}), got ({width_res},{height_res}). "
                    "All floor images must have the same dimensions."
                )

            processed_graphs_data.append(
                graph_result)  # Store the graph itself

        # Combine graphs
        for floor_graph in processed_graphs_data:
            # 每个楼层的图使用节点ID作为图节点，数据中包含node_obj
            # 直接合并节点和边，保持一致的存储格式
            self.super_graph.add_nodes_from(floor_graph.nodes(data=True))
            self.super_graph.add_edges_from(floor_graph.edges(data=True))

        if force_vertical_tolerance is not None:
            self.vertical_connection_tolerance = force_vertical_tolerance
        elif self.config.DEFAULT_VERTICAL_CONNECTION_TOLERANCE == 0:  # 0 might mean auto-calculate
            self.vertical_connection_tolerance = self._auto_calculate_vertical_tolerance()

        self._connect_floors()

        logger.info(
            f"SuperNetwork construction complete. Total nodes: {self.super_graph.number_of_nodes()}")
        return self.super_graph

    def _auto_calculate_vertical_tolerance(self) -> int:
        """
        Automatically calculates a tolerance for connecting vertical nodes
        based on their typical proximity (if not specified).
        """
        # 获取所有垂直节点
        vertical_nodes = []
        for node_id in self.super_graph.nodes():
            node_data = self.super_graph.nodes[node_id]
            node_obj = node_data.get('node_obj')
            if isinstance(node_obj, Node) and node_obj.node_type in self.config.VERTICAL_TYPES:
                vertical_nodes.append(node_obj)
        
        if not vertical_nodes or len(vertical_nodes) < 2:
            return self.config.DEFAULT_VERTICAL_CONNECTION_TOLERANCE  # Fallback

        # Consider only XY positions for tolerance calculation
        positions_xy = np.array([(node.x, node.y) for node in vertical_nodes])
        if len(positions_xy) < 2:
            return self.config.DEFAULT_VERTICAL_CONNECTION_TOLERANCE

        try:
            tree = KDTree(positions_xy)
            # Find distance to the nearest neighbor for each vertical node (excluding itself)
            # k=2 includes self and nearest
            distances, _ = tree.query(positions_xy, k=2)

            # Use distances to the actual nearest neighbor (second column)
            # Filter out zero distances if k=1 was used or if duplicates exist
            # Avoid self-match if k=1
            nearest_distances = distances[:, 1][distances[:, 1] > 1e-6]

            if nearest_distances.size == 0:
                return self.config.DEFAULT_VERTICAL_CONNECTION_TOLERANCE

            avg_min_distance = np.mean(nearest_distances)
            # Tolerance could be a factor of this average minimum distance
            # Example: 50% of avg min distance
            calculated_tolerance = int(avg_min_distance * 0.5)
            logger.info(
                f"Auto-calculated vertical tolerance: {calculated_tolerance} (based on avg_min_dist: {avg_min_distance:.2f})")
            return max(10, calculated_tolerance)  # Ensure a minimum tolerance
        except Exception as e:
            logger.error(
                f"Error in auto-calculating tolerance: {e}. Using default.")
            return self.config.DEFAULT_VERTICAL_CONNECTION_TOLERANCE

    def _connect_floors(self) -> None:
        """
        Connects vertical transport nodes (e.g., stairs, elevators) between
        different floors if they are of the same type and spatially close in XY.
        """
        # 获取所有垂直节点
        all_vertical_nodes_in_graph = []
        for node_id in self.super_graph.nodes():
            node_data = self.super_graph.nodes[node_id]
            node_obj = node_data.get('node_obj')
            if isinstance(node_obj, Node) and node_obj.node_type in self.config.VERTICAL_TYPES:
                all_vertical_nodes_in_graph.append(node_obj)

        if not all_vertical_nodes_in_graph:
            logger.info("No vertical nodes found to connect between floors.")
            return

        # Group vertical nodes by their specific type (e.g., 'Stairs', 'Elevator')
        nodes_by_type: Dict[str, List[Node]] = {}
        for node in all_vertical_nodes_in_graph:
            nodes_by_type.setdefault(node.node_type, []).append(node)

        logger.info(
            f"Attempting to connect floors. Tolerance: {self.vertical_connection_tolerance} pixels.")
        connected_pairs_count = 0

        for node_type, nodes_of_this_type in nodes_by_type.items():
            if len(nodes_of_this_type) < 2:
                continue  # Not enough nodes of this type to form a connection

            # Sort nodes by Z-level, then by Y, then by X for potentially more stable pairing
            # Though KDTree approach doesn't strictly need pre-sorting.
            nodes_of_this_type.sort(
                key=lambda n: (n.z, n.y, n.x))

            # Build KDTree for XY positions of nodes of this specific type
            positions_xy = np.array([(node.x, node.y)
                                    for node in nodes_of_this_type])
            if positions_xy.shape[0] < 2:
                continue  # Need at least 2 points for KDTree sensible query

            try:
                kdtree = KDTree(positions_xy)
            except Exception as e:
                logger.error(
                    f"Could not build KDTree for vertical node type {node_type}: {e}")
                continue

            processed_nodes_indices = set()  # To avoid redundant checks

            for i, current_node in enumerate(nodes_of_this_type):
                if i in processed_nodes_indices:
                    continue

                # Query for other nodes of the SAME TYPE within the XY tolerance
                # query_ball_point returns indices into the `positions_xy` array
                indices_in_ball = kdtree.query_ball_point(
                    (current_node.x, current_node.y), r=self.vertical_connection_tolerance)

                for neighbor_idx in indices_in_ball:
                    if neighbor_idx == i:  # Don't connect to self
                        continue

                    neighbor_node = nodes_of_this_type[neighbor_idx]

                    # Crucial check: Ensure they are on different floors (Z-levels differ significantly)
                    # Z-levels are too close (same floor)
                    if abs(current_node.z - neighbor_node.z) < 1.0:
                        continue

                    # Connect if not already connected (使用节点ID)
                    if not self.super_graph.has_edge(current_node.id, neighbor_node.id):
                        self.super_graph.add_edge(
                            current_node.id, neighbor_node.id, type='vertical_connection')
                        connected_pairs_count += 1
                        # Mark both as processed for this type of pairing to avoid re-pairing B with A if A-B done
                        # This might be too aggressive if a node can connect to multiple above/below.
                        # A simpler approach is to just let KDTree find pairs.
                        # The has_edge check prevents duplicate edges.

                processed_nodes_indices.add(i)

        logger.info(
            f"Inter-floor connections made for {connected_pairs_count} pairs of vertical nodes.")
</file>

<file path="src/algorithms/constraint_manager.py">
"""
约束管理器 - 统一处理布局优化中的各种约束条件
"""

from src.rl_optimizer.utils.setup import setup_logger
import random
import numpy as np
from typing import List, Dict, Set, Tuple, Optional, Any
from dataclasses import dataclass
import pandas as pd

from src.config import RLConfig
from src.rl_optimizer.data.cache_manager import CacheManager

logger = setup_logger(__name__)


@dataclass
class DepartmentInfo:
    """科室信息"""
    name: str
    area_requirement: float
    is_fixed: bool = False
    fixed_position: Optional[int] = None
    adjacency_preferences: List[str] = None
    
    def __post_init__(self):
        if self.adjacency_preferences is None:
            self.adjacency_preferences = []


@dataclass 
class SlotInfo:
    """槽位信息"""
    index: int
    name: str
    area: float
    is_available: bool = True


class ConstraintManager:
    """
    约束管理器
    
    统一管理布局优化中的各种约束条件，包括：
    - 面积约束：科室面积需求与槽位面积的匹配
    - 固定位置约束：某些科室必须位于特定位置
    - 相邻性约束：某些科室之间的邻接偏好
    - 容量约束：每个槽位只能放置一个科室
    """
    
    def __init__(self,
                 config: RLConfig,
                 cache_manager: CacheManager):
        """
        初始化约束管理器
        
        Args:
            config: RL配置对象，包含面积容差等参数
            cache_manager: 缓存管理器，提供节点和面积数据
        """
        self.config = config
        self.cache_manager = cache_manager
        self.placeable_slots = cache_manager.placeable_slots
        self.placeable_departments = cache_manager.placeable_departments
        self.area_tolerance_ratio = config.AREA_SCALING_FACTOR
        
        # 初始化槽位和科室信息
        self.slots_info = self._initialize_slots_info()
        self.departments_info = self._initialize_departments_info()
        
        # 构建索引映射（优化查找性能）
        self.dept_name_to_index = {
            dept.name: i for i, dept in enumerate(self.departments_info)
        }
        self.slot_name_to_index = {
            slot.name: i for i, slot in enumerate(self.slots_info)
        }
        
        # 构建约束关系
        self.area_compatibility_matrix = self._build_area_compatibility_matrix()
        self.fixed_assignments = self._build_fixed_assignments()
        
        logger.info(f"约束管理器初始化完成:")
        logger.info(f"  可用槽位数: {len(self.slots_info)}")
        logger.info(f"  可放置科室数: {len(self.departments_info)}")
        logger.info(f"  面积容差: {self.area_tolerance_ratio}")
        logger.info(f"  面积兼容对数: {self.area_compatibility_matrix.sum()}")
        logger.info(f"  固定分配数: {len(self.fixed_assignments)}")
    
    def _initialize_slots_info(self) -> List[SlotInfo]:
        """初始化槽位信息"""
        slots_info = []
        for i, slot_name in enumerate(self.placeable_slots):
            # 从行程时间矩阵中获取槽位面积信息
            # 这里假设槽位面积存储在travel_times的索引属性中
            # 如果没有面积信息，使用默认值
            area = self._get_slot_area(slot_name)
            slots_info.append(SlotInfo(
                index=i,
                name=slot_name,
                area=area,
                is_available=True
            ))
        return slots_info
    
    def _initialize_departments_info(self) -> List[DepartmentInfo]:
        """初始化科室信息"""
        departments_info = []
        for dept_name in self.placeable_departments:
            # 获取科室面积需求
            area_req = self._get_department_area_requirement(dept_name)
            
            # 检查是否为固定科室
            is_fixed, fixed_pos = self._check_fixed_department(dept_name)
            
            departments_info.append(DepartmentInfo(
                name=dept_name,
                area_requirement=area_req,
                is_fixed=is_fixed,
                fixed_position=fixed_pos,
                adjacency_preferences=self._get_adjacency_preferences(dept_name)
            ))
        return departments_info
    
    def _get_slot_area(self, slot_name: str) -> float:
        """
        获取槽位面积
        
        Args:
            slot_name: 槽位名称
            
        Returns:
            float: 槽位面积
        """
        # 从CacheManager获取真实面积数据
        placeable_df = self.cache_manager.placeable_nodes_df
        area_data = placeable_df[placeable_df['node_id'] == slot_name]['area']
        if not area_data.empty:
            return float(area_data.iloc[0])
        else:
            logger.warning(f"未找到槽位 {slot_name} 的面积信息，使用默认值")
            return 100.0  # 默认面积
    
    def _get_department_area_requirement(self, dept_name: str) -> float:
        """
        获取科室面积需求
        
        Args:
            dept_name: 科室名称
            
        Returns:
            float: 面积需求
        """
        # 从CacheManager获取真实面积数据
        placeable_df = self.cache_manager.placeable_nodes_df
        area_data = placeable_df[placeable_df['node_id'] == dept_name]['area']
        if not area_data.empty:
            return float(area_data.iloc[0])
        else:
            logger.warning(f"未找到科室 {dept_name} 的面积信息，使用默认值")
            return 100.0  # 默认面积
    
    def _check_fixed_department(self, dept_name: str) -> Tuple[bool, Optional[int]]:
        """
        检查科室是否需要固定位置
        
        Args:
            dept_name: 科室名称
            
        Returns:
            Tuple[bool, Optional[int]]: (是否固定, 固定位置索引)
        """
        # 这里定义需要固定位置的科室
        # 例如：入口、急诊科等可能需要固定在特定位置
        fixed_departments = {
            '急诊科': 0,  # 固定在第一个槽位
            '入口': None,  # 固定但位置待定
        }
        
        if dept_name in fixed_departments:
            return True, fixed_departments[dept_name]
        return False, None
    
    def _get_adjacency_preferences(self, dept_name: str) -> List[str]:
        """
        获取科室相邻偏好
        
        Args:
            dept_name: 科室名称
            
        Returns:
            List[str]: 偏好相邻的科室列表
        """
        # 定义科室间的相邻偏好关系
        adjacency_preferences = {
            '妇科': ['超声科', '采血处'],
            '心血管内科': ['采血处', '超声科', '放射科'],
            '呼吸内科': ['采血处', '放射科'],
            '采血处': ['检验中心'],
            '超声科': ['放射科'],
        }
        
        return adjacency_preferences.get(dept_name, [])
    
    def _build_area_compatibility_matrix(self) -> any:
        """
        构建面积兼容性矩阵
        
        Returns:
            numpy.ndarray: 兼容性矩阵 [slots x departments]
        """
        import numpy as np
        
        num_slots = len(self.slots_info)
        num_depts = len(self.departments_info)
        compatibility = np.zeros((num_slots, num_depts), dtype=bool)
        
        for i, slot in enumerate(self.slots_info):
            for j, dept in enumerate(self.departments_info):
                # 检查面积兼容性
                area_diff = abs(slot.area - dept.area_requirement)
                max_allowed_diff = dept.area_requirement * self.area_tolerance_ratio
                compatibility[i, j] = area_diff <= max_allowed_diff
        
        return compatibility
    
    def _build_fixed_assignments(self) -> Dict[str, int]:
        """
        构建固定分配映射
        
        Returns:
            Dict[str, int]: 科室名 -> 槽位索引的映射
        """
        fixed_assignments = {}
        for dept in self.departments_info:
            if dept.is_fixed and dept.fixed_position is not None:
                fixed_assignments[dept.name] = dept.fixed_position
        return fixed_assignments
    
    def is_valid_layout(self, layout: List[str]) -> bool:
        """
        检查布局是否满足所有约束（优化检查顺序：先简单后复杂）
        
        Args:
            layout: 布局列表，索引为槽位，值为科室名
            
        Returns:
            bool: 是否有效
        """
        # 1. 最快的检查：长度匹配
        if len(layout) != len(self.slots_info):
            return False
        
        # 2. 快速检查：唯一性约束（O(n)）
        if not self._check_uniqueness_constraints(layout):
            return False
        
        # 3. 中等复杂度：固定位置约束（O(固定数量)）
        if not self._check_fixed_constraints(layout):
            return False
        
        # 4. 最复杂的检查：面积约束（需要矩阵查找）
        if not self._check_area_constraints(layout):
            return False
        
        return True
    
    def _check_uniqueness_constraints(self, layout: List[str]) -> bool:
        """检查唯一性约束（每个科室只能出现一次，不允许null）- 优化版"""
        # 首先检查是否有None值
        if None in layout:
            logger.debug(f"布局中包含None值")
            return False
        
        seen = set()
        for dept_name in layout:
            # 已经检查过None，这里dept_name不会是None
            if dept_name in seen:
                return False  # 发现重复，立即返回
            seen.add(dept_name)
        
        # 检查是否所有科室都被放置
        placed_depts = seen
        all_depts = set(dept.name for dept in self.departments_info)
        missing_depts = all_depts - placed_depts
        
        if missing_depts:
            logger.debug(f"缺少科室: {missing_depts}")
            return False
        
        return True
    
    def _check_fixed_constraints(self, layout: List[str]) -> bool:
        """检查固定位置约束"""
        for dept_name, fixed_slot_idx in self.fixed_assignments.items():
            if layout[fixed_slot_idx] != dept_name:
                return False
        return True
    
    def _check_area_constraints(self, layout: List[str]) -> bool:
        """检查面积约束"""
        for slot_idx, dept_name in enumerate(layout):
            if dept_name is None:
                continue
            
            dept_idx = self._get_department_index(dept_name)
            if dept_idx is None:
                return False
            
            if not self.area_compatibility_matrix[slot_idx, dept_idx]:
                return False
        
        return True
    
    
    def _get_department_index(self, dept_name: str) -> Optional[int]:
        """获取科室在departments_info中的索引（使用哈希表优化）"""
        return self.dept_name_to_index.get(dept_name)
    
    def generate_original_layout(self) -> List[str]:
        """
        生成原始布局（未经优化的基准布局）
        将科室按顺序直接映射到槽位，不进行任何随机化或优化
        
        Returns:
            List[str]: 原始布局
        """
        # 在这个系统中，placeable_departments 和 placeable_slots 是相同的
        # 原始布局就是简单地将科室按原始顺序排列
        # 确保返回的是副本，避免外部修改影响原始列表
        layout = self.placeable_departments.copy()
        
        # 验证布局完整性
        if len(layout) != len(self.slots_info):
            logger.warning(f"科室数量({len(layout)})与槽位数量({len(self.slots_info)})不匹配")
            # 如果数量不匹配，调整布局长度
            while len(layout) < len(self.slots_info):
                # 这种情况不应该发生，但为了健壮性添加处理
                logger.error(f"科室数量少于槽位数量，这不应该发生！")
                layout.append(layout[0])  # 临时填充，避免None
            layout = layout[:len(self.slots_info)]
        
        return layout
    
    def generate_valid_layout(self) -> List[str]:
        """
        生成一个满足约束的有效布局
        
        Returns:
            List[str]: 有效布局
        """
        # 首先创建一个包含所有科室的列表
        available_depts = list(self.placeable_departments)
        layout = []
        
        # 如果有固定位置约束，先处理
        fixed_positions = {} #TODO: 固定位置约束
        for dept_name, slot_idx in self.fixed_assignments.items():
            if dept_name in available_depts:
                fixed_positions[slot_idx] = dept_name
                available_depts.remove(dept_name)
        
        # 随机打乱剩余科室
        random.shuffle(available_depts)
        
        # 填充布局
        dept_idx = 0
        for slot_idx in range(len(self.slots_info)):
            if slot_idx in fixed_positions:
                # 使用固定位置的科室
                layout.append(fixed_positions[slot_idx])
            else:
                # 填充剩余科室
                if dept_idx < len(available_depts):
                    layout.append(available_depts[dept_idx])
                    dept_idx += 1
                else:
                    # 不应该发生，但为了健壮性
                    logger.error(f"生成布局时科室不足，这不应该发生！")
                    # 使用第一个可用科室填充
                    layout.append(self.placeable_departments[0])
        
        # 验证生成的布局
        if not self.is_valid_layout(layout):
            logger.warning("生成的布局不满足约束，返回原始布局")
            return self.generate_original_layout()
        
        return layout
    
    def get_compatible_departments(self, slot_idx: int) -> List[str]:
        """
        获取与指定槽位兼容的科室列表
        
        Args:
            slot_idx: 槽位索引
            
        Returns:
            List[str]: 兼容的科室名称列表
        """
        compatible_depts = []
        for dept_idx, dept in enumerate(self.departments_info):
            if self.area_compatibility_matrix[slot_idx, dept_idx]:
                compatible_depts.append(dept.name)
        return compatible_depts
    
    def get_swap_candidates(self, layout: List[str], slot1: int, slot2: int) -> bool:
        """
        检查两个槽位是否可以交换科室
        
        Args:
            layout: 当前布局
            slot1: 槽位1索引
            slot2: 槽位2索引
            
        Returns:
            bool: 是否可以交换
        """
        dept1, dept2 = layout[slot1], layout[slot2]
        
        # 创建交换后的布局进行验证
        new_layout = layout.copy()
        new_layout[slot1], new_layout[slot2] = dept2, dept1
        
        return self.is_valid_layout(new_layout)
    
    def calculate_constraint_violation_penalty(self, layout: List[str]) -> float:
        """
        计算布局的约束违反惩罚
        
        Args:
            layout: 布局
            
        Returns:
            float: 惩罚值（0表示无违反）
        """
        penalty = 0.0
        
        # 固定位置违反惩罚
        for dept_name, fixed_slot_idx in self.fixed_assignments.items():
            if layout[fixed_slot_idx] != dept_name:
                penalty += 1000.0  # 高惩罚
        
        # 面积约束违反惩罚
        for slot_idx, dept_name in enumerate(layout):
            if dept_name is None:
                continue
            
            dept_idx = self._get_department_index(dept_name)
            if dept_idx is not None and not self.area_compatibility_matrix[slot_idx, dept_idx]:
                penalty += 500.0  # 中等惩罚
        
        return penalty


class SmartConstraintRepairer:
    """
    智能约束修复器
    
    提供多种约束修复策略，用于修复违反约束的布局。
    支持贪心面积匹配、交换优化、随机修复等多种修复方法。
    """
    
    def __init__(self, constraint_manager: ConstraintManager):
        """
        初始化智能约束修复器
        
        Args:
            constraint_manager: 约束管理器实例
        """
        self.constraint_manager = constraint_manager
        self.logger = setup_logger(__name__)
        
        # 修复统计信息
        self.repair_attempts = 0
        self.successful_repairs = 0
        
    def repair_layout(self, 
                     layout: List[str], 
                     strategy: str = 'greedy_area_matching', 
                     max_attempts: int = 10) -> List[str]:
        """
        修复布局约束违反
        
        Args:
            layout: 待修复的布局
            strategy: 修复策略 ('greedy_area_matching', 'swap_optimization', 'random_repair')
            max_attempts: 最大修复尝试次数
            
        Returns:
            List[str]: 修复后的布局
        """
        self.repair_attempts += 1
        
        if self.constraint_manager.is_valid_layout(layout):
            return layout.copy()
        
        # 根据策略选择修复方法
        if strategy == 'greedy_area_matching':
            repaired = self._greedy_area_matching_repair(layout, max_attempts)
        elif strategy == 'swap_optimization':
            repaired = self._swap_optimization_repair(layout, max_attempts)
        elif strategy == 'random_repair':
            repaired = self._random_repair(layout, max_attempts)
        else:
            self.logger.warning(f"未知修复策略: {strategy}, 使用默认策略")
            repaired = self._greedy_area_matching_repair(layout, max_attempts)
        
        if self.constraint_manager.is_valid_layout(repaired):
            self.successful_repairs += 1
            self.logger.debug(f"布局修复成功，策略: {strategy}")
        else:
            self.logger.warning(f"布局修复失败，策略: {strategy}, 生成新的有效布局")
            repaired = self.constraint_manager.generate_valid_layout()
        
        return repaired
    
    def _greedy_area_matching_repair(self, layout: List[str], max_attempts: int) -> List[str]:
        """
        贪心面积匹配修复策略
        
        基于面积兼容性重新分配科室到槽位，优先考虑面积匹配度最高的组合。
        
        Args:
            layout: 待修复的布局
            max_attempts: 最大尝试次数
            
        Returns:
            List[str]: 修复后的布局
        """
        # 获取所有可放置的科室和槽位
        all_departments = list(self.constraint_manager.placeable_departments)
        all_slots = list(range(len(self.constraint_manager.slots_info)))
        
        # 创建面积匹配度矩阵
        import numpy as np
        
        compatibility_scores = np.zeros((len(all_slots), len(all_departments)))
        
        for slot_idx in all_slots:
            for dept_idx, dept_name in enumerate(all_departments):
                slot_info = self.constraint_manager.slots_info[slot_idx]
                dept_info = self.constraint_manager.departments_info[dept_idx]
                
                # 计算面积匹配度（越接近1表示匹配度越高）
                area_diff = abs(slot_info.area - dept_info.area_requirement)
                max_area = max(slot_info.area, dept_info.area_requirement)
                compatibility_scores[slot_idx, dept_idx] = 1.0 - (area_diff / max_area) if max_area > 0 else 0.0
        
        # 使用匈牙利算法或贪心算法进行最优匹配
        repaired_layout = self._hungarian_assignment(all_slots, all_departments, compatibility_scores)
        
        # 如果匈牙利算法失败，使用贪心备选方案
        if repaired_layout is None:
            repaired_layout = self._greedy_assignment(all_slots, all_departments, compatibility_scores)
        
        return repaired_layout
    
    def _hungarian_assignment(self, 
                             slots: List[int], 
                             departments: List[str], 
                             scores: 'np.ndarray') -> Optional[List[str]]:
        """
        使用匈牙利算法进行最优分配
        
        Args:
            slots: 槽位索引列表
            departments: 科室名称列表
            scores: 兼容性得分矩阵
            
        Returns:
            Optional[List[str]]: 分配结果布局，失败时返回None
        """
        try:
            from scipy.optimize import linear_sum_assignment
            
            # 将最大化问题转换为最小化问题
            cost_matrix = 1.0 - scores
            
            # 解决分配问题
            slot_indices, dept_indices = linear_sum_assignment(cost_matrix)
            
            # 构建布局
            layout = [''] * len(slots)
            for slot_idx, dept_idx in zip(slot_indices, dept_indices):
                layout[slot_idx] = departments[dept_idx]
            
            # 验证分配是否满足约束
            if self.constraint_manager.is_valid_layout(layout):
                return layout
            else:
                return None
                
        except ImportError:
            self.logger.warning("scipy未安装，无法使用匈牙利算法，回退到贪心算法")
            return None
        except Exception as e:
            self.logger.warning(f"匈牙利算法执行失败: {e}")
            return None
    
    def _greedy_assignment(self, 
                          slots: List[int], 
                          departments: List[str], 
                          scores: 'np.ndarray') -> List[str]:
        """
        贪心分配算法
        
        按照兼容性得分从高到低进行分配。
        
        Args:
            slots: 槽位索引列表
            departments: 科室名称列表
            scores: 兼容性得分矩阵
            
        Returns:
            List[str]: 分配结果布局
        """
        import numpy as np
        
        layout = [''] * len(slots)
        used_departments = set()
        used_slots = set()
        
        # 处理固定位置约束
        for dept_name, fixed_slot_idx in self.constraint_manager.fixed_assignments.items():
            if dept_name in departments and fixed_slot_idx in slots:
                layout[fixed_slot_idx] = dept_name
                used_departments.add(dept_name)
                used_slots.add(fixed_slot_idx)
        
        # 获取所有未分配的(槽位,科室)对及其得分
        candidates = []
        for slot_idx in slots:
            if slot_idx not in used_slots:
                for dept_idx, dept_name in enumerate(departments):
                    if dept_name not in used_departments:
                        score = scores[slot_idx, dept_idx]
                        candidates.append((score, slot_idx, dept_name))
        
        # 按得分降序排序
        candidates.sort(key=lambda x: x[0], reverse=True)
        
        # 贪心分配
        for score, slot_idx, dept_name in candidates:
            if slot_idx not in used_slots and dept_name not in used_departments:
                # 检查面积约束
                dept_idx = self.constraint_manager.dept_name_to_index.get(dept_name)
                if (dept_idx is not None and 
                    self.constraint_manager.area_compatibility_matrix[slot_idx, dept_idx]):
                    layout[slot_idx] = dept_name
                    used_departments.add(dept_name)
                    used_slots.add(slot_idx)
        
        # 填充剩余未分配的位置（如果有）
        remaining_depts = [d for d in departments if d not in used_departments]
        remaining_slots = [s for s in slots if s not in used_slots]
        
        for slot_idx, dept_name in zip(remaining_slots, remaining_depts):
            layout[slot_idx] = dept_name
        
        return layout
    
    def _swap_optimization_repair(self, layout: List[str], max_attempts: int) -> List[str]:
        """
        交换优化修复策略
        
        通过局部交换操作改善布局的约束满足度。
        
        Args:
            layout: 待修复的布局
            max_attempts: 最大尝试次数
            
        Returns:
            List[str]: 修复后的布局
        """
        current_layout = layout.copy()
        
        for attempt in range(max_attempts):
            if self.constraint_manager.is_valid_layout(current_layout):
                break
            
            # 寻找违反约束的位置
            violation_positions = self._find_constraint_violations(current_layout)
            
            if not violation_positions:
                break
            
            # 随机选择一个违反位置进行修复
            violation_pos = random.choice(violation_positions)
            
            # 寻找可以交换的位置
            for swap_pos in range(len(current_layout)):
                if swap_pos != violation_pos:
                    if self.constraint_manager.get_swap_candidates(current_layout, violation_pos, swap_pos):
                        # 执行交换
                        current_layout[violation_pos], current_layout[swap_pos] = \
                            current_layout[swap_pos], current_layout[violation_pos]
                        break
            
            # 如果交换后仍有问题，尝试随机重分配违反位置
            if violation_pos in self._find_constraint_violations(current_layout):
                compatible_depts = self.constraint_manager.get_compatible_departments(violation_pos)
                if compatible_depts:
                    # 随机选择一个兼容的科室
                    new_dept = random.choice(compatible_depts)
                    # 找到这个科室当前的位置并交换
                    if new_dept in current_layout:
                        current_pos = current_layout.index(new_dept)
                        current_layout[violation_pos], current_layout[current_pos] = \
                            current_layout[current_pos], current_layout[violation_pos]
        
        return current_layout
    
    def _random_repair(self, layout: List[str], max_attempts: int) -> List[str]:
        """
        随机修复策略
        
        通过随机重新排列违反约束的科室来修复布局。
        
        Args:
            layout: 待修复的布局
            max_attempts: 最大尝试次数
            
        Returns:
            List[str]: 修复后的布局
        """
        current_layout = layout.copy()
        
        for attempt in range(max_attempts):
            if self.constraint_manager.is_valid_layout(current_layout):
                break
            
            # 处理唯一性约束违反
            seen = set()
            duplicates = []
            for i, dept in enumerate(current_layout):
                if dept in seen:
                    duplicates.append(i)
                else:
                    seen.add(dept)
            
            # 处理重复科室
            if duplicates:
                all_depts = set(self.constraint_manager.placeable_departments)
                missing_depts = list(all_depts - seen)
                random.shuffle(missing_depts)
                
                for i, dup_idx in enumerate(duplicates):
                    if i < len(missing_depts):
                        current_layout[dup_idx] = missing_depts[i]
            
            # 处理面积约束违反
            violation_positions = self._find_constraint_violations(current_layout)
            for pos in violation_positions:
                compatible_depts = self.constraint_manager.get_compatible_departments(pos)
                if compatible_depts:
                    # 随机选择兼容科室
                    new_dept = random.choice(compatible_depts)
                    # 如果该科室已在布局中，找到其位置并交换
                    if new_dept in current_layout:
                        other_pos = current_layout.index(new_dept)
                        current_layout[pos], current_layout[other_pos] = \
                            current_layout[other_pos], current_layout[pos]
        
        return current_layout
    
    def _find_constraint_violations(self, layout: List[str]) -> List[int]:
        """
        查找违反约束的位置
        
        Args:
            layout: 布局
            
        Returns:
            List[int]: 违反约束的槽位索引列表
        """
        violations = []
        
        # 检查面积约束违反
        for slot_idx, dept_name in enumerate(layout):
            if dept_name is None:
                continue
            
            dept_idx = self.constraint_manager._get_department_index(dept_name)
            if dept_idx is not None and not self.constraint_manager.area_compatibility_matrix[slot_idx, dept_idx]:
                violations.append(slot_idx)
        
        # 检查固定位置约束违反
        for dept_name, fixed_slot_idx in self.constraint_manager.fixed_assignments.items():
            if layout[fixed_slot_idx] != dept_name:
                violations.append(fixed_slot_idx)
        
        return list(set(violations))  # 去重
    
    def get_repair_statistics(self) -> Dict[str, Any]:
        """
        获取修复统计信息
        
        Returns:
            Dict[str, Any]: 统计信息
        """
        success_rate = (self.successful_repairs / self.repair_attempts 
                       if self.repair_attempts > 0 else 0.0)
        
        return {
            'total_repair_attempts': self.repair_attempts,
            'successful_repairs': self.successful_repairs,
            'repair_success_rate': success_rate
        }
    
    def reset_statistics(self):
        """重置修复统计信息"""
        self.repair_attempts = 0
        self.successful_repairs = 0
</file>

<file path="src/network/node_creators.py">
"""
Defines strategies for creating different types of nodes in the network.

This module uses the Strategy design pattern where each node type (Room,
Connection, Pedestrian, etc.) has its own creator class derived from a
base class.
"""

import abc
import cv2
import numpy as np
from src.rl_optimizer.utils.setup import setup_logger
from scipy.spatial import KDTree
from typing import Dict, Tuple, List, Any, Optional

from src.config import NetworkConfig
from .node import Node
from .graph_manager import GraphManager
from src.image_processing.processor import ImageProcessor

logger = setup_logger(__name__)


class BaseNodeCreator(abc.ABC):
    """
    Abstract base class for node creators.
    """

    def __init__(self,
                 config: NetworkConfig,
                 color_map_data: Dict[Tuple[int, int, int], Dict[str, Any]],
                 image_processor: ImageProcessor,
                 graph_manager: GraphManager):
        self.config = config
        self.color_map_data = color_map_data
        self.image_processor = image_processor
        self.graph_manager = graph_manager
        self.types_map_name_to_rgb: Dict[str, Tuple[int, int, int]] = \
            {details['name']: rgb for rgb, details in color_map_data.items()}
        self.types_map_name_to_time: Dict[str, float] = \
            {details['name']: details.get('time', 1.0)
             for rgb, details in color_map_data.items()}

    def _get_color_rgb_by_name(self, type_name: str) -> Optional[Tuple[int, int, int]]:
        return self.types_map_name_to_rgb.get(type_name)

    def _get_time_by_name(self, type_name: str) -> float:
        return self.types_map_name_to_time.get(type_name, self.config.PEDESTRIAN_TIME)

    def _get_ename_by_name(self, type_name: str) -> Optional[str]:
        return self.color_map_data.get(self._get_color_rgb_by_name(type_name), {}).get('e_name')

    def _get_code_by_name(self, type_name: str) -> Optional[Dict[str, Any]]:
        return self.color_map_data.get(self._get_color_rgb_by_name(type_name), {}).get('code')

    def _create_mask_for_type(self,
                              image_data: np.ndarray,
                              target_type_name: str,
                              apply_morphology: bool = True,
                              morphology_operation: str = 'close_open',
                              morphology_kernel_size: Optional[Tuple[int, int]] = None
                              ) -> Optional[np.ndarray]:
        """Creates a binary mask for a single specified node type."""
        color_rgb = self._get_color_rgb_by_name(target_type_name)
        if color_rgb is None:
            logger.warning(f"Warning: Color for type '{target_type_name}' not found. Cannot create mask.")
            return None

        mask = np.all(image_data == np.array(
            color_rgb, dtype=np.uint8).reshape(1, 1, 3), axis=2)
        mask = mask.astype(np.uint8) * 255

        if apply_morphology:
            kernel_size = morphology_kernel_size or self.config.MORPHOLOGY_KERNEL_SIZE
            mask = self.image_processor.apply_morphology(
                mask,
                operation=morphology_operation,
                kernel_size=kernel_size
            )
        return mask

    def _find_connected_components(self, mask: np.ndarray, connectivity: int = 4) \
            -> Tuple[int, np.ndarray, np.ndarray, np.ndarray]:
        return cv2.connectedComponentsWithStats(mask, connectivity=connectivity)

    @abc.abstractmethod
    def create_nodes(self,
                     processed_image_data: np.ndarray,
                     id_map: np.ndarray,
                     z_level: int):
        pass

class RoomNodeCreator(BaseNodeCreator):
    """Creates nodes for room-type areas."""
    def create_nodes(self, processed_image_data: np.ndarray, id_map: np.ndarray, z_level: int):
        target_room_types = self.config.ROOM_TYPES
        if not target_room_types: return

        for room_type_name in target_room_types:
            mask = self._create_mask_for_type(processed_image_data, room_type_name)
            if mask is None: continue

            retval, labels, stats, centroids = self._find_connected_components(mask)
            if retval <= 1: continue

            node_time = self._get_time_by_name(room_type_name)
            e_name = self._get_ename_by_name(room_type_name)
            code = self._get_code_by_name(room_type_name)

            for i in range(1, retval):
                area = float(stats[i, cv2.CC_STAT_AREA])
                if area < self.config.AREA_THRESHOLD: continue

                centroid_x, centroid_y = centroids[i]
                position = (int(centroid_x), int(centroid_y), z_level)
                node_id = self.graph_manager.generate_node_id()
                room_node = Node(node_id=node_id, node_type=room_type_name,
                                x=position[0], y=position[1], z=position[2],
                                time=node_time, area=area, e_name=e_name, code=code)
                self.graph_manager.add_node(room_node)
                id_map[labels == i] = room_node.id

class VerticalNodeCreator(BaseNodeCreator):
    """Creates nodes for vertical transport areas (stairs, elevators, escalators)."""
    def create_nodes(self, processed_image_data: np.ndarray, id_map: np.ndarray, z_level: int):
        target_vertical_types = self.config.VERTICAL_TYPES
        if not target_vertical_types: return

        for vertical_type_name in target_vertical_types:
            mask = self._create_mask_for_type(processed_image_data, vertical_type_name)
            if mask is None: continue

            retval, labels, stats, centroids = self._find_connected_components(mask)
            if retval <= 1: continue

            node_time = self._get_time_by_name(vertical_type_name)
            e_name = self._get_ename_by_name(vertical_type_name)
            code = self._get_code_by_name(vertical_type_name)

            for i in range(1, retval):
                area = float(stats[i, cv2.CC_STAT_AREA])
                if area < self.config.AREA_THRESHOLD: continue

                centroid_x, centroid_y = centroids[i]
                position = (int(centroid_x), int(centroid_y), z_level)
                node_id = self.graph_manager.generate_node_id()
                v_node = Node(node_id=node_id, node_type=vertical_type_name,
                              x=position[0], y=position[1], z=position[2],
                              time=node_time, area=area, e_name=e_name, code=code)
                self.graph_manager.add_node(v_node)
                id_map[labels == i] = v_node.id

class MeshBasedNodeCreator(BaseNodeCreator): # New base for Pedestrian and Outside
    """Base class for creators that generate a mesh of nodes within areas."""

    def _create_mesh_nodes_for_mask(self,
                                    mask: np.ndarray,
                                    region_type_name: str,
                                    id_map: np.ndarray, # Pass id_map to update
                                    id_map_value_for_area: int, # Value to mark the area in id_map
                                    z_level: int,
                                    node_time: float,
                                    e_name: Optional[str],
                                    code: Optional[str],
                                    grid_size_multiplier: int):
        """Helper to create mesh nodes within a given mask and connect them."""
        # First, mark the entire area in id_map with the special area identifier
        id_map[mask != 0] = id_map_value_for_area

        retval, labels, stats, _ = self._find_connected_components(mask, connectivity=8)
        grid_size = self.config.GRID_SIZE * grid_size_multiplier
        # Estimated area for a single mesh node
        mesh_node_area = float(1)


        for i in range(1, retval): # For each connected component
            component_area = stats[i, cv2.CC_STAT_AREA]
            if component_area < self.config.AREA_THRESHOLD: # Ensure component itself is large enough
                continue

            x_stat, y_stat, w_stat, h_stat, _ = stats[i]

            gx = np.arange(x_stat + grid_size / 2, x_stat + w_stat, grid_size) # Center points in grid cells
            gy = np.arange(y_stat + grid_size / 2, y_stat + h_stat, grid_size)
            if len(gx) == 0 or len(gy) == 0: continue # Avoid empty grid

            grid_points_x, grid_points_y = np.meshgrid(gx, gy)

            grid_points_y_int = grid_points_y.astype(int).clip(0, mask.shape[0] - 1)
            grid_points_x_int = grid_points_x.astype(int).clip(0, mask.shape[1] - 1)
            
            valid_mask_indices = labels[grid_points_y_int, grid_points_x_int] == i
            
            valid_x_coords = grid_points_x[valid_mask_indices]
            valid_y_coords = grid_points_y[valid_mask_indices]

            component_nodes: List[Node] = []
            for vx, vy in zip(valid_x_coords, valid_y_coords):
                pos = (int(vx), int(vy), z_level)
                node_id = self.graph_manager.generate_node_id()
                mesh_node = Node(node_id=node_id, node_type=region_type_name,
                                x=pos[0], y=pos[1], z=pos[2],
                                time=node_time, area=mesh_node_area,
                                e_name=e_name, code=code)
                self.graph_manager.add_node(mesh_node)
                component_nodes.append(mesh_node)
                # Optionally, mark the exact grid cell in id_map with the mesh_node.id
                # id_map[int(vy-grid_size/2):int(vy+grid_size/2), int(vx-grid_size/2):int(vx+grid_size/2)] = mesh_node.id
                # For now, the broader area is already marked.

            if not component_nodes or len(component_nodes) < 2:
                continue

            node_positions_2d = np.array([(node.x, node.y) for node in component_nodes])
            kdtree = KDTree(node_positions_2d)
            # Max distance to connect (diagonal of a grid cell, plus a small tolerance)
            max_distance_connect = np.sqrt(2) * grid_size * 1.05

            for j, current_node in enumerate(component_nodes):
                # Query for k-nearest, then filter by distance
                # k=9 includes self + 8 neighbors in a square grid
                distances, indices_k_nearest = kdtree.query(
                    (current_node.x, current_node.y),
                    k=min(len(component_nodes), self.config.MESH_NODE_CONNECTIVITY_K), # Ensure k is not > num_points
                    distance_upper_bound=max_distance_connect
                )

                for dist_val, neighbor_idx in zip(distances, indices_k_nearest):
                    if neighbor_idx >= len(component_nodes) or dist_val > max_distance_connect :
                        continue # Out of bounds or too far

                    neighbor_node = component_nodes[neighbor_idx]
                    if current_node.id == neighbor_node.id:
                        continue
                    
                    self.graph_manager.connect_nodes_by_ids(current_node.id, neighbor_node.id)

class PedestrianNodeCreator(MeshBasedNodeCreator):
    """Creates mesh nodes for pedestrian areas (e.g., corridors)."""
    def create_nodes(self, processed_image_data: np.ndarray, id_map: np.ndarray, z_level: int):
        target_pedestrian_types = self.config.PEDESTRIAN_TYPES
        if not target_pedestrian_types: return

        for ped_type_name in target_pedestrian_types:
            e_name = self._get_ename_by_name(ped_type_name)
            code = self._get_code_by_name(ped_type_name)
            mask = self._create_mask_for_type(processed_image_data, ped_type_name)
            if mask is None: continue
            
            self._create_mesh_nodes_for_mask(
                mask=mask,
                region_type_name=ped_type_name,
                id_map=id_map,
                id_map_value_for_area=self.config.PEDESTRIAN_ID_MAP_VALUE,
                z_level=z_level,
                node_time=self.config.PEDESTRIAN_TIME,
                grid_size_multiplier=1,
                e_name=e_name,
                code=code
            )

class OutsideNodeCreator(MeshBasedNodeCreator):
    """Creates mesh nodes for outside areas."""
    def create_nodes(self, processed_image_data: np.ndarray, id_map: np.ndarray, z_level: int):
        target_outside_types = self.config.OUTSIDE_TYPES
        if not target_outside_types: return

        for outside_type_name in target_outside_types: # Should typically be just one '室外'
            mask = self._create_mask_for_type(processed_image_data, outside_type_name)
            if mask is None: continue
            
            self._create_mesh_nodes_for_mask(
                mask=mask,
                region_type_name=outside_type_name,
                id_map=id_map,
                id_map_value_for_area=self.config.OUTSIDE_ID_MAP_VALUE,
                z_level=z_level,
                node_time=self._get_time_by_name(outside_type_name) * self.config.OUTSIDE_MESH_TIMES_FACTOR,
                grid_size_multiplier=self.config.OUTSIDE_MESH_TIMES_FACTOR
            )

class ConnectionNodeCreator(BaseNodeCreator):
    """Creates nodes for connections (e.g., doors) and links them to adjacent areas."""
    def create_nodes(self, processed_image_data: np.ndarray, id_map: np.ndarray, z_level: int):
        target_connection_types = self.config.CONNECTION_TYPES
        if not target_connection_types: return

        pass_through_ids_in_id_map = [self.config.BACKGROUND_ID_MAP_VALUE]
        dilation_kernel_np = np.ones(self.config.CONNECTION_DILATION_KERNEL_SIZE, np.uint8)

        for conn_type_name in target_connection_types:
            mask = self._create_mask_for_type(processed_image_data, conn_type_name)
            e_name = self._get_ename_by_name(conn_type_name)
            code = self._get_code_by_name(conn_type_name)
            if mask is None: continue

            retval, labels, stats, centroids = self._find_connected_components(mask)
            if retval <= 1: continue

            for i in range(1, retval): # For each door component
                area = float(stats[i, cv2.CC_STAT_AREA])
                # Doors can be smaller, adjust threshold if needed, e.g., AREA_THRESHOLD / 4
                if area < self.config.AREA_THRESHOLD / 10 and area < 5: # Allow very small doors
                    continue

                centroid_x, centroid_y = centroids[i]
                position = (int(centroid_x), int(centroid_y), z_level)

                node_id = self.graph_manager.generate_node_id()
                conn_node = Node(node_id=node_id, node_type=conn_type_name,
                                x=position[0], y=position[1], z=position[2],
                                time=self.config.CONNECTION_TIME, area=area,
                                e_name=e_name, code=code)
                self.graph_manager.add_node(conn_node)

                component_mask_pixels = (labels == i)
                id_map[component_mask_pixels] = conn_node.id # Mark door pixels with its own ID

                # Dilate the door component mask to find neighboring regions/nodes in id_map
                # Need to convert boolean mask `component_mask_pixels` to uint8 for dilate
                uint8_component_mask = component_mask_pixels.astype(np.uint8) * 255
                dilated_component_mask = cv2.dilate(uint8_component_mask, dilation_kernel_np, iterations=1)
                
                neighbor_ids_in_map = np.unique(id_map[dilated_component_mask != 0])

                # Determine door type
                is_connected_to_outside = self.config.OUTSIDE_ID_MAP_VALUE in neighbor_ids_in_map
                is_connected_to_pedestrian = self.config.PEDESTRIAN_ID_MAP_VALUE in neighbor_ids_in_map

                if is_connected_to_outside:
                    conn_node.door_type = 'out'
                elif is_connected_to_pedestrian:
                    # If it connects to pedestrian and NOT to outside, it's an 'in' door (e.g. from corridor to room)
                    # Or if it connects pedestrian to room.
                    # If a door connects pedestrian area to an outside area, it's more like an 'out' door.
                    # This needs careful definition based on your use case.
                    # For now: if it sees pedestrian and not outside, consider it 'in' (towards rooms/internal).
                    # If it sees pedestrian AND outside, the 'out' takes precedence.
                    conn_node.door_type = 'in'
                else: # Connects only to actual nodes (rooms, vertical, other doors)
                    conn_node.door_type = 'room' # Default for internal doors

                # Connect the door node to the identified neighboring ACTUAL nodes
                for neighbor_id_val in neighbor_ids_in_map:
                    if neighbor_id_val == conn_node.id or neighbor_id_val in pass_through_ids_in_id_map:
                        continue
                    
                    # Check if it's an actual node ID (positive)
                    # Special area IDs (OUTSIDE_ID_MAP_VALUE, PEDESTRIAN_ID_MAP_VALUE) are negative or large positive.
                    if neighbor_id_val > 0: # Assuming actual node IDs are positive and start from 1
                        target_node = self.graph_manager.get_node_by_id(neighbor_id_val)
                        if target_node and target_node.id != conn_node.id:
                            self.graph_manager.connect_nodes_by_ids(conn_node.id, target_node.id)
</file>

<file path="src/plotting/plotter.py">
"""
Defines plotter classes for visualizing network graphs using Matplotlib and Plotly.
"""
import abc
import pathlib
from src.rl_optimizer.utils.setup import setup_logger
import networkx as nx
import numpy as np
import plotly.graph_objects as go
from typing import Dict, Tuple, Any, List, Optional

from src.config import NetworkConfig  # 依赖配置类
from src.network.node import Node     # 依赖节点类

logger = setup_logger(__name__)


class BasePlotter(abc.ABC):
    """
    Abstract base class for graph plotters.
    """

    def __init__(self,
                 config: NetworkConfig,
                 color_map_data: Dict[Tuple[int, int, int], Dict[str, Any]]):
        """
        Initializes the BasePlotter.

        Args:
            config: The network configuration object.
            color_map_data: The global color map dictionary.
        """
        self.config = config
        self.color_map_data = color_map_data
        self.type_to_plot_color_cache: Dict[str, str] = {}  # 缓存节点类型到绘图颜色的映射

    def _validate_node_coordinates(self, node: Node, node_id: str = None) -> bool:
        """
        验证节点坐标的有效性。
        
        Args:
            node: 要验证的节点对象
            node_id: 节点ID（用于错误报告）
            
        Returns:
            bool: 坐标是否有效
        """
        try:
            if not hasattr(node, 'x') or not hasattr(node, 'y') or not hasattr(node, 'z'):
                return False
            
            coords = [node.x, node.y, node.z]
            return all(
                isinstance(c, (int, float)) and 
                not np.isnan(c) and 
                np.isfinite(c) 
                for c in coords
            )
        except (TypeError, AttributeError):
            return False

    def _get_edge_type_config(self) -> Dict[str, Dict[str, Any]]:
        """
        获取边类型的样式配置。
        
        Returns:
            Dict: 边类型配置字典
        """
        return {
            'horizontal': {
                'color': self.config.HORIZONTAL_EDGE_COLOR,
                'width': self.config.EDGE_WIDTH,
                'name': 'Horizontal Connection',
                'dash': None
            },
            'vertical': {
                'color': self.config.VERTICAL_EDGE_COLOR, 
                'width': self.config.EDGE_WIDTH * 1.5,
                'name': 'Vertical Connection',
                'dash': None
            },
            'door': {
                'color': self.config.DOOR_EDGE_COLOR,
                'width': self.config.EDGE_WIDTH * 1.2,
                'name': 'Door Connection',
                'dash': None
            },
            'special': {
                'color': self.config.SPECIAL_EDGE_COLOR,
                'width': self.config.EDGE_WIDTH * 1.3,
                'name': 'Special Connection',
                'dash': None
            }
        }

    def _classify_edge_type(self, start_node: Node, end_node: Node, edge_attr: dict, z0: float, z1: float) -> str:
        """
        智能分类边的类型，用于不同的可视化样式。
        
        使用精确匹配而非包含匹配，避免误分类问题。
        
        Args:
            start_node: 起始节点对象
            end_node: 终止节点对象
            edge_attr: 边的属性字典
            z0: 起始节点Z坐标
            z1: 终止节点Z坐标
            
        Returns:
            边类型字符串：'horizontal', 'vertical', 'door', 'special'
            
        Raises:
            ValueError: 当节点对象无效时抛出异常
        """
        # 输入验证
        if not isinstance(start_node, Node) or not isinstance(end_node, Node):
            raise ValueError(f"Invalid node objects: start_node={type(start_node)}, end_node={type(end_node)}")
        
        if not hasattr(start_node, 'node_type') or not hasattr(end_node, 'node_type'):
            raise ValueError("Node objects must have 'node_type' attribute")
        
        if start_node.node_type is None or end_node.node_type is None:
            raise ValueError("Node types cannot be None")
        
        # 检查Z坐标有效性
        if not isinstance(z0, (int, float)) or not isinstance(z1, (int, float)):
            raise ValueError(f"Invalid Z coordinates: z0={type(z0)}, z1={type(z1)}")
        
        # 检查Z坐标数值有效性（NaN和无穷大）
        if not (np.isfinite(z0) and np.isfinite(z1)):
            raise ValueError(f"Z coordinates must be finite: z0={z0}, z1={z1}")
        
        # 检查是否为垂直连接（跨楼层）
        z_diff_threshold = getattr(self.config, 'VERTICAL_CONNECTION_Z_THRESHOLD', 0.1)
        if abs(z0 - z1) > z_diff_threshold:
            return 'vertical'
        
        # 使用配置中的类型定义进行精确匹配
        start_type = str(start_node.node_type).strip()
        end_type = str(end_node.node_type).strip()

        # 检查是否涉及Door连接（使用配置中的CONNECTION_TYPES）
        connection_types = set(getattr(self.config, 'CONNECTION_TYPES', ['Door']))
        if start_type in connection_types or end_type in connection_types:
            return 'door'
        
        # 检查是否为特殊类型连接（使用配置中的VERTICAL_TYPES）
        vertical_types = set(getattr(self.config, 'VERTICAL_TYPES', ['电梯', '扶梯', '楼梯']))
        if start_type in vertical_types or end_type in vertical_types:
            return 'special'
        
        # 默认为水平连接
        return 'horizontal'

    def _get_node_color(self, node_type: str) -> str:
        """
        Determines the plotting color for a given node type.

        Uses colors from `color_map_data` if `NODE_COLOR_FROM_MAP` is True in config,
        otherwise uses a default Plotly color. Caches results.

        Args:
            node_type: The type of the node (e.g., 'Room', 'Door').

        Returns:
            A string representing the color (e.g., 'rgb(R,G,B)' or a named Plotly color).
        """
        if node_type in self.type_to_plot_color_cache:
            return self.type_to_plot_color_cache[node_type]

        default_plotly_color = '#1f77b4'  # Plotly's default blue

        if self.config.NODE_COLOR_FROM_MAP and self.color_map_data:
            for rgb_tuple, details in self.color_map_data.items():
                if details.get('name') == node_type:
                    color_str = f'rgb{rgb_tuple}'
                    self.type_to_plot_color_cache[node_type] = color_str
                    return color_str

        self.type_to_plot_color_cache[node_type] = default_plotly_color
        return default_plotly_color

    @abc.abstractmethod
    def plot(self,
             graph: nx.Graph,
             output_path: Optional[pathlib.Path] = None,
             title: str = "Network Graph",
             # For Plotly layout, original image width
             graph_width: Optional[int] = None,
             # For Plotly layout, original image height
             graph_height: Optional[int] = None,
             # For SuperNetwork floor labels
             floor_z_map: Optional[Dict[int, float]] = None
             ):
        """
        Abstract method to plot the graph.

        Args:
            graph: The NetworkX graph to plot.
            output_path: Optional path to save the plot. If None, displays the plot.
            title: The title for the plot.
            graph_width: Original width of the (floor plan) image space. Used by Plotly.
            graph_height: Original height of the (floor plan) image space. Used by Plotly.
            floor_z_map: Mapping from floor number to Z-coordinate, for floor slider labels.
        """
        pass


class PlotlyPlotter(BasePlotter):
    """
    Generates interactive 3D network graph visualizations using Plotly.
    """

    def _create_floor_selection_controls(self,
                                         all_z_levels: List[float],
                                         min_z: float, max_z: float,
                                         floor_z_map_for_labels: Optional[Dict[int,
                                                                               float]] = None,
                                         base_floor_for_labels: int = 0
                                         ) -> Dict[str, Any]:
        """
        Creates slider controls for selecting and viewing individual floors or all floors.
        Args:
            all_z_levels: Sorted list of unique Z-coordinates present in the graph.
            min_z: Minimum Z-coordinate.
            max_z: Maximum Z-coordinate.
            floor_z_map_for_labels: Mapping from actual floor number to Z-coordinate.
            base_floor_for_labels: The base floor number for labeling (e.g. 0 for ground, 1 for first).
        """
        if not all_z_levels:
            return {"sliders": []}

        # Create floor labels. Try to map Z-levels back to "human-readable" floor numbers.
        z_to_floor_label_map: Dict[float, str] = {}
        if floor_z_map_for_labels:
            # Invert floor_z_map_for_labels to map z -> floor_num for easier lookup
            # Handle potential multiple floors at the same Z (unlikely with good input)
            z_to_floor_num: Dict[float, List[int]] = {}
            for fn, z_val in floor_z_map_for_labels.items():
                z_to_floor_num.setdefault(z_val, []).append(fn)

            for z_level in all_z_levels:
                floor_nums_at_z = z_to_floor_num.get(z_level)
                if floor_nums_at_z:
                    # If multiple floor numbers map to the same z_level, list them or take first
                    f_num_str = "/".join(map(str, sorted(floor_nums_at_z)))
                    # e.g., F1, F-1/B1
                    z_to_floor_label_map[z_level] = f"F{f_num_str}"
                # Fallback if z_level not in map (should not happen if map is complete)
                else:
                    z_to_floor_label_map[z_level] = f"Z={z_level:.1f}"
        else:  # Fallback if no floor_z_map is provided
            for i, z_level in enumerate(all_z_levels):
                # Attempt simple labeling if base_floor is known
                floor_num_guess = base_floor_for_labels + i  # This is a rough guess
                z_to_floor_label_map[z_level] = f"F{floor_num_guess} (Z={z_level:.1f})"

        slider_steps = []
        for z_level in all_z_levels:
            label = z_to_floor_label_map.get(z_level, f"Z={z_level:.1f}")
            slider_steps.append(dict(
                label=label,
                method="relayout",
                args=[{"scene.zaxis.range": [z_level - self.config.DEFAULT_FLOOR_HEIGHT / 2 + 0.1,
                                             z_level + self.config.DEFAULT_FLOOR_HEIGHT / 2 - 0.1]}]  # View single floor
            ))

        # Add a step to show all floors
        slider_steps.append(dict(
            label="All Floors",
            method="relayout",
            args=[{"scene.zaxis.range": [min_z - self.config.DEFAULT_FLOOR_HEIGHT * 0.5,
                                         max_z + self.config.DEFAULT_FLOOR_HEIGHT * 0.5]}]  # View all
        ))

        sliders = [dict(
            active=len(all_z_levels),  # Default to "All Floors"
            currentvalue={"prefix": "Current Display: "},
            pad={"t": 50},
            steps=slider_steps,
            name="Floor Selection"
        )]
        return {"sliders": sliders}

    def plot(self,
             graph: nx.Graph,
             output_path: Optional[pathlib.Path] = None,
             title: str = "3D Network Graph",
             graph_width: Optional[int] = None,
             graph_height: Optional[int] = None,
             floor_z_map: Optional[Dict[int, float]] = None
             ):
        if not graph.nodes:
            # Ensure logger is defined/imported
            logger.warning("PlotlyPlotter: Graph has no nodes to plot.")
            return

        node_traces = []
        edge_traces = []  # Renamed from edge_trace to edge_traces as it's a list

        nodes_data_by_type: Dict[str, Dict[str, list]] = {}
        all_node_objects = [data.get('node_obj', node_id)
                            for node_id, data in graph.nodes(data=True)]
        all_node_objects = [n for n in all_node_objects if isinstance(n, Node)]

        if not all_node_objects:
            logger.warning(
                "PlotlyPlotter: No Node objects found in graph nodes. Cannot plot.")
            return

        all_z_coords_present = sorted(
            list(set(n.z for n in all_node_objects)))
        min_z = min(all_z_coords_present) if all_z_coords_present else 0
        max_z = max(all_z_coords_present) if all_z_coords_present else 0

        for node_obj in all_node_objects:
            node_type = node_obj.node_type
            e_name = node_obj.e_name
            code = node_obj.code
            if node_type not in nodes_data_by_type:
                nodes_data_by_type[node_type] = {
                    'x': [], 'y': [], 'z': [],
                    'visible_text': [],  # For text always visible next to node
                    'hover_text': [],   # For text visible on hover
                    'sizes': [],
                    'ids': []
                }

            x, y, z = node_obj.x, node_obj.y, node_obj.z
            plot_x = (
                graph_width - x) if self.config.IMAGE_MIRROR and graph_width is not None else x

            nodes_data_by_type[node_type]['x'].append(plot_x)
            nodes_data_by_type[node_type]['y'].append(y)
            nodes_data_by_type[node_type]['z'].append(z)
            nodes_data_by_type[node_type]['ids'].append(node_obj.id)

            # --- Text Configuration ---
            # 1. Visible text (always shown next to the marker if mode includes 'text')
            #    Only show node_type if SHOW_PEDESTRIAN_LABELS is True or it's not a pedestrian node.
            #    Otherwise, show empty string to hide permanent text for certain types.
            is_ped_type = node_type in self.config.PEDESTRIAN_TYPES
            can_show_permanent_label = not is_ped_type or self.config.SHOW_PEDESTRIAN_LABELS

            nodes_data_by_type[node_type]['visible_text'].append(
                code if can_show_permanent_label else "")

            # 2. Hover text (always detailed)
            hover_label = (
                f"ID: {node_obj.id}<br>"
                f"Type: {node_type}<br>"
                f"Pos: ({x},{y},{z})<br>"
                f"Time: {node_obj.time:.2f}<br>"
                f"Area: {getattr(node_obj, 'area', 0):.2f}"
            )
            if hasattr(node_obj, 'door_type') and node_obj.door_type:
                hover_label += f"<br>Door: {node_obj.door_type}"
            nodes_data_by_type[node_type]['hover_text'].append(hover_label)

            # Node size
            size = self.config.NODE_SIZE_DEFAULT
            if is_ped_type:
                size = self.config.NODE_SIZE_PEDESTRIAN
            elif node_type in self.config.CONNECTION_TYPES:
                size = self.config.NODE_SIZE_CONNECTION
            elif node_type in self.config.VERTICAL_TYPES:
                size = self.config.NODE_SIZE_VERTICAL
            elif node_type in self.config.ROOM_TYPES:
                size = self.config.NODE_SIZE_ROOM
            elif node_type in self.config.OUTSIDE_TYPES:
                size = self.config.NODE_SIZE_OUTSIDE
            nodes_data_by_type[node_type]['sizes'].append(size)

        for node_type, data in nodes_data_by_type.items():
            if not data['x']:
                continue

            # Determine mode: if all 'visible_text' for this type are empty, just use 'markers'
            # Otherwise, use 'markers+text' to show the type.
            current_mode = 'markers'
            # Check if any visible text is non-empty
            if any(vt for vt in data['visible_text']):
                current_mode = 'markers+text'

            # If SHOW_PEDESTRIAN_LABELS is False and it's a pedestrian type, override to 'markers'
            if node_type in self.config.PEDESTRIAN_TYPES and not self.config.SHOW_PEDESTRIAN_LABELS:
                current_mode = 'markers'

            node_trace = go.Scatter3d(
                x=data['x'], y=data['y'], z=data['z'],
                mode=current_mode,  # Dynamically set mode
                marker=dict(
                    size=data['sizes'],
                    sizemode='diameter',  # This should make size in screen pixels
                    color=self._get_node_color(node_type),
                    opacity=self.config.NODE_OPACITY,
                    line=dict(width=1, color='DarkSlateGrey')
                ),
                # Text to display next to markers if mode includes 'text'
                text=data['visible_text'],
                hovertext=data['hover_text'],  # Text for hover box
                # Use 'text' from hovertext (Plotly default is 'all')
                hoverinfo='text',
                # if hovertext is set, hoverinfo='text' uses hovertext.
                # if hovertext is not set, hoverinfo='text' uses the 'text' property.
                name=node_type,
                customdata=data['ids'],
                textposition="top center",
                textfont=dict(  # Optional: style the permanently visible text
                    size=9,  # Smaller font for permanent labels
                    # color='black'
                )
            )
            node_traces.append(node_trace)

        # --- Prepare Edge Data with Enhanced Classification ---
        # 存储不同类型边的坐标数据
        edge_data = {
            'horizontal': {'x': [], 'y': [], 'z': []},
            'vertical': {'x': [], 'y': [], 'z': []},
            'door': {'x': [], 'y': [], 'z': []},
            'special': {'x': [], 'y': [], 'z': []}
        }
        
        # 性能和错误统计
        edges_processed = 0
        edges_skipped = 0
        classification_errors = 0
        coordinate_errors = 0
        
        # 输入验证
        if not graph or len(graph.edges) == 0:
            logger.warning("PlotlyPlotter: 图中没有边需要处理")
            edge_traces = []
        else:
            # 创建节点ID到Node对象的映射以提高查找性能
            node_id_to_obj = {}
            invalid_node_count = 0
            
            for node_id, node_data in graph.nodes(data=True):
                node_obj = node_data.get('node_obj')
                if isinstance(node_obj, Node):
                    node_id_to_obj[node_id] = node_obj
                else:
                    invalid_node_count += 1
            
            if invalid_node_count > 0:
                logger.warning(f"发现 {invalid_node_count} 个无效节点对象")
            
            logger.debug(f"创建节点映射表完成，包含 {len(node_id_to_obj)} 个有效节点")

            # 遍历所有边并正确获取节点对象
            for edge_start_id, edge_end_id, edge_attr in graph.edges(data=True):
                edges_processed += 1
                
                try:
                    # 边界条件检查：边属性
                    if edge_attr is None:
                        edge_attr = {}
                    
                    # 通过节点ID获取Node对象
                    start_node = node_id_to_obj.get(edge_start_id)
                    end_node = node_id_to_obj.get(edge_end_id)
                    
                    # 验证节点对象存在性和有效性
                    if not isinstance(start_node, Node) or not isinstance(end_node, Node):
                        edges_skipped += 1
                        if edges_processed <= 10:  # 只记录前10个错误避免日志泛滥
                            logger.warning(f"边 ({edge_start_id}, {edge_end_id}) 的节点对象无效: "
                                         f"start_node={type(start_node)}, end_node={type(end_node)}")
                        continue
                    
                    # 验证节点坐标有效性
                    if not self._validate_node_coordinates(start_node, str(edge_start_id)) or \
                       not self._validate_node_coordinates(end_node, str(edge_end_id)):
                        coordinate_errors += 1
                        edges_skipped += 1
                        if coordinate_errors <= 5:
                            logger.warning(f"边 ({edge_start_id}, {edge_end_id}) 的节点坐标无效")
                        continue
                    
                    # 获取节点位置信息
                    x0, y0, z0 = start_node.x, start_node.y, start_node.z
                    x1, y1, z1 = end_node.x, end_node.y, end_node.z
                    
                    # 应用镜像变换（如果启用）
                    if self.config.IMAGE_MIRROR and graph_width is not None and isinstance(graph_width, (int, float)):
                        plot_x0 = graph_width - x0
                        plot_x1 = graph_width - x1
                    else:
                        plot_x0, plot_x1 = x0, x1
                    
                    # 智能边类型分类（带异常处理）
                    try:
                        edge_type = self._classify_edge_type(start_node, end_node, edge_attr, z0, z1)
                        
                        # 验证分类结果
                        if edge_type not in edge_data:
                            classification_errors += 1
                            if classification_errors <= 5:
                                logger.warning(f"未知的边类型 '{edge_type}'，使用默认类型 'horizontal'")
                            edge_type = 'horizontal'
                        
                    except Exception as class_e:
                        classification_errors += 1
                        edges_skipped += 1
                        if classification_errors <= 5:
                            logger.warning(f"分类边 ({edge_start_id}, {edge_end_id}) 时出错: {class_e}，跳过该边")
                        continue
                    
                    # 将边数据添加到相应类别
                    edge_data[edge_type]['x'].extend([plot_x0, plot_x1, None])
                    edge_data[edge_type]['y'].extend([y0, y1, None])
                    edge_data[edge_type]['z'].extend([z0, z1, None])
                    
                except Exception as e:
                    edges_skipped += 1
                    logger.warning(f"处理边 ({edge_start_id}, {edge_end_id}) 时发生未知错误: {type(e).__name__}: {str(e)}")
                    continue
            
            # 详细统计报告
            successful_edges = edges_processed - edges_skipped
            logger.info(f"边处理统计: 总计 {edges_processed} 条边，成功处理 {successful_edges} 条，跳过 {edges_skipped} 条")
            if coordinate_errors > 0:
                logger.info(f"坐标错误: {coordinate_errors} 条边")
            if classification_errors > 0:
                logger.info(f"分类错误: {classification_errors} 条边")
            
            # 按类型统计边数
            type_counts = {edge_type: len(data['x']) // 3 for edge_type, data in edge_data.items() if data['x']}
            if type_counts:
                logger.info(f"边类型分布: {type_counts}")

            # 创建不同类型边的可视化轨迹
            edge_traces = []
            edge_style_config = self._get_edge_type_config()
        
        # 为每种边类型创建Scatter3d轨迹
        for edge_type, data in edge_data.items():
            if not data['x']:  # 跳过空数据
                continue
                
            style = edge_style_config[edge_type]
            line_config = {
                'color': style['color'],
                'width': style['width']
            }
            if style['dash']:
                line_config['dash'] = style['dash']
            
            edge_trace = go.Scatter3d(
                x=data['x'],
                y=data['y'], 
                z=data['z'],
                mode='lines',
                line=line_config,
                hoverinfo='none',
                name=style['name'],
                showlegend=True,  # 显示在图例中
                legendgroup=f'edges_{edge_type}'  # 分组管理
            )
            edge_traces.append(edge_trace)
            
        logger.info(f"创建了 {len(edge_traces)} 种类型的边轨迹")

        # --- Layout and Figure (remains largely the same) ---
        layout = go.Layout(
            title=title,
            showlegend=True,
            hovermode='closest',
            margin=dict(b=20, l=5, r=5, t=40),
            scene=dict(
                xaxis=dict(
                    title='X', autorange='reversed' if self.config.IMAGE_MIRROR else True),
                yaxis=dict(
                    title='Y',
                    autorange='reversed', # 反转Y轴
                ),
                zaxis=dict(title='Z (Floor)'),
                aspectmode="manual",
                aspectratio=dict(
                    x=self.config.X_AXIS_RATIO,
                    y=self.config.Y_AXIS_RATIO,
                    z=self.config.Z_AXIS_RATIO
                ),
                camera=dict(
                    # projection=dict(type='orthographic'),
                    eye=dict(x=1.25, y=1.25, z=1.25)
                    )
            ),
            legend=dict(
                orientation="v",    # 垂直排列
                x=0.02,             # X 位置 (靠近左边缘)
                y=1.0,              # Y 位置 (靠近顶部)
                xanchor="left",     # X 锚点
                yanchor="top",      # Y 锚点
                bgcolor="rgba(255, 255, 255, 0.7)", # 可选：浅色背景提高可读性
                # bordercolor="rgba(120, 120, 120, 0.7)", # 可选：边框颜色
                # borderwidth=1         # 可选：边框宽度
            )
        )

        if len(all_z_coords_present) > 1:
            floor_controls = self._create_floor_selection_controls(
                all_z_coords_present, min_z, max_z, floor_z_map)
            layout.update(floor_controls)

        fig = go.Figure(data=node_traces + edge_traces, layout=layout)

        if output_path:
            output_path.parent.mkdir(parents=True, exist_ok=True)
            fig.write_html(str(output_path), config=self.config.PLOTLY_CONFIG)
            # Ensure logger
            logger.info(f"Plotly graph saved to {output_path}")
        else:
            fig.show()
</file>

<file path="src/network/node.py">
"""简化的节点类，用于网络图构建"""

from typing import Tuple, Optional, Dict, Any
from src.rl_optimizer.utils.setup import setup_logger

logger = setup_logger(__name__)


class Node:
    """
    表示网络图中的一个节点
    
    存储节点的基本信息，包括位置、类型、名称等属性
    """
    
    def __init__(self, 
                 node_id: int,
                 x: float = 0.0, 
                 y: float = 0.0, 
                 z: float = 0.0,
                 node_type: str = "",
                 name: Optional[str] = None,
                 time: float = 1.0,
                 color: Optional[Tuple[int, int, int]] = None,
                 e_name: Optional[str] = None,
                 code: Optional[str] = None,
                 **kwargs):
        """
        初始化节点
        
        Args:
            node_id: 节点唯一标识符
            x, y, z: 节点的三维坐标
            node_type: 节点类型（如房间、走廊、门等）
            name: 节点名称
            time: 节点通行时间
            color: 节点颜色（RGB元组）
            **kwargs: 其他属性
        """
        self.id = node_id
        self.x = float(x)
        self.y = float(y)
        self.z = float(z)
        self.node_type = node_type
        self.name = name if name is not None else f"{node_type}_{node_id}"
        self.time = float(time)
        self.color = color
        self.e_name = e_name
        self.code = code

        # 存储其他属性
        for key, value in kwargs.items():
            setattr(self, key, value)
    
    @property
    def position(self) -> Tuple[float, float, float]:
        """获取节点位置坐标"""
        return (self.x, self.y, self.z)
    
    def __str__(self) -> str:
        """节点的字符串表示"""
        return f"Node(id={self.id}, type={self.node_type}, name={self.name}, pos=({self.x:.1f}, {self.y:.1f}, {self.z:.1f}))"
    
    def __repr__(self) -> str:
        """节点的详细字符串表示"""
        return self.__str__()
    
    def to_dict(self) -> Dict[str, Any]:
        """将节点转换为字典格式"""
        return {
            'id': self.id,
            'x': self.x,
            'y': self.y,
            'z': self.z,
            'type': self.node_type,
            'name': self.name,
            'time': self.time,
            'color': self.color,
            'e_name': self.e_name,
            'code': self.code
        }
</file>

<file path="src/algorithms/ppo_optimizer.py">
"""
PPO优化器 - 基于强化学习的布局优化算法
"""

import torch
import time
import numpy as np
from pathlib import Path
from typing import List, Optional, Dict, Any
from stable_baselines3.common.env_util import make_vec_env
from sb3_contrib.common.maskable.callbacks import MaskableEvalCallback as EvalCallback
from sb3_contrib import MaskablePPO
from sb3_contrib.common.wrappers import ActionMasker
from sb3_contrib.common.maskable.policies import MaskableActorCriticPolicy
from sb3_contrib.common.maskable.evaluation import evaluate_policy
from stable_baselines3.common.monitor import Monitor

from src.algorithms.base_optimizer import BaseOptimizer, OptimizationResult
from src.algorithms.constraint_manager import ConstraintManager
from src.rl_optimizer.env.cost_calculator import CostCalculator
from src.rl_optimizer.env.layout_env import LayoutEnv
from src.rl_optimizer.env.vec_env_wrapper import EpisodeInfoVecEnvWrapper
from src.rl_optimizer.model.policy_network import LayoutTransformer
from src.rl_optimizer.utils.setup import setup_logger, save_json
from src.rl_optimizer.utils.lr_scheduler import get_lr_scheduler
from src.rl_optimizer.utils.checkpoint_callback import CheckpointCallback
from src.rl_optimizer.data.cache_manager import CacheManager
from src.rl_optimizer.utils.shared_state_manager import get_shared_state_manager
from src.rl_optimizer.utils.tensorboard_callback import TensorboardBaselineCallback
from src.config import RLConfig

logger = setup_logger(__name__)


class PretrainedEvalCallback(EvalCallback):
    """
    扩展的评估回调，支持从预训练模型设置初始best_mean_reward
    """
    
    def __init__(
        self,
        eval_env,
        pretrained_model_path: Optional[str] = None,
        best_model_save_path: Optional[str] = None,
        log_path: Optional[str] = None,
        eval_freq: int = 10000,
        n_eval_episodes: int = 5,
        deterministic: bool = True,
        render: bool = False,
        verbose: int = 1,
        warn: bool = True
    ):
        super().__init__(
            eval_env=eval_env,
            best_model_save_path=best_model_save_path,
            log_path=log_path,
            eval_freq=eval_freq,
            n_eval_episodes=n_eval_episodes,
            deterministic=deterministic,
            render=render,
            verbose=verbose,
            warn=warn
        )
        self.pretrained_model_path = pretrained_model_path
        
    def _init_callback(self) -> None:
        """初始化回调，如果有预训练模型则评估其性能"""
        super()._init_callback()
        
        if self.pretrained_model_path and Path(self.pretrained_model_path).exists():
            logger.info(f"正在评估预训练模型性能: {self.pretrained_model_path}")
            
            try:
                # 加载预训练模型
                pretrained_model = MaskablePPO.load(
                    self.pretrained_model_path,
                    env=self.eval_env,
                    device='cuda' if torch.cuda.is_available() else 'cpu'
                )
                
                # 评估预训练模型
                episode_rewards, _ = evaluate_policy(
                    pretrained_model,
                    self.eval_env,
                    n_eval_episodes=self.n_eval_episodes,
                    render=self.render,
                    deterministic=self.deterministic,
                    return_episode_rewards=True,
                    warn=self.warn
                )
                
                # 计算平均奖励
                mean_reward = np.mean(episode_rewards)
                std_reward = np.std(episode_rewards)
                
                # 设置为初始best_mean_reward
                self.best_mean_reward = float(mean_reward)
                
                logger.info(f"预训练模型初始性能: {mean_reward:.2f} +/- {std_reward:.2f}")
                logger.info(f"设置初始best_mean_reward = {self.best_mean_reward:.2f}")
                
                # 记录到日志
                if self.logger:
                    self.logger.record("eval/initial_pretrained_reward", float(mean_reward))
                    self.logger.record("eval/initial_pretrained_std", float(std_reward))
                    
            except Exception as e:
                logger.error(f"评估预训练模型时发生错误: {e}")
                logger.info("将使用默认的best_mean_reward = -inf")
                # 出错时保持默认行为
                self.best_mean_reward = -np.inf


def get_action_mask_from_info(infos: List[Dict]) -> np.ndarray:
    """从矢量化环境的info字典列表中提取动作掩码"""
    return np.array([info.get("action_mask", []) for info in infos])


class PPOOptimizer(BaseOptimizer):
    """
    PPO优化器
    
    基于强化学习的PPO算法实现布局优化，使用MaskablePPO来处理动作掩码约束。
    """
    
    def __init__(self, 
                 cost_calculator: CostCalculator,
                 constraint_manager: ConstraintManager,
                 config: RLConfig,
                 cache_manager: CacheManager,
                 pretrained_model_path: Optional[str] = None):
        """
        初始化PPO优化器
        
        Args:
            cost_calculator: 成本计算器
            constraint_manager: 约束管理器
            config: RL配置
            cache_manager: 缓存管理器
            pretrained_model_path: 预训练模型路径（可选，用于从已有模型继续训练）
        """
        super().__init__(cost_calculator, constraint_manager, "PPO")
        self.config = config
        self.cache_manager = cache_manager
        
        # 初始化共享状态管理器（如果启用动态基线）
        if self.config.ENABLE_DYNAMIC_BASELINE:
            self.shared_state_manager = get_shared_state_manager(
                alpha=self.config.EMA_ALPHA,
                warmup_episodes=self.config.BASELINE_WARMUP_EPISODES
            )
            logger.info(f"动态基线共享状态管理器已初始化: alpha={self.config.EMA_ALPHA}, "
                       f"warmup_episodes={self.config.BASELINE_WARMUP_EPISODES}")
        else:
            self.shared_state_manager = None
            logger.info("未启用动态基线，使用传统奖励计算")
        
        # 环境参数
        self.env_kwargs = {
            "config": self.config,
            "cache_manager": self.cache_manager,
            "cost_calculator": self.cost_calculator,
            "constraint_manager": self.constraint_manager,
            "shared_state_manager": self.shared_state_manager
        }
        
        # 训练状态
        self.model = None
        self.vec_env = None
        self.resume_model_path = None
        self.completed_steps = 0
        self.best_model_dir = None  # 保存最佳模型目录路径
        self.pretrained_model_path = pretrained_model_path  # 保存预训练模型路径
    
    def optimize(self, 
                 initial_layout: Optional[List[str]] = None,
                 max_iterations: int = None,
                 total_timesteps: int = None,
                 original_layout: Optional[List[str]] = None,
                 original_cost: Optional[float] = None,
                 **kwargs) -> OptimizationResult:
        """
        执行PPO优化
        
        Args:
            initial_layout: 初始布局（PPO会自动探索）
            max_iterations: 最大迭代次数（使用total_timesteps代替）
            total_timesteps: 总训练步数
            original_layout: 原始布局（未经优化的基准）
            original_cost: 原始布局的成本
            **kwargs: 其他PPO参数
            
        Returns:
            OptimizationResult: 优化结果
        """
        self.start_optimization()
        
        # 保存原始布局信息
        self.original_layout = original_layout
        self.original_cost = original_cost
        
        # 使用配置中的参数或传入的参数
        if total_timesteps is None:
            total_timesteps = self.config.TOTAL_TIMESTEPS
        
        logger.info(f"开始PPO优化，总训练步数: {total_timesteps}")
        
        try:
            # 检查是否需要恢复训练
            self._check_for_resume()
            
            # 计算剩余训练步数
            remaining_steps = max(0, total_timesteps - self.completed_steps)
            if remaining_steps == 0:
                logger.info("训练已完成，加载最佳模型进行评估")
                best_layout, best_cost = self._evaluate_best_model()
                self.update_best_solution(best_layout, best_cost)
                return self.finish_optimization()
            
            # 创建环境和模型
            self._setup_environment_and_model(remaining_steps)
            
            # 执行训练
            self._train_model(remaining_steps)
            
            # 评估最佳模型
            best_layout, best_cost = self._evaluate_best_model()
            self.update_best_solution(best_layout, best_cost)
            
        except KeyboardInterrupt:
            logger.warning("训练被用户中断")
            if self.model:
                self._save_interrupted_model()
        except Exception as e:
            logger.error(f"PPO优化过程中发生错误: {e}", exc_info=True)
            raise
        
        return self.finish_optimization()
    
    def finish_optimization(self) -> OptimizationResult:
        """
        结束PPO优化过程并返回结果，包含动态基线统计信息
        
        Returns:
            OptimizationResult: 优化结果
        """
        # 调用父类的finish_optimization方法
        result = super().finish_optimization()
        
        # 如果启用了动态基线，添加相关统计信息
        if self.config.ENABLE_DYNAMIC_BASELINE and self.shared_state_manager is not None:
            baseline_stats = self.shared_state_manager.get_statistics()
            
            logger.info("=== 动态基线统计信息 ===")
            logger.info(f"总episodes: {baseline_stats['total_episodes']}")
            logger.info(f"全局episode计数: {baseline_stats['global_episode_count']}")
            logger.info(f"预热状态: {'完成' if baseline_stats['warmup_complete'] else '未完成'}")
            logger.info(f"运行时间: {baseline_stats['uptime_seconds']:.2f}秒")
            
            # 记录各种基线值
            ema_states = baseline_stats.get('ema_states', {})
            for key, ema_info in ema_states.items():
                if ema_info and ema_info.get('is_initialized'):
                    baseline_value = ema_info.get('value', 0)
                    std_value = ema_info.get('std', 0)
                    count = ema_info.get('count', 0)
                    logger.info(f"{key}: 基线={baseline_value:.6f}, 标准差={std_value:.6f}, 样本数={count}")
            
            # 将基线统计信息添加到结果中
            if not hasattr(result, 'additional_metrics'):
                result.additional_metrics = {}
            result.additional_metrics['dynamic_baseline_stats'] = baseline_stats
        
        return result
    
    def _check_for_resume(self):
        """检查是否需要从checkpoint恢复训练"""
        if not self.config.RESUME_TRAINING:
            return
            
        if self.config.PRETRAINED_MODEL_PATH:
            model_path = Path(self.config.PRETRAINED_MODEL_PATH)
            if not model_path.exists():
                logger.warning(f"指定的预训练模型不存在: {model_path}")
                return
            self.resume_model_path = str(model_path)
        else:
            # 自动查找最新的checkpoint
            checkpoint_callback = CheckpointCallback(
                save_freq=1,
                save_path=self.config.LOG_PATH
            )
            model_path = checkpoint_callback.get_latest_checkpoint()
            if model_path:
                self.resume_model_path = str(model_path)
            else:
                logger.info("未找到可用的checkpoint，将开始全新训练")
                return
        
        # 加载checkpoint元数据
        metadata = CheckpointCallback.load_checkpoint_metadata(self.resume_model_path)
        if metadata:
            self.completed_steps = metadata.get("training_progress", {}).get("num_timesteps", 0)
            logger.info(f"从checkpoint恢复训练，已完成步数: {self.completed_steps}")
        else:
            logger.warning("无法加载checkpoint元数据，从步数0开始")
    
    def _setup_environment_and_model(self, remaining_steps: int):
        """设置环境和模型"""
        logger.info(f"正在创建 {self.config.NUM_ENVS} 个并行环境...")
        
        # 创建矢量化环境
        vec_env = make_vec_env(
            lambda: ActionMasker(LayoutEnv(**self.env_kwargs), LayoutEnv._action_mask_fn),
            n_envs=self.config.NUM_ENVS
        )
        
        # 使用自定义包装器确保episode信息正确传递
        self.vec_env = EpisodeInfoVecEnvWrapper(vec_env)
        
        logger.info("矢量化环境创建成功，已添加episode信息包装器")
        
        # 创建或加载模型
        if self.resume_model_path:
            self._load_pretrained_model()
        elif self.pretrained_model_path:
            # 如果提供了预训练模型路径，从该模型继续训练
            self.resume_model_path = self.pretrained_model_path
            logger.info(f"使用预训练模型继续训练: {self.pretrained_model_path}")
            self._load_pretrained_model()
        else:
            self._create_new_model()
    
    def _load_pretrained_model(self):
        """加载预训练模型"""
        logger.info(f"正在加载预训练模型: {self.resume_model_path}")
        
        try:
            self.model = MaskablePPO.load(
                self.resume_model_path,
                env=self.vec_env,
                device='cuda' if torch.cuda.is_available() else 'cpu'
            )
            
            # 重新设置学习率调度器
            if hasattr(self.model, 'lr_schedule'):
                lr_scheduler = get_lr_scheduler(
                    schedule_type=self.config.LEARNING_RATE_SCHEDULE_TYPE,
                    initial_lr=self.config.LEARNING_RATE_INITIAL,
                    final_lr=self.config.LEARNING_RATE_FINAL
                )
                self.model.lr_schedule = lr_scheduler
                logger.info("学习率调度器已重新设置")
                
            logger.info("预训练模型加载成功")
            
        except Exception as e:
            logger.error(f"加载预训练模型失败: {e}")
            raise
    
    def _create_new_model(self):
        """创建新的PPO模型"""
        logger.info("创建全新的PPO模型...")
        
        # 创建学习率调度器
        lr_scheduler = get_lr_scheduler(
            schedule_type=self.config.LEARNING_RATE_SCHEDULE_TYPE,
            initial_lr=self.config.LEARNING_RATE_INITIAL,
            final_lr=self.config.LEARNING_RATE_FINAL
        )
        
        logger.info(f"使用学习率调度器: {self.config.LEARNING_RATE_SCHEDULE_TYPE}")
        logger.info(f"初始学习率: {self.config.LEARNING_RATE_INITIAL}, 最终学习率: {self.config.LEARNING_RATE_FINAL}")
        
        # 定义策略网络参数
        policy_kwargs = {
            "features_extractor_class": LayoutTransformer,
            "features_extractor_kwargs": {
                "features_dim": self.config.FEATURES_DIM,  # 使用统一的配置属性
                "config": self.config
            },
            "net_arch": dict(pi=[self.config.POLICY_NET_ARCH] * self.config.POLICY_NET_LAYERS,
                            vf=[self.config.VALUE_NET_ARCH] * self.config.VALUE_NET_LAYERS)
        }
        
        # 创建PPO模型
        self.model = MaskablePPO(
            MaskableActorCriticPolicy,
            self.vec_env,
            learning_rate=lr_scheduler,
            n_steps=self.config.N_STEPS,
            batch_size=self.config.BATCH_SIZE,
            n_epochs=self.config.N_EPOCHS,
            gamma=self.config.GAMMA,
            gae_lambda=self.config.GAE_LAMBDA,
            clip_range=self.config.CLIP_RANGE,
            ent_coef=self.config.ENT_COEF,
            vf_coef=self.config.VF_COEF,
            max_grad_norm=self.config.MAX_GRAD_NORM,
            policy_kwargs=policy_kwargs,
            verbose=1,
            device='cuda' if torch.cuda.is_available() else 'cpu',
            tensorboard_log=str(self.config.LOG_PATH)
        )
        
        logger.info("PPO模型创建成功")
    
    def _train_model(self, remaining_steps: int):
        """训练模型"""
        timestamp = time.strftime("%Y%m%d-%H%M%S")
        log_dir = self.config.LOG_PATH / f"ppo_layout_{timestamp}"
        result_dir = self.config.RESULT_PATH / "model" / f"ppo_layout_{timestamp}"
        
        # 如果是恢复训练，尝试使用原有目录
        if self.resume_model_path:
            checkpoint_path = Path(self.resume_model_path)
            # 尝试找到包含 ppo_layout_ 的父目录
            current_path = checkpoint_path.parent
            while current_path != current_path.parent:  # 直到根目录
                if "ppo_layout_" in current_path.name:
                    log_dir = self.config.LOG_PATH / current_path.name
                    result_dir = self.config.RESULT_PATH / "model" / current_path.name
                    break
                current_path = current_path.parent
        
        # 创建目录
        log_dir.mkdir(parents=True, exist_ok=True)
        result_dir.mkdir(parents=True, exist_ok=True)
        
        # 保存最佳模型目录路径
        self.best_model_dir = result_dir
        
        # 设置回调
        callbacks = []
        
        # Tensorboard回调
        tensorboard_callback = TensorboardBaselineCallback(
        log_freq=1000,  # 每1000步记录一次
        verbose=1
        )
        callbacks.append(tensorboard_callback)
        
        # Checkpoint回调
        checkpoint_callback = CheckpointCallback(
            save_freq=self.config.CHECKPOINT_FREQUENCY,
            save_path=str(log_dir / "checkpoints"),
            name_prefix="checkpoint"
        )
        callbacks.append(checkpoint_callback)
        
        # 评估回调
        eval_vec_env = make_vec_env(
            lambda: Monitor(
                ActionMasker(LayoutEnv(**self.env_kwargs), LayoutEnv._action_mask_fn),
                allow_early_resets=True,
                info_keywords=("time_cost",)
            ),
            n_envs=1
        )
        eval_env = EpisodeInfoVecEnvWrapper(eval_vec_env)
        
        # 判断是否使用预训练模型的评估回调
        if self.pretrained_model_path or self.resume_model_path:
            # 使用预训练模型路径（优先使用pretrained_model_path）
            model_path_for_eval = self.pretrained_model_path or self.resume_model_path
            
            logger.info(f"使用PretrainedEvalCallback，基于模型: {model_path_for_eval}")
            eval_callback = PretrainedEvalCallback(
                eval_env,
                pretrained_model_path=model_path_for_eval,
                best_model_save_path=str(result_dir / "best_model"),
                log_path=str(log_dir / "eval_logs"),
                eval_freq=self.config.EVAL_FREQUENCY,
                n_eval_episodes=5,
                deterministic=True,
                render=False,
                verbose=1
            )
        else:
            # 使用标准评估回调
            logger.info("使用标准EvalCallback")
            eval_callback = EvalCallback(
                eval_env,
                best_model_save_path=str(result_dir / "best_model"),
                log_path=str(log_dir / "eval_logs"),
                eval_freq=self.config.EVAL_FREQUENCY,
                deterministic=True,
                render=False
            )
        callbacks.append(eval_callback)
        
        # 开始训练
        logger.info(f"开始训练，剩余步数: {remaining_steps}")
        logger.info(f"日志保存路径: {log_dir}")
        
        self.model.learn(
            total_timesteps=remaining_steps,
            callback=callbacks,
            tb_log_name="PPO",
            progress_bar=True,
            reset_num_timesteps=False if self.resume_model_path else True
        )
        
        # 训练完成
        logger.info("🎉 训练完成！")
        logger.info("=" * 80)
        
        # 保存最终模型
        final_model_path = log_dir / "final_model.zip"
        self.model.save(str(final_model_path))
        logger.info(f"最终模型已保存到: {final_model_path}")
        
        # 保存训练配置
        config_path = log_dir / "training_config.json"
        config_data = self.config.__dict__.copy()
        save_json(config_data, str(config_path))
        logger.info(f"训练配置已保存到: {config_path}")
    
    def _evaluate_best_model(self) -> tuple[List[str], float]:
        """评估最佳模型并返回最优布局和成本"""
        best_model_path = None
        
        # 1. 确定最佳模型路径
        if self.best_model_dir:
            best_model_path = self.best_model_dir / "best_model" / "best_model.zip"
            if not best_model_path.exists():
                logger.warning(f"最佳模型文件不存在: {best_model_path}")
                best_model_path = None
        
        # 2. 如果没有最佳模型，尝试使用最终模型
        if not best_model_path:
            final_model_paths = [
                self.config.LOG_PATH / f"ppo_layout_*/final_model.zip",
                self.config.LOG_PATH / "final_model.zip"
            ]
            
            for pattern in final_model_paths:
                import glob
                matches = glob.glob(str(pattern))
                if matches:
                    # 使用最新的模型
                    best_model_path = Path(max(matches, key=lambda x: Path(x).stat().st_mtime))
                    logger.info(f"使用最终模型作为备选: {best_model_path}")
                    break
        
        # 3. 如果仍然没有找到模型，返回默认布局
        if not best_model_path or not best_model_path.exists():
            logger.warning("未找到可用的训练模型，返回初始布局")
            best_layout = self.generate_initial_layout()
            best_cost = self.evaluate_layout(best_layout)
            return best_layout, best_cost
        
        try:
            # 4. 加载最佳模型
            logger.info(f"正在加载最佳模型: {best_model_path}")
            best_model = MaskablePPO.load(
                str(best_model_path),
                device='cuda' if torch.cuda.is_available() else 'cpu'
            )
            
            # 5. 创建评估环境（使用ActionMasker包装）
            from sb3_contrib.common.wrappers import ActionMasker
            eval_env = ActionMasker(LayoutEnv(**self.env_kwargs), LayoutEnv._action_mask_fn)
            
            # 6. 执行评估（可以多次运行取最佳结果）
            best_layout = None
            best_cost = float('inf')
            n_eval_episodes = 5  # 评估5次取最佳
            
            for episode in range(n_eval_episodes):
                obs = eval_env.reset()
                # 处理新版Gym API返回的tuple
                if isinstance(obs, tuple):
                    obs = obs[0]
                terminated = False
                
                while not terminated:
                    # 获取动作掩码（从内部环境）
                    inner_env_for_mask = eval_env.env if hasattr(eval_env, 'env') else eval_env
                    action_mask = inner_env_for_mask.get_action_mask()
                    
                    # 使用掩码进行预测
                    action, _ = best_model.predict(obs, action_masks=action_mask, deterministic=True)
                    result = eval_env.step(int(action))
                    # 处理不同版本API的返回值
                    if len(result) == 5:
                        obs, _, terminated, _, _ = result
                    else:
                        obs, _, terminated, _ = result[:4]
                
                # 获取当前episode的布局和成本
                # ActionMasker包装了原始环境，需要访问内部环境
                inner_env = eval_env.env if hasattr(eval_env, 'env') else eval_env
                current_layout = inner_env._get_final_layout_str()
                current_cost = self.cost_calculator.calculate_total_cost(current_layout)
                
                if current_cost < best_cost:
                    best_cost = current_cost
                    best_layout = current_layout
                    logger.info(f"评估Episode {episode+1}/{n_eval_episodes}: 发现更优布局，成本: {best_cost:.2f}")
                else:
                    logger.info(f"评估Episode {episode+1}/{n_eval_episodes}: 成本: {current_cost:.2f}")
            
            logger.info(f"最佳模型评估完成，最优成本: {best_cost:.2f}")
            return best_layout, best_cost
            
        except Exception as e:
            logger.error(f"加载或评估模型时发生错误: {e}")
            logger.info("使用默认布局作为备选方案")
            best_layout = self.generate_initial_layout()
            best_cost = self.evaluate_layout(best_layout)
            return best_layout, best_cost
    
    def _save_interrupted_model(self):
        """保存被中断的模型"""
        timestamp = time.strftime("%Y%m%d-%H%M%S")
        interrupted_path = self.config.LOG_PATH / f"interrupted_model_{timestamp}.zip"
        self.model.save(str(interrupted_path))
        logger.info(f"中断的模型已保存到: {interrupted_path}")
    
    def get_additional_metrics(self) -> Dict[str, Any]:
        """获取PPO特定的额外指标"""
        metrics = {
            "total_timesteps": self.config.TOTAL_TIMESTEPS,
            "completed_steps": self.completed_steps,
            "num_envs": self.config.NUM_ENVS,
            "learning_rate_schedule": self.config.LEARNING_RATE_SCHEDULE_TYPE,
            "resume_training": self.config.RESUME_TRAINING
        }
        
        if self.model is not None:
            metrics.update({
                "model_device": str(self.model.device),
                "policy_class": str(type(self.model.policy))
            })
        
        return metrics
</file>

<file path="src/rl_optimizer/env/layout_env.py">
# src/rl_optimizer/env/layout_env.py

import gymnasium as gym
from gymnasium import spaces
import numpy as np
from typing import List, Dict, Any, Tuple, Optional
from functools import lru_cache
from sklearn.cluster import DBSCAN

from src.config import RLConfig
from src.rl_optimizer.data.cache_manager import CacheManager
from src.rl_optimizer.env.cost_calculator import CostCalculator
from src.rl_optimizer.utils.setup import setup_logger
from src.algorithms.constraint_manager import ConstraintManager
from src.rl_optimizer.env.adjacency_reward_calculator import create_adjacency_calculator
from src.rl_optimizer.utils.shared_state_manager import SharedStateManager, get_shared_state_manager
from src.rl_optimizer.utils.reward_normalizer import RewardNormalizer, RewardComponents

logger = setup_logger(__name__)  # 使用默认INFO级别

class LayoutEnv(gym.Env):
    """
    医院布局优化的强化学习环境 (范式A: 选科室，填槽位)。

    遵循Gymnasium接口，通过自回归方式构建布局。环境按照一个在每个回合
    开始时随机打乱的槽位顺序进行填充。在每一步，智能体从所有尚未被
    放置的科室中选择一个，放入当前待填充的槽位。

    - **状态 (Observation)**: 字典，包含当前部分布局、已放置科室的掩码、
                              以及当前待填充槽位的面积信息。
    - **动作 (Action)**: 离散值，代表在 `placeable_depts` 列表中的科室索引。
    - **奖励 (Reward)**: 仅在回合结束时给予的稀疏奖励，基于加权总通行时间
                      和软约束。
    - **约束 (Constraints)**: 硬约束（面积、强制相邻）通过动作掩码实现，
                          确保智能体只能选择合法的科室。
    """

    metadata = {'render_modes': ['human']}

    def __init__(self, config: RLConfig, cache_manager: CacheManager, cost_calculator: CostCalculator, constraint_manager: ConstraintManager, shared_state_manager: Optional[SharedStateManager] = None):
        """
        初始化布局优化环境。

        Args:
            config (RLConfig): RL优化器的配置对象。
            cache_manager (CacheManager): 已初始化的数据缓存管理器。
            cost_calculator (CostCalculator): 已初始化的成本计算器。
            constraint_manager (ConstraintManager): 约束管理器。
            shared_state_manager (SharedStateManager, optional): 共享状态管理器，用于动态基线。
        """
        super().__init__()
        self.config = config
        self.cm = cache_manager
        self.cc = cost_calculator
        self.constraint_manager = constraint_manager
        self.current_reward_info = None

        self._initialize_nodes_and_slots()
        self._define_spaces()
        self._initialize_state_variables()
        
        # 初始化动态基线和奖励归一化组件
        if self.config.ENABLE_DYNAMIC_BASELINE:
            if shared_state_manager is None:
                self.shared_state = get_shared_state_manager(
                    alpha=self.config.EMA_ALPHA,
                    warmup_episodes=self.config.BASELINE_WARMUP_EPISODES
                )
            else:
                self.shared_state = shared_state_manager
            
            self.reward_normalizer = RewardNormalizer(self.config, self.shared_state)
            logger.info("动态基线奖励归一化已启用")
        else:
            self.shared_state = None
            self.reward_normalizer = None
            logger.info("使用传统固定奖励计算")
        
        # 初始化相邻性奖励相关组件
        if self.config.ENABLE_ADJACENCY_REWARD:
            self._initialize_adjacency_components()

        logger.info(f"环境初始化完成：{self.num_slots}个可用槽位，{self.num_depts}个待放置科室。")

    def _initialize_nodes_and_slots(self):
        """从CacheManager获取并设置节点和槽位信息。"""
        self.placeable_slots = self.cm.placeable_slots
        self.slot_areas = self.cm.placeable_nodes_df['area'].values
        self.num_slots = len(self.placeable_slots)

        self.placeable_depts = self.cm.placeable_departments
        self.dept_areas_map = dict(zip(self.cm.placeable_nodes_df['node_id'], self.cm.placeable_nodes_df['area']))
        self.num_depts = len(self.placeable_depts)
        
        if self.num_slots != self.num_depts:
            raise ValueError(f"槽位数 ({self.num_slots}) 与待布局科室数 ({self.num_depts}) 不匹配!")
        
        self.dept_to_idx = {dept: i for i, dept in enumerate(self.placeable_depts)}
        
    def _initialize_adjacency_components(self):
        """
        初始化相邻性奖励计算所需的组件和预计算矩阵。
        """
        logger.info("初始化相邻性奖励组件...")
        
        # 获取行程时间矩阵（已过滤掉面积行）
        self.travel_times_matrix = self.cm.travel_times_matrix.copy()
        
        # 选择使用优化的相邻性计算器还是传统方法
        use_optimized_calculator = getattr(self.config, 'ENABLE_ADJACENCY_OPTIMIZATION', True)
        
        if use_optimized_calculator:
            # 使用优化的相邻性奖励计算器
            logger.info("启用优化相邻性奖励计算器")
            self.adjacency_calculator = create_adjacency_calculator(
                config=self.config,
                placeable_depts=self.placeable_depts,
                travel_times_matrix=self.travel_times_matrix,
                constraint_manager=self.constraint_manager
            )
            self.use_optimized_adjacency = True
        else:
            # 使用传统的相邻性计算方法（向后兼容）
            logger.info("使用传统相邻性奖励计算方法")
            self.use_optimized_adjacency = False
            
            # 预计算空间相邻性矩阵
            if self.config.ADJACENCY_PRECOMPUTE:
                self._precompute_spatial_adjacency()
                
            # 初始化功能相邻性映射
            self._initialize_functional_adjacency()
            
            # 初始化连通性相邻性（如果需要）
            if self.config.CONNECTIVITY_ADJACENCY_WEIGHT > 0:
                self._precompute_connectivity_adjacency()
            
        logger.info("相邻性奖励组件初始化完成")

    def _precompute_spatial_adjacency(self):
        """
        预计算基于分位数的空间相邻性矩阵。
        使用分位数阈值避免硬编码距离值。
        """
        logger.debug("预计算空间相邻性矩阵...")
        
        # 获取所有有效的节点名称（排除None值）
        valid_nodes = [node for node in self.placeable_depts if node in self.travel_times_matrix.columns]
        n_nodes = len(valid_nodes)
        
        if n_nodes < 2:
            logger.warning("可放置节点数量过少，跳过空间相邻性预计算")
            self.spatial_adjacency_matrix = np.zeros((n_nodes, n_nodes))
            return
        
        # 构建距离矩阵
        distance_matrix = np.zeros((n_nodes, n_nodes))
        for i, node1 in enumerate(valid_nodes):
            for j, node2 in enumerate(valid_nodes):
                if i != j and node1 in self.travel_times_matrix.index and node2 in self.travel_times_matrix.columns:
                    distance_matrix[i, j] = self.travel_times_matrix.loc[node1, node2]
        
        # 计算距离的分位数阈值
        upper_triangle_distances = distance_matrix[np.triu_indices_from(distance_matrix, k=1)]
        valid_distances = upper_triangle_distances[upper_triangle_distances > 0]
        
        if len(valid_distances) == 0:
            logger.warning("无有效距离数据，使用默认空间相邻性矩阵")
            self.spatial_adjacency_matrix = np.eye(n_nodes)
            return
        
        threshold = np.percentile(valid_distances, self.config.ADJACENCY_PERCENTILE_THRESHOLD * 100)
        logger.debug(f"空间相邻性距离阈值（{self.config.ADJACENCY_PERCENTILE_THRESHOLD*100}分位数）: {threshold:.2f}")
        
        # 生成空间相邻性矩阵
        self.spatial_adjacency_matrix = (distance_matrix <= threshold).astype(float)
        np.fill_diagonal(self.spatial_adjacency_matrix, 0)  # 自身不相邻
        
        # 确保矩阵对称
        self.spatial_adjacency_matrix = (self.spatial_adjacency_matrix + self.spatial_adjacency_matrix.T) / 2
        
        adjacency_ratio = np.sum(self.spatial_adjacency_matrix) / (n_nodes * (n_nodes - 1))
        logger.debug(f"空间相邻性矩阵生成完成，相邻比例: {adjacency_ratio:.3f}")

    def _initialize_functional_adjacency(self):
        """
        初始化基于医疗功能的相邻性偏好映射。
        """
        logger.debug("初始化功能相邻性映射...")
        
        # 创建通用名称到节点的映射
        self.generic_to_nodes = {}
        for dept in self.placeable_depts:
            generic_name = dept.split('_')[0]
            if generic_name not in self.generic_to_nodes:
                self.generic_to_nodes[generic_name] = []
            self.generic_to_nodes[generic_name].append(dept)
        
        # 创建功能相邻性偏好矩阵
        n_nodes = len(self.placeable_depts)
        self.functional_adjacency_matrix = np.zeros((n_nodes, n_nodes))
        
        for i, dept1 in enumerate(self.placeable_depts):
            generic1 = dept1.split('_')[0]
            for j, dept2 in enumerate(self.placeable_depts):
                generic2 = dept2.split('_')[0]
                
                if i != j:
                    # 查找医疗功能相邻性偏好
                    preference_score = self._get_functional_preference(generic1, generic2)
                    self.functional_adjacency_matrix[i, j] = preference_score
        
        logger.debug(f"功能相邻性矩阵生成完成，形状: {self.functional_adjacency_matrix.shape}")

    def _get_functional_preference(self, generic1: str, generic2: str) -> float:
        """
        获取两个通用科室之间的功能相邻性偏好分数。
        
        Args:
            generic1: 第一个科室的通用名称
            generic2: 第二个科室的通用名称
            
        Returns:
            float: 偏好分数，正数表示偏好相邻，负数表示偏好分离
        """
        preferences = self.config.MEDICAL_ADJACENCY_PREFERENCES
        
        # 正向偏好
        if generic1 in preferences and generic2 in preferences[generic1]:
            return preferences[generic1][generic2]
        
        # 反向偏好
        if generic2 in preferences and generic1 in preferences[generic2]:
            return preferences[generic2][generic1]
        
        # 默认无偏好
        return 0.0

    def _precompute_connectivity_adjacency(self):
        """
        预计算基于图连通性的相邻性矩阵。
        考虑多跳路径的连通性。
        """
        logger.debug("预计算连通性相邻性矩阵...")
        
        n_nodes = len(self.placeable_depts)
        self.connectivity_adjacency_matrix = np.zeros((n_nodes, n_nodes))
        
        # 获取距离矩阵
        if not hasattr(self, 'spatial_adjacency_matrix'):
            logger.warning("空间相邻性矩阵未初始化，跳过连通性相邻性计算")
            return
        
        # 计算多跳路径的连通性
        distance_matrix = np.zeros((n_nodes, n_nodes))
        for i, node1 in enumerate(self.placeable_depts):
            for j, node2 in enumerate(self.placeable_depts):
                if i != j and node1 in self.travel_times_matrix.index and node2 in self.travel_times_matrix.columns:
                    distance_matrix[i, j] = self.travel_times_matrix.loc[node1, node2]
        
        # 基于距离的多跳连通性
        valid_distances = distance_matrix[distance_matrix > 0]
        if len(valid_distances) > 0:
            connectivity_threshold = np.percentile(valid_distances, self.config.CONNECTIVITY_DISTANCE_PERCENTILE * 100)
            
            # 计算连通性权重（距离越近权重越高）
            for i in range(n_nodes):
                for j in range(n_nodes):
                    if i != j and distance_matrix[i, j] > 0:
                        if distance_matrix[i, j] <= connectivity_threshold:
                            # 使用指数衰减函数计算连通性权重
                            weight = np.exp(-distance_matrix[i, j] / connectivity_threshold)
                            self.connectivity_adjacency_matrix[i, j] = weight
        
        logger.debug(f"连通性相邻性矩阵生成完成，非零元素数: {np.count_nonzero(self.connectivity_adjacency_matrix)}")

    def _define_spaces(self):
        """定义观测空间和动作空间。"""
        # 动作空间：选择一个科室进行放置，或者跳过当前槽位
        # 动作 0 到 num_depts-1：选择科室索引
        # 动作 num_depts：跳过当前槽位
        self.action_space = spaces.Discrete(self.num_depts + 1)
        self.SKIP_ACTION = self.num_depts  # 跳过动作的索引

        # 观测空间：使用Box空间来明确定义形状，避免SB3的意外转换
        self.observation_space = spaces.Dict({
            # layout[i] = k+1 表示槽位i放置了索引为k的科室。0表示空。
            "layout": spaces.Box(
                low=0, 
                high=self.num_depts, # 最大值为科室数 (num_depts-1)+1
                shape=(self.num_slots,), 
                dtype=np.int32
            ),
            # placed_mask[k] = 1 表示索引为k的科室已被放置。
            "placed_mask": spaces.MultiBinary(self.num_depts),
            # 当前待填充槽位的索引
            "current_slot_idx": spaces.Box(low=0, high=self.num_slots - 1, shape=(1,), dtype=np.int32),
            # 跳过的槽位数量
            "num_skipped_slots": spaces.Box(low=0, high=self.num_slots, shape=(1,), dtype=np.int32)
        })

    def _initialize_state_variables(self):
        """初始化每个回合都会改变的状态变量。"""
        self.current_step = 0
        # layout的索引是物理槽位索引，值是科室索引+1
        self.layout = np.zeros(self.num_slots, dtype=np.int32)
        self.placed_mask = np.zeros(self.num_depts, dtype=bool)
        # 每个回合开始时需要被打乱的槽位处理顺序
        self.shuffled_slot_indices = np.arange(self.num_slots)
        # 跟踪跳过的槽位
        self.skipped_slots = set()
        # 势函数相关状态
        self.previous_potential = 0.0  # 上一步的势函数值

    def reset(self, *, seed: Optional[int] = None, options: Optional[dict] = None) -> Tuple[Dict, Dict]:
        """重置环境，返回初始观测。"""
        super().reset(seed=seed)
        self._initialize_state_variables()
        # 修改：按槽位面积从小到大排序，而不是随机打乱
        # self.shuffled_slot_indices = np.argsort(self.slot_areas)

        np.random.shuffle(self.shuffled_slot_indices) # 尝试随机布局提高泛化性能
        
        # 初始化势函数值（空布局的势函数为0）
        self.previous_potential = 0.0
        
        # 添加调试日志，显示槽位填充顺序
        logger.debug(f"槽位填充顺序（按面积从小到大）:")
        for i, slot_idx in enumerate(self.shuffled_slot_indices[:5]):  # 只显示前5个
            logger.debug(f"  {i+1}. {self.placeable_slots[slot_idx]} - 面积: {self.slot_areas[slot_idx]:.2f}")
        if len(self.shuffled_slot_indices) > 5:
            logger.debug(f"  ... 共 {len(self.shuffled_slot_indices)} 个槽位")
        
        return self._get_obs(), self._get_info(terminated=False)
    
    def step(self, action: int) -> Tuple[Dict, float, bool, bool, Dict]:
        """执行一个动作：在当前槽位放置选定的科室，或者跳过当前槽位。"""
        # 更严格的输入验证
        if not (0 <= action <= self.num_depts):  # 包括跳过动作
            logger.error(f"无效的动作索引: {action}，有效范围是 [0, {self.num_depts}]")
            return self._get_obs(), self.config.INVALID_ACTION_PENALTY, True, False, self._get_info(terminated=False)
        
        # 如果启用势函数奖励，计算当前状态的势函数
        if self.config.ENABLE_POTENTIAL_REWARD:
            current_potential = self._calculate_potential()
        
        # 检查是否为跳过动作
        if action == self.SKIP_ACTION:
            # 跳过当前槽位
            current_slot_idx = self.shuffled_slot_indices[self.current_step]
            self.skipped_slots.add(current_slot_idx)
            logger.debug(f"跳过槽位 {self.placeable_slots[current_slot_idx]} (索引: {current_slot_idx})")
            immediate_reward = self.config.REWARD_SKIP_PENALTY
        else:
            # 检查科室是否已被放置
            if self.placed_mask[action]:
                logger.error(f"严重错误：智能体选择了已被放置的科室！动作={action}, 科室={self.placeable_depts[action]}")
                logger.error(f"当前步骤: {self.current_step}, placed_mask: {self.placed_mask}")
                logger.error(f"当前动作掩码: {self.get_action_mask()}")
                return self._get_obs(), self.config.INVALID_ACTION_PENALTY, True, False, self._get_info(terminated=False)

            # 确定当前要填充的物理槽位索引
            slot_to_fill = self.shuffled_slot_indices[self.current_step]
            
            # 更新状态：在布局中记录放置，并标记科室为"已用"
            self.layout[slot_to_fill] = action + 1  # 动作是科室索引，存储时+1
            self.placed_mask[action] = True
            logger.debug(f"在槽位 {self.placeable_slots[slot_to_fill]} 放置科室 {self.placeable_depts[action]}")
            
            # 成功放置一个科室，给予即时奖励
            immediate_reward = self.config.REWARD_PLACEMENT_BONUS
        
        # 更新步骤计数
        self.current_step += 1

        # 改进的终止条件：考虑所有科室都已放置或所有槽位都已处理
        all_departments_placed = all(self.placed_mask)
        all_slots_processed = (self.current_step >= self.num_slots)
        terminated = all_departments_placed or all_slots_processed
        
        # 计算奖励
        if self.config.ENABLE_POTENTIAL_REWARD:
            # 计算新状态的势函数
            new_potential = self._calculate_potential()
            # 势函数奖励 = γ * Φ(s') - Φ(s)
            potential_reward = self.config.GAMMA * new_potential - current_potential
            potential_reward *= self.config.POTENTIAL_REWARD_WEIGHT
            
            # 更新势函数值
            self.previous_potential = new_potential
            
            if terminated:
                # 终止时的奖励 = 即时奖励 + 势函数奖励 + 最终奖励
                self.cached_final_reward = self._calculate_final_reward()
                reward = immediate_reward + potential_reward + self.cached_final_reward
                # 只在回合结束时显示势函数奖励汇总（包括面积匹配信息）
                logger.info(f"势函数奖励汇总: 最终势={new_potential:.4f}, 势函数总奖励={potential_reward:.4f}")
                # 计算并显示面积匹配统计
                self._log_area_match_statistics()
            else:
                # 非终止时的奖励 = 即时奖励 + 势函数奖励
                reward = immediate_reward + potential_reward
                # 调试级别的日志，正常训练时不显示
                logger.debug(f"势函数奖励计算: 当前势={current_potential:.4f}, 新势={new_potential:.4f}, 势函数奖励={potential_reward:.4f}")
        else:
            # 原有奖励机制：即时奖励（成功放置）+ 最终奖励（终止时的总体评分）
            if terminated:
                self.cached_final_reward = self._calculate_final_reward()  # 缓存最终奖励，避免重复计算
                reward = immediate_reward + self.cached_final_reward
            else:
                reward = immediate_reward  # 只有即时奖励
        
        info = self._get_info(terminated)

        return self._get_obs(), reward, terminated, False, info
    
    def _get_obs(self) -> Dict[str, Any]:
        """构建并返回当前观测字典。"""
        if self.current_step >= self.num_slots:
            current_slot_idx = 0
        else:
            current_slot_idx = self.shuffled_slot_indices[self.current_step]

        return {
            "layout": self.layout,
            "placed_mask": self.placed_mask,
            "current_slot_idx": np.array([current_slot_idx], dtype=np.int32),
            "num_skipped_slots": np.array([len(self.skipped_slots)], dtype=np.int32)
        }
    
    def _get_info(self, terminated: bool = False) -> Dict[str, Any]:
        """返回包含动作掩码和episode信息的附加信息。"""
        info = {"action_mask": self.get_action_mask()}
        
        # 如果episode结束，添加训练指标信息
        if terminated:
            # 构造最终布局（包括空槽位）
            final_layout_depts = []
            placed_depts = []
            
            for slot_idx in range(self.num_slots):
                dept_id = self.layout[slot_idx]
                if dept_id > 0:
                    dept_name = self.placeable_depts[dept_id - 1]
                    final_layout_depts.append(dept_name)
                    placed_depts.append(dept_name)
                else:
                    final_layout_depts.append(None)
            
            num_empty_slots = len(self.skipped_slots)
            num_placed_depts = len(placed_depts)
            
            # Episode结束时添加详细信息
            info['episode'] = {
                'time_cost': self.current_reward_info.raw_components.time_cost,  # 原始时间成本
                'placement_bonus': self.current_reward_info.raw_components.placement_bonus,  # 成功放置的累积奖励
                'final_reward': self.cached_final_reward if hasattr(self, 'cached_final_reward') else 0.0,  # 使用缓存的最终奖励
                'num_placed_depts': num_placed_depts,  # 放置的科室数
                'num_empty_slots': num_empty_slots,  # 空槽位数
                'layout': final_layout_depts.copy(),  # 完整布局（包括None）
                'placed_layout': placed_depts.copy(),  # 只包含已放置的科室
                'r': self.cached_final_reward if hasattr(self, 'cached_final_reward') else 0.0, # 总奖励
                'l': self.current_step  # episode长度
            }

            info['time_cost'] = self.current_reward_info.raw_components.time_cost
            
            if num_placed_depts > 0:
                info['episode']['per_process_costs'] = self.cc.calculate_per_process_cost(placed_depts)
            
            # 添加动态基线统计信息
            if self.config.ENABLE_DYNAMIC_BASELINE and self.reward_normalizer is not None:
                normalization_stats = self.reward_normalizer.get_normalization_stats()
                info['episode']['baseline_stats'] = {
                    'time_cost_baseline': normalization_stats.get('time_cost_baseline'),
                    'adjacency_baseline': normalization_stats.get('adjacency_baseline'),
                    'area_match_baseline': normalization_stats.get('area_match_baseline'),
                    'warmup_complete': normalization_stats.get('warmup_complete', False),
                    'episode_count': normalization_stats.get('episode_count', 0)
                }
                
                # 如果有归一化奖励信息，也添加进去
                if hasattr(self, '_last_normalized_reward_info'):
                    info['episode']['normalized_reward'] = self._last_normalized_reward_info.total_normalized_reward
                    info['episode']['is_relative_improvement'] = self._last_normalized_reward_info.is_relative_improvement
                    info['episode']['improvement_scores'] = self._last_normalized_reward_info.improvement_scores
        
        return info
    
    def get_action_mask(self) -> np.ndarray:
        """
        计算当前步骤下所有合法的动作掩码。
        
        该方法根据当前待填充槽位的面积约束，返回一个布尔数组，指示哪些未放置的科室可被合法选择。
        如果没有合适的科室，则允许跳过当前槽位。
        
        返回值:
            np.ndarray: 长度等于(num_depts + 1)的布尔数组，前 num_depts 个元素对应科室，最后一个元素对应跳过动作。
        """
        if self.current_step >= self.num_slots:
            return np.zeros(self.num_depts + 1, dtype=bool)

        # 1. 初始化动作掩码（包括跳过动作）
        action_mask = np.zeros(self.num_depts + 1, dtype=bool)
        
        # 2. 检查哪些科室可以放置在当前槽位
        current_slot_idx = self.shuffled_slot_indices[self.current_step]
        
        # 检查每个未放置的科室的兼容性
        for dept_idx in range(self.num_depts):
            if not self.placed_mask[dept_idx]:  # 只检查未放置的科室
                # 使用 ConstraintManager 的兼容性矩阵进行面积兼容性检查
                if self.constraint_manager.area_compatibility_matrix[current_slot_idx, dept_idx]:
                    action_mask[dept_idx] = True
        
        # 3. 始终允许跳过动作（如果配置允许）
        if self.config.ALLOW_PARTIAL_LAYOUT:
            action_mask[self.SKIP_ACTION] = True
        
        # 4. 如果没有任何合法动作，强制允许跳过（安全检查）
        if not np.any(action_mask):
            current_slot_name = self.placeable_slots[current_slot_idx]
            logger.warning(
                f"在步骤 {self.current_step}，槽位 '{current_slot_name}' 无可放置的科室，强制允许跳过动作"
            )
            action_mask[self.SKIP_ACTION] = True
        
        return action_mask
    
    def _calculate_final_reward(self) -> float:
        """
        在回合结束时，计算最终奖励。
        
        支持两种模式：
        1. 动态基线模式：使用归一化奖励和相对改进
        """
        # 构建最终布局（包括空槽位）
        final_layout_depts = []
        placed_depts = []
        
        for slot_idx in range(self.num_slots):
            dept_id = self.layout[slot_idx]
            if dept_id > 0:
                dept_name = self.placeable_depts[dept_id - 1]
                final_layout_depts.append(dept_name)
                placed_depts.append(dept_name)
            else:
                final_layout_depts.append(None)
        
        # 计算空槽位数量
        num_empty_slots = len(self.skipped_slots)
        num_placed_depts = len(placed_depts)
        
        # 如果终止但还有未放置的科室，显示警告
        if num_placed_depts < self.num_depts:
            num_unplaced = self.num_depts - num_placed_depts
            logger.warning(f"Episode终止但仍有 {num_unplaced} 个科室未放置")
        
        logger.debug(f"布局统计: 放置 {num_placed_depts} 个科室，跳过 {num_empty_slots} 个槽位")
        
        # 1. 计算原始奖励组件
        raw_components = self._compute_raw_reward_components(placed_depts, final_layout_depts, num_empty_slots, num_placed_depts)
        
        # 2. 使用归一化奖励计算模式
        return self._compute_normalized_reward(raw_components)
    
    def _compute_raw_reward_components(self, placed_depts: List[str], final_layout_depts: List[Optional[str]], 
                                      num_empty_slots: int, num_placed_depts: int) -> RewardComponents:
        """
        计算原始奖励组件
        
        Returns:
            RewardComponents: 包含各种原始奖励组件的数据类
        """
        components = RewardComponents()
        
        # 1. 计算时间成本
        layout_tuple = tuple(final_layout_depts)
        if num_placed_depts > 0:
            raw_time_cost = self.cc.calculate_total_cost(layout_tuple)
            components.time_cost = raw_time_cost
        else:
            components.time_cost = 1e10  # 极大惩罚值
            
        # 2. 计算相邻性奖励
        if self.config.ENABLE_ADJACENCY_REWARD and num_placed_depts >= 2:
            components.adjacency_reward = self._calculate_adjacency_reward(layout_tuple)
        else:
            components.adjacency_reward = 0.0
        
        # 3. 计算面积匹配奖励
        area_match_stats = self._calculate_area_match_statistics()
        components.area_match_reward = area_match_stats.get('avg_match_score', 0.0)
        
        # 4. 计算惩罚和奖励
        components.skip_penalty = -num_empty_slots * self.config.REWARD_EMPTY_SLOT_PENALTY
        components.completion_bonus = self.config.REWARD_COMPLETION_BONUS if num_empty_slots == 0 else 0.0
        components.placement_bonus = num_placed_depts * self.config.REWARD_PLACEMENT_BONUS
        
        return components
    
    def _compute_normalized_reward(self, raw_components: RewardComponents) -> float:
        """
        使用动态基线计算归一化奖励
        
        Args:
            raw_components: 原始奖励组件
            
        Returns:
            float: 归一化后的总奖励
        """
        # 增加episode计数
        current_episode = self.shared_state.increment_episode_count()
        
        # 更新基线（每隔一定频率更新）
        if current_episode % self.config.BASELINE_UPDATE_FREQUENCY == 0:
            self.reward_normalizer.update_baselines(raw_components)
        
        # 归一化奖励
        reward_info = self.reward_normalizer.normalize_reward_components(raw_components)
        
        # 存储最后的归一化奖励信息，供_get_info使用
        self._last_normalized_reward_info = reward_info
        
        # 记录详细日志
        # loguru直接使用debug，会根据级别自动过滤
        logger.info(f"Episode {current_episode} 动态基线奖励: 原始时间成本={raw_components.time_cost:.2f}秒, "
                    f"归一化总奖励={reward_info.total_normalized_reward:.4f}")
        
        if reward_info.is_relative_improvement:
            logger.info(f"使用相对改进奖励，改进分数: {reward_info.improvement_scores}")
        
        # 详细调试日志
        self.reward_normalizer.log_reward_info(reward_info, current_episode)

        self.current_reward_info = reward_info
        return reward_info.total_normalized_reward
    
    def _compute_traditional_reward(self, raw_components: RewardComponents) -> float:
        """
        使用传统方法计算奖励
        
        Args:
            raw_components: 原始奖励组件
            
        Returns:
            float: 传统计算的总奖励
        """
        # 时间成本奖励（负值，转换为奖励）
        time_reward = -raw_components.time_cost / self.config.REWARD_SCALE_FACTOR
        
        # 其他奖励组件
        total_reward = (
            time_reward * self.config.REWARD_TIME_WEIGHT +
            raw_components.adjacency_reward * self.config.REWARD_ADJACENCY_WEIGHT +
            raw_components.skip_penalty +
            raw_components.placement_bonus +
            raw_components.completion_bonus
        )
        
        logger.info(f"传统奖励计算: 时间成本={raw_components.time_cost:.2f}秒, "
                   f"时间奖励={time_reward:.4f}, 总奖励={total_reward:.4f}")
        
        return total_reward
    
    def _calculate_reward(self) -> float:
        """
        在回合结束时，根据最终布局计算并返回总奖励。
        
        奖励由以下部分组成：
        1. 时间成本奖励：根据已放置科室的通行时间计算
        2. 邻接约束奖励：软约束奖励
        3. 空槽位惩罚：根据跳过的槽位数量给予惩罚
        """
        # 构建最终布局（包括空槽位）
        final_layout_depts = []
        placed_depts = []
        
        for slot_idx in range(self.num_slots):
            dept_id = self.layout[slot_idx]
            if dept_id > 0:
                dept_name = self.placeable_depts[dept_id - 1]
                final_layout_depts.append(dept_name)
                placed_depts.append(dept_name)
            else:
                final_layout_depts.append(None)
        
        # 计算空槽位数量
        num_empty_slots = len(self.skipped_slots)
        num_placed_depts = len(placed_depts)
        
        logger.debug(f"布局统计: 放置 {num_placed_depts} 个科室，跳过 {num_empty_slots} 个槽位")
        
        # 1. 计算时间成本奖励（只基于已放置的科室）
        if num_placed_depts > 0:
            # 只传递已放置的科室给成本计算器
            time_cost = self.cc.calculate_total_cost(placed_depts)
            time_reward = -time_cost / 1e4  # 缩放以稳定训练
        else:
            # 如果没有放置任何科室，时间成本为0
            time_cost = 0.0
            time_reward = 0.0
        
        # 2. 邻接约束奖励（只基于已放置的科室）
        if self.config.ENABLE_ADJACENCY_REWARD:
            layout_tuple = tuple(final_layout_depts)  # 转换为元组以便缓存
            adjacency_reward = self._calculate_adjacency_reward(layout_tuple)
        else:
            adjacency_reward = 0.0
        
        # 3. 空槽位惩罚
        empty_penalty = num_empty_slots * self.config.EMPTY_SLOT_PENALTY_FACTOR / 1e4  # 缩放保持一致
        
        # 总奖励计算
        total_reward = (
            self.config.REWARD_TIME_WEIGHT * time_reward + 
            self.config.REWARD_ADJACENCY_WEIGHT * adjacency_reward -
            empty_penalty  # 直接减去惩罚
        )
        
        logger.debug(f"奖励组成: 时间={time_reward:.6f}, 邻接={adjacency_reward:.6f}, 空槽位惩罚={empty_penalty:.6f}, 总计={total_reward:.6f}")
        
        return total_reward
    
    def _calculate_adjacency_reward(self, layout_tuple: Tuple[str, ...]) -> float:
        """
        计算当前布局的多维度相邻性奖励。
        
        相邻性奖励包含三个维度：
        1. 功能相邻性：基于医疗流程的功能协作关系  
        Args:
            layout_tuple: 当前布局的元组形式（用于LRU缓存）
            
        Returns:
            float: 综合相邻性奖励分数
        """
        if not self.config.ENABLE_ADJACENCY_REWARD:
            return 0.0

        if np.sum(np.array(layout_tuple) != None) < 2:
            return 0.0
        
        # 使用优化的相邻性奖励计算器
        try:
            rewards_dict = self.adjacency_calculator.calculate_reward(
                layout_tuple
            )
            total_reward = rewards_dict.get('total_reward', 0.0)
            
            return total_reward
            
        except Exception as e:
            logger.error(f"优化相邻性计算器执行失败：{e}, 返回0奖励")
            return 0.0
    
    def _calculate_legacy_adjacency_reward(self, placed_depts: List[str]) -> float:
        """
        传统的相邻性奖励计算方法（向后兼容）。
        
        Args:
            placed_depts: 已放置的科室列表
            
        Returns:
            float: 综合相邻性奖励分数
        """
        # 计算各维度相邻性奖励
        spatial_reward = self._calculate_spatial_adjacency_reward(placed_depts)
        functional_reward = self._calculate_functional_adjacency_reward(placed_depts)
        connectivity_reward = 0.0
        
        if self.config.CONNECTIVITY_ADJACENCY_WEIGHT > 0:
            connectivity_reward = self._calculate_connectivity_adjacency_reward(placed_depts)
        
        # 加权组合各维度奖励
        total_reward = (
            self.config.SPATIAL_ADJACENCY_WEIGHT * spatial_reward +
            self.config.FUNCTIONAL_ADJACENCY_WEIGHT * functional_reward +
            self.config.CONNECTIVITY_ADJACENCY_WEIGHT * connectivity_reward
        ) * self.config.ADJACENCY_REWARD_BASE
        
        # 调试日志
        # loguru直接使用debug，会根据级别自动过滤
        if True:  # loguru handles level filtering automatically  # DEBUG级别
            logger.debug(f"传统相邻性奖励详情: 空间={spatial_reward:.3f}, "
                        f"功能={functional_reward:.3f}, 连通性={connectivity_reward:.3f}, "
                        f"总计={total_reward:.3f}")
        
        return total_reward

    def _calculate_spatial_adjacency_reward(self, placed_depts: List[str]) -> float:
        """
        计算空间相邻性奖励。
        基于预计算的空间相邻性矩阵。
        
        Args:
            placed_depts: 已放置的科室列表
            
        Returns:
            float: 空间相邻性奖励分数
        """
        # 安全性检查：矩阵是否存在
        if not hasattr(self, 'spatial_adjacency_matrix') or self.spatial_adjacency_matrix is None:
            logger.debug("空间相邻性矩阵不存在，返回0奖励")
            return 0.0
        
        # 安全性检查：输入验证
        if not placed_depts or len(placed_depts) < 2:
            logger.debug(f"科室数量不足以计算空间相邻性：{len(placed_depts) if placed_depts else 0}")
            return 0.0
        
        # 安全性检查：矩阵维度验证
        matrix_shape = self.spatial_adjacency_matrix.shape
        if len(matrix_shape) != 2 or matrix_shape[0] == 0 or matrix_shape[1] == 0:
            logger.error(f"空间相邻性矩阵维度异常：{matrix_shape}")
            return 0.0
        
        reward = 0.0
        count = 0
        failed_lookups = 0
        
        try:
            for i, dept1 in enumerate(placed_depts):
                # 安全的字典查找
                if dept1 is None or dept1 not in self.dept_to_idx:
                    failed_lookups += 1
                    logger.debug(f"科室 '{dept1}' 不在索引映射中")
                    continue
                    
                idx1 = self.dept_to_idx[dept1]
                
                # 完整的边界检查
                if not (0 <= idx1 < matrix_shape[0]):
                    failed_lookups += 1
                    logger.warning(f"科室 '{dept1}' 索引 {idx1} 超出矩阵行范围 [0, {matrix_shape[0]})")
                    continue
                
                for j, dept2 in enumerate(placed_depts[i+1:], i+1):
                    # 安全的字典查找
                    if dept2 is None or dept2 not in self.dept_to_idx:
                        failed_lookups += 1
                        logger.debug(f"科室 '{dept2}' 不在索引映射中")
                        continue
                        
                    idx2 = self.dept_to_idx[dept2]
                    
                    # 完整的边界检查
                    if not (0 <= idx2 < matrix_shape[1]):
                        failed_lookups += 1
                        logger.warning(f"科室 '{dept2}' 索引 {idx2} 超出矩阵列范围 [0, {matrix_shape[1]})")
                        continue
                    
                    # 安全的矩阵访问
                    try:
                        adjacency_score = self.spatial_adjacency_matrix[idx1, idx2]
                        
                        # 验证矩阵值的有效性
                        if np.isnan(adjacency_score) or np.isinf(adjacency_score):
                            logger.warning(f"空间相邻性矩阵包含无效值：[{idx1}, {idx2}] = {adjacency_score}")
                            continue
                        
                        reward += adjacency_score
                        count += 1
                        
                    except (IndexError, TypeError) as e:
                        failed_lookups += 1
                        logger.error(f"空间相邻性矩阵访问错误 [{idx1}, {idx2}]：{e}")
                        continue
        
        except Exception as e:
            logger.error(f"空间相邻性奖励计算过程中发生未预期错误：{e}")
            return 0.0
        
        # 记录统计信息
        if failed_lookups > 0:
            logger.debug(f"空间相邻性计算中有 {failed_lookups} 次查找失败")
        
        # 返回平均相邻性分数
        if count > 0:
            avg_reward = reward / count
            logger.debug(f"空间相邻性奖励：{reward:.4f} / {count} = {avg_reward:.4f}")
            return avg_reward
        else:
            logger.debug("空间相邻性计算：无有效科室对")
            return 0.0

    def _calculate_functional_adjacency_reward(self, placed_depts: List[str]) -> float:
        """
        计算功能相邻性奖励。
        基于医疗流程驱动的功能协作关系。
        
        Args:
            placed_depts: 已放置的科室列表
            
        Returns:
            float: 功能相邻性奖励分数
        """
        # 安全性检查：矩阵是否存在
        if not hasattr(self, 'functional_adjacency_matrix') or self.functional_adjacency_matrix is None:
            logger.debug("功能相邻性矩阵不存在，返回0奖励")
            return 0.0
        
        # 安全性检查：输入验证
        if not placed_depts or len(placed_depts) < 2:
            logger.debug(f"科室数量不足以计算功能相邻性：{len(placed_depts) if placed_depts else 0}")
            return 0.0
        
        # 安全性检查：矩阵维度验证
        matrix_shape = self.functional_adjacency_matrix.shape
        if len(matrix_shape) != 2 or matrix_shape[0] == 0 or matrix_shape[1] == 0:
            logger.error(f"功能相邻性矩阵维度异常：{matrix_shape}")
            return 0.0
        
        reward = 0.0
        count = 0
        failed_lookups = 0
        
        try:
            for i, dept1 in enumerate(placed_depts):
                # 安全的字典查找
                if dept1 is None or dept1 not in self.dept_to_idx:
                    failed_lookups += 1
                    logger.debug(f"科室 '{dept1}' 不在索引映射中")
                    continue
                    
                idx1 = self.dept_to_idx[dept1]
                
                # 完整的边界检查
                if not (0 <= idx1 < matrix_shape[0]):
                    failed_lookups += 1
                    logger.warning(f"科室 '{dept1}' 索引 {idx1} 超出矩阵行范围 [0, {matrix_shape[0]})")
                    continue
                
                for j, dept2 in enumerate(placed_depts[i+1:], i+1):
                    # 安全的字典查找
                    if dept2 is None or dept2 not in self.dept_to_idx:
                        failed_lookups += 1
                        logger.debug(f"科室 '{dept2}' 不在索引映射中")
                        continue
                        
                    idx2 = self.dept_to_idx[dept2]
                    
                    # 完整的边界检查
                    if not (0 <= idx2 < matrix_shape[1]):
                        failed_lookups += 1
                        logger.warning(f"科室 '{dept2}' 索引 {idx2} 超出矩阵列范围 [0, {matrix_shape[1]})")
                        continue
                    
                    # 安全的矩阵访问
                    try:
                        preference_score = self.functional_adjacency_matrix[idx1, idx2]
                        
                        # 验证矩阵值的有效性
                        if np.isnan(preference_score) or np.isinf(preference_score):
                            logger.warning(f"功能相邻性矩阵包含无效值：[{idx1}, {idx2}] = {preference_score}")
                            continue
                        
                        # 正向偏好给予奖励，负向偏好给予惩罚
                        if preference_score > 0:
                            reward += preference_score
                        elif preference_score < 0:
                            # 安全地获取惩罚倍数
                            penalty_multiplier = getattr(self.config, 'ADJACENCY_PENALTY_MULTIPLIER', 1.0)
                            reward += preference_score * penalty_multiplier
                        # preference_score == 0 时不计入奖励
                        
                        count += 1
                        
                    except (IndexError, TypeError) as e:
                        failed_lookups += 1
                        logger.error(f"功能相邻性矩阵访问错误 [{idx1}, {idx2}]：{e}")
                        continue
        
        except Exception as e:
            logger.error(f"功能相邻性奖励计算过程中发生未预期错误：{e}")
            return 0.0
        
        # 记录统计信息
        if failed_lookups > 0:
            logger.debug(f"功能相邻性计算中有 {failed_lookups} 次查找失败")
        
        # 返回平均功能相邻性分数
        if count > 0:
            avg_reward = reward / count
            logger.debug(f"功能相邻性奖励：{reward:.4f} / {count} = {avg_reward:.4f}")
            return avg_reward
        else:
            logger.debug("功能相邻性计算：无有效科室对")
            return 0.0

    def _calculate_connectivity_adjacency_reward(self, placed_depts: List[str]) -> float:
        """
        计算连通性相邻性奖励。
        基于图连通性的可达性关系，实现真正的多跳路径分析。
        
        Args:
            placed_depts: 已放置的科室列表
            
        Returns:
            float: 连通性相邻性奖励分数
        """
        # 安全性检查：矩阵是否存在
        if not hasattr(self, 'connectivity_adjacency_matrix') or self.connectivity_adjacency_matrix is None:
            logger.debug("连通性相邻性矩阵不存在，返回0奖励")
            return 0.0
        
        # 安全性检查：输入验证
        if not placed_depts or len(placed_depts) < 2:
            logger.debug(f"科室数量不足以计算连通性相邻性：{len(placed_depts) if placed_depts else 0}")
            return 0.0
        
        # 安全性检查：矩阵维度验证
        matrix_shape = self.connectivity_adjacency_matrix.shape
        if len(matrix_shape) != 2 or matrix_shape[0] == 0 or matrix_shape[1] == 0:
            logger.error(f"连通性相邻性矩阵维度异常：{matrix_shape}")
            return 0.0
        
        reward = 0.0
        count = 0
        failed_lookups = 0
        
        try:
            for i, dept1 in enumerate(placed_depts):
                # 安全的字典查找
                if dept1 is None or dept1 not in self.dept_to_idx:
                    failed_lookups += 1
                    logger.debug(f"科室 '{dept1}' 不在索引映射中")
                    continue
                    
                idx1 = self.dept_to_idx[dept1]
                
                # 完整的边界检查
                if not (0 <= idx1 < matrix_shape[0]):
                    failed_lookups += 1
                    logger.warning(f"科室 '{dept1}' 索引 {idx1} 超出矩阵行范围 [0, {matrix_shape[0]})")
                    continue
                
                for j, dept2 in enumerate(placed_depts[i+1:], i+1):
                    # 安全的字典查找
                    if dept2 is None or dept2 not in self.dept_to_idx:
                        failed_lookups += 1
                        logger.debug(f"科室 '{dept2}' 不在索引映射中")
                        continue
                        
                    idx2 = self.dept_to_idx[dept2]
                    
                    # 完整的边界检查
                    if not (0 <= idx2 < matrix_shape[1]):
                        failed_lookups += 1
                        logger.warning(f"科室 '{dept2}' 索引 {idx2} 超出矩阵列范围 [0, {matrix_shape[1]})")
                        continue
                    
                    # 安全的矩阵访问和多跳路径分析
                    try:
                        # 直接连通性权重
                        direct_connectivity = self.connectivity_adjacency_matrix[idx1, idx2]
                        
                        # 验证矩阵值的有效性
                        if np.isnan(direct_connectivity) or np.isinf(direct_connectivity):
                            logger.warning(f"连通性相邻性矩阵包含无效值：[{idx1}, {idx2}] = {direct_connectivity}")
                            continue
                        
                        # 实现多跳路径分析
                        multi_hop_connectivity = self._calculate_multi_hop_connectivity(idx1, idx2, dept1, dept2)
                        
                        # 组合直接连通性和多跳连通性
                        total_connectivity = direct_connectivity + 0.5 * multi_hop_connectivity
                        
                        reward += total_connectivity
                        count += 1
                        
                    except (IndexError, TypeError) as e:
                        failed_lookups += 1
                        logger.error(f"连通性矩阵访问错误 [{idx1}, {idx2}]：{e}")
                        continue
        
        except Exception as e:
            logger.error(f"连通性相邻性奖励计算过程中发生未预期错误：{e}")
            return 0.0
        
        # 记录统计信息
        if failed_lookups > 0:
            logger.debug(f"连通性相邻性计算中有 {failed_lookups} 次查找失败")
        
        # 返回平均连通性相邻性分数
        if count > 0:
            avg_reward = reward / count
            logger.debug(f"连通性相邻性奖励：{reward:.4f} / {count} = {avg_reward:.4f}")
            return avg_reward
        else:
            logger.debug("连通性相邻性计算：无有效科室对")
            return 0.0

    def _calculate_multi_hop_connectivity(self, idx1: int, idx2: int, dept1: str, dept2: str) -> float:
        """
        计算两个科室之间的多跳连通性权重。
        基于真正的图连通性分析，考虑2-3跳的间接路径。
        
        Args:
            idx1: 第一个科室的索引
            idx2: 第二个科室的索引
            dept1: 第一个科室的名称（用于日志）
            dept2: 第二个科室的名称（用于日志）
            
        Returns:
            float: 多跳连通性权重（0-1之间）
        """
        try:
            # 获取配置参数并进行验证
            max_path_length = getattr(self.config, 'CONNECTIVITY_MAX_PATH_LENGTH', 3)
            weight_decay = getattr(self.config, 'CONNECTIVITY_WEIGHT_DECAY', 0.8)
            
            # 参数验证
            if max_path_length < 2 or max_path_length > 5:
                logger.warning(f"多跳路径长度配置异常：{max_path_length}，使用默认值3")
                max_path_length = 3
                
            if weight_decay <= 0 or weight_decay >= 1:
                logger.warning(f"权重衰减因子配置异常：{weight_decay}，使用默认值0.8")
                weight_decay = 0.8
            
            # 获取行程时间矩阵进行多跳分析
            if not hasattr(self, 'travel_times_matrix') or self.travel_times_matrix is None:
                logger.debug("行程时间矩阵不存在，无法计算多跳连通性")
                return 0.0
            
            # 安全获取节点名称并验证存在性
            try:
                if dept1 not in self.travel_times_matrix.index or dept2 not in self.travel_times_matrix.columns:
                    logger.debug(f"科室 '{dept1}' 或 '{dept2}' 不在行程时间矩阵中")
                    return 0.0
            except Exception as e:
                logger.debug(f"检查科室是否在行程时间矩阵中时发生错误：{e}")
                return 0.0

            multi_hop_weight = 0.0
            
            # 计算2跳和3跳路径的连通性
            for path_length in range(2, min(max_path_length + 1, 4)):  # 限制最大路径长度为3
                try:
                    path_weight = self._calculate_path_connectivity(dept1, dept2, path_length, weight_decay)
                    if path_weight > 0:
                        # 路径长度越长，权重衰减越多
                        discounted_weight = path_weight * (weight_decay ** (path_length - 1))
                        multi_hop_weight += discounted_weight
                        
                        logger.debug(f"{path_length}跳路径连通性：{dept1} -> {dept2}，权重={discounted_weight:.4f}")
                        
                except Exception as e:
                    logger.debug(f"计算{path_length}跳路径时发生错误：{e}")
                    continue
            
            # 限制多跳权重在合理范围内
            multi_hop_weight = min(multi_hop_weight, 1.0)
            
            if multi_hop_weight > 0:
                logger.debug(f"多跳连通性总权重：{dept1} <-> {dept2} = {multi_hop_weight:.4f}")
                
            return multi_hop_weight
            
        except Exception as e:
            logger.error(f"多跳连通性计算过程中发生未预期错误：{e}")
            return 0.0
    
    def _calculate_path_connectivity(self, start_dept: str, end_dept: str, path_length: int, weight_decay: float) -> float:
        """
        计算指定路径长度的连通性权重。
        使用动态规划方法计算最短路径权重。
        
        Args:
            start_dept: 起始科室名称
            end_dept: 目标科室名称
            path_length: 路径长度（跳数）
            weight_decay: 权重衰减因子
            
        Returns:
            float: 路径连通性权重
        """
        try:
            # 安全性检查：行程时间矩阵验证
            if not hasattr(self, 'travel_times_matrix') or self.travel_times_matrix is None:
                logger.debug("行程时间矩阵不存在，无法计算多跳路径连通性")
                return 0.0
            
            # 安全性检查：起始和结束科室存在性
            if (start_dept not in self.travel_times_matrix.index or 
                end_dept not in self.travel_times_matrix.columns):
                logger.debug(f"科室 '{start_dept}' 或 '{end_dept}' 不在行程时间矩阵中")
                return 0.0
                
            # 获取所有可能的中介节点（增加安全检查）
            try:
                available_nodes = []
                for node in self.placeable_depts:
                    if (node and node != start_dept and node != end_dept and 
                        node in self.travel_times_matrix.index and 
                        node in self.travel_times_matrix.columns):
                        available_nodes.append(node)
            except Exception as e:
                logger.error(f"获取可用中介节点时发生错误：{e}")
                return 0.0
            
            if len(available_nodes) == 0:
                logger.debug(f"没有可用的中介节点用于计算{path_length}跳路径：{start_dept} -> {end_dept}")
                return 0.0
                
            best_path_weight = 0.0
            
            if path_length == 2:
                # 2跳路径：start -> intermediate -> end
                for intermediate in available_nodes:
                    try:
                        # 安全的矩阵访问
                        time1 = self.travel_times_matrix.loc[start_dept, intermediate]
                        time2 = self.travel_times_matrix.loc[intermediate, end_dept]
                        
                        # 数值有效性检查
                        if (time1 > 0 and time2 > 0 and 
                            not (np.isnan(time1) or np.isnan(time2) or np.isinf(time1) or np.isinf(time2))):
                            # 使用调和平均数计算路径权重（安全除法）
                            try:
                                path_weight = 2.0 / (1.0/time1 + 1.0/time2)
                                # 转换为连通性权重（时间越短，连通性越强）
                                connectivity_weight = 1.0 / (1.0 + path_weight / 100.0)  # 标准化到0-1
                                best_path_weight = max(best_path_weight, connectivity_weight)
                            except ZeroDivisionError:
                                logger.debug(f"2跳路径计算中出现除零错误：{start_dept}->{intermediate}->{end_dept}")
                                continue
                            
                    except (KeyError, TypeError, ValueError) as e:
                        logger.debug(f"计算2跳路径权重时出错：{start_dept}->{intermediate}->{end_dept}，错误：{e}")
                        continue
                        
            elif path_length == 3:
                # 3跳路径：start -> int1 -> int2 -> end
                for int1 in available_nodes:
                    for int2 in available_nodes:
                        if int1 != int2:  # 避免循环
                            try:
                                # 安全的矩阵访问
                                time1 = self.travel_times_matrix.loc[start_dept, int1]
                                time2 = self.travel_times_matrix.loc[int1, int2]
                                time3 = self.travel_times_matrix.loc[int2, end_dept]
                                
                                # 数值有效性检查
                                if (time1 > 0 and time2 > 0 and time3 > 0 and 
                                    not any(np.isnan([time1, time2, time3]) or np.isinf([time1, time2, time3]))):
                                    # 使用调和平均数计算路径权重（安全除法）
                                    try:
                                        path_weight = 3.0 / (1.0/time1 + 1.0/time2 + 1.0/time3)
                                        connectivity_weight = 1.0 / (1.0 + path_weight / 100.0)
                                        best_path_weight = max(best_path_weight, connectivity_weight)
                                    except ZeroDivisionError:
                                        logger.debug(f"3跳路径计算中出现除零错误：{start_dept}->{int1}->{int2}->{end_dept}")
                                        continue
                                    
                            except (KeyError, TypeError, ValueError) as e:
                                logger.debug(f"计算3跳路径权重时出错：{start_dept}->{int1}->{int2}->{end_dept}，错误：{e}")
                                continue
            
            return best_path_weight
            
        except Exception as e:
            logger.error(f"路径连通性计算过程中发生未预期错误：{e}")
            return 0.0
    
    def _calculate_area_match_score(self, dept_idx: int, slot_idx: int) -> float:
        """
        计算科室与槽位的面积匹配度分数。
        
        Args:
            dept_idx: 科室在placeable_depts中的索引
            slot_idx: 槽位在placeable_slots中的索引
            
        Returns:
            float: 0到1之间的匹配度分数，1表示完美匹配，0表示差异最大
        """
        # 获取科室和槽位的面积
        dept_name = self.placeable_depts[dept_idx]
        dept_area = self.dept_areas_map[dept_name]
        slot_area = self.slot_areas[slot_idx]
        
        # 避免除零错误
        if dept_area == 0 or slot_area == 0:
            logger.warning(f"面积为0：科室 {dept_name} 面积={dept_area}, 槽位索引 {slot_idx} 面积={slot_area}")
            return 0.0
        
        # 计算相对差异（使用较大值作为基准）
        relative_diff = abs(dept_area - slot_area) / max(dept_area, slot_area)
        
        # 转换为0-1的匹配分数（差异越小，分数越高）
        # 使用AREA_SCALING_FACTOR作为容差阈值
        match_score = max(0.0, 1.0 - relative_diff / self.config.AREA_SCALING_FACTOR)
        
        return match_score
    
    def _calculate_potential(self) -> float:
        """
        计算当前布局状态的势函数值。
        势函数 Φ(layout) = -1 * (时间成本部分) + 面积匹配奖励部分 + 相邻性奖励部分
        
        Returns:
            float: 当前状态的势函数值
        """
        # 构建完整的布局（包括空槽位）
        layout_with_nulls = []
        placed_depts = []
        
        for slot_idx in range(self.num_slots):
            dept_id = self.layout[slot_idx]
            if dept_id > 0:
                dept_name = self.placeable_depts[dept_id - 1]
                layout_with_nulls.append(dept_name)
                placed_depts.append(dept_name)
            else:
                layout_with_nulls.append(None)
        
        # === 时间成本部分 ===
        # 如果没有放置科室或只有一个科室，时间成本势函数为0
        time_cost_potential = 0.0
        if len(placed_depts) > 1:
            # 计算已放置科室的总时间成本
            # 传递完整布局（包含None），让CostCalculator正确映射科室到槽位
            time_cost = self.cc.calculate_total_cost(layout_with_nulls)
            # 势函数为负的时间成本（缩放后）
            time_cost_potential = -time_cost / self.config.REWARD_SCALE_FACTOR
        
        # === 面积匹配奖励部分 ===
        area_match_reward = 0.0
        num_matched_depts = 0
        total_match_score = 0.0
        
        for slot_idx in range(self.num_slots):
            dept_id = self.layout[slot_idx]
            if dept_id > 0:
                dept_idx = dept_id - 1
                match_score = self._calculate_area_match_score(dept_idx, slot_idx)
                area_match_reward += match_score * self.config.AREA_MATCH_BONUS_BASE
                total_match_score += match_score
                num_matched_depts += 1
        
        # 计算平均匹配度（用于日志）
        avg_match_score = total_match_score / num_matched_depts if num_matched_depts > 0 else 0.0
        
        # === 相邻性奖励部分 ===
        adjacency_reward = 0.0
        if self.config.ENABLE_ADJACENCY_REWARD and len(placed_depts) >= 2:
            # 使用元组以便LRU缓存
            layout_tuple = tuple(layout_with_nulls)
            adjacency_reward = self._calculate_adjacency_reward(layout_tuple)
        
        # === 组合所有部分势函数 ===
        total_potential = (
            time_cost_potential + 
            area_match_reward * self.config.AREA_MATCH_REWARD_WEIGHT +
            adjacency_reward * self.config.ADJACENCY_REWARD_WEIGHT
        )
        
        # 详细的调试日志
        # loguru直接使用debug，会根据级别自动过滤
        if True:  # loguru handles level filtering automatically  # DEBUG级别
            logger.debug(f"势函数计算详情: "
                        f"已放置{len(placed_depts)}个科室, "
                        f"时间成本势={time_cost_potential:.4f}, "
                        f"面积匹配奖励={area_match_reward:.4f}, "
                        f"相邻性奖励={adjacency_reward:.4f}, "
                        f"平均匹配度={avg_match_score:.3f}, "
                        f"总势函数={total_potential:.4f}")
        
        return total_potential
    
    def _calculate_area_match_statistics(self) -> Dict[str, float]:
        """
        计算当前布局的面积匹配统计信息。
        
        Returns:
            Dict[str, float]: 包含平均、最小、最大匹配度的字典
        """
        match_scores = []
        
        for slot_idx in range(self.num_slots):
            dept_id = self.layout[slot_idx]
            if dept_id > 0:
                dept_idx = dept_id - 1
                match_score = self._calculate_area_match_score(dept_idx, slot_idx)
                match_scores.append(match_score)
        
        if not match_scores:
            return {
                'avg_match_score': 0.0,
                'min_match_score': 0.0,
                'max_match_score': 0.0,
                'num_matched': 0
            }
        
        return {
            'avg_match_score': sum(match_scores) / len(match_scores),
            'min_match_score': min(match_scores),
            'max_match_score': max(match_scores),
            'num_matched': len(match_scores)
        }
    
    def _calculate_adjacency_statistics(self, placed_depts: List[str]) -> Dict[str, float]:
        """
        计算当前布局的相邻性统计信息。
        
        Args:
            placed_depts: 已放置的科室列表
            
        Returns:
            Dict[str, float]: 包含各维度相邻性奖励的字典
        """
        if not self.config.ENABLE_ADJACENCY_REWARD or len(placed_depts) < 2:
            return {
                'spatial_reward': 0.0,
                'functional_reward': 0.0,
                'connectivity_reward': 0.0,
                'total_reward': 0.0
            }
        
        # 选择使用优化计算器还是传统方法
        if hasattr(self, 'use_optimized_adjacency') and self.use_optimized_adjacency:
            # 使用优化的相邻性奖励计算器
            try:
                rewards_dict = self.adjacency_calculator.calculate_reward(
                    tuple(placed_depts)
                )
                return {
                    'spatial_reward': rewards_dict.get('spatial_reward', 0.0),
                    'functional_reward': rewards_dict.get('functional_reward', 0.0),
                    'connectivity_reward': rewards_dict.get('connectivity_reward', 0.0),
                    'total_reward': rewards_dict.get('total_reward', 0.0)
                }
            except Exception as e:
                logger.error(f"优化相邻性统计计算失败，降级到传统方法：{e}")
                # 降级到传统方法
        
        # 使用传统方法计算各维度相邻性奖励
        spatial_reward = self._calculate_spatial_adjacency_reward(placed_depts)
        functional_reward = self._calculate_functional_adjacency_reward(placed_depts)
        connectivity_reward = 0.0
        
        if self.config.CONNECTIVITY_ADJACENCY_WEIGHT > 0:
            connectivity_reward = self._calculate_connectivity_adjacency_reward(placed_depts)
        
        # 计算总奖励
        total_reward = (
            self.config.SPATIAL_ADJACENCY_WEIGHT * spatial_reward +
            self.config.FUNCTIONAL_ADJACENCY_WEIGHT * functional_reward +
            self.config.CONNECTIVITY_ADJACENCY_WEIGHT * connectivity_reward
        ) * self.config.ADJACENCY_REWARD_BASE
        
        return {
            'spatial_reward': spatial_reward,
            'functional_reward': functional_reward,
            'connectivity_reward': connectivity_reward,
            'total_reward': total_reward
        }

    def _log_area_match_statistics(self):
        """记录面积匹配统计信息到日志。"""
        stats = self._calculate_area_match_statistics()
        if stats['num_matched'] > 0:
            adjacency_info = ""
            if self.config.ENABLE_ADJACENCY_REWARD:
                # 获取当前已放置的科室
                placed_depts = []
                for slot_idx in range(self.num_slots):
                    dept_id = self.layout[slot_idx]
                    if dept_id > 0:
                        placed_depts.append(self.placeable_depts[dept_id - 1])
                
                if len(placed_depts) >= 2:
                    adj_stats = self._calculate_adjacency_statistics(placed_depts)
                    adjacency_info = (f", 相邻性奖励: 空间={adj_stats['spatial_reward']:.3f}, "
                                    f"功能={adj_stats['functional_reward']:.3f}, "
                                    f"连通性={adj_stats['connectivity_reward']:.3f}, "
                                    f"总计={adj_stats['total_reward']:.3f}")
            
            logger.info(f"面积匹配统计: 平均匹配度={stats['avg_match_score']:.3f}, "
                       f"最小={stats['min_match_score']:.3f}, 最大={stats['max_match_score']:.3f}, "
                       f"已匹配科室数={stats['num_matched']}{adjacency_info}")
    
    
    def render(self, mode="human"):
        """(可选) 渲染环境状态，用于调试。"""
        if mode == "human":
            print(f"--- Step: {self.current_step} ---")
            current_slot_idx = self.shuffled_slot_indices[self.current_step]
            current_slot = self.placeable_slots[current_slot_idx]
            print(f"Current Slot to Fill: {current_slot} (Area: {self.slot_areas[current_slot_idx]:.2f})")
            
            layout_str = []
            for i in range(self.num_slots):
                dept_id = self.layout[i]
                if dept_id > 0:
                    layout_str.append(f"{self.placeable_slots[i]}: {self.placeable_depts[dept_id-1]}")
                else:
                    layout_str.append(f"{self.placeable_slots[i]}: EMPTY")
            print("Current Layout:\n" + "\n".join(layout_str))

    @staticmethod
    def _action_mask_fn(env: 'LayoutEnv') -> np.ndarray:
        """
        ActionMasker 包装器所需的静态方法，用于提取动作掩码。
        
        Args:
            env (LayoutEnv): 环境实例。
            
        Returns:
            np.ndarray: 当前状态下的动作掩码。
        """
        return env.get_action_mask()

    def _get_final_layout_str(self) -> List[str]:
        """
        获取最终布局的科室名称列表。
        
        Returns:
            List[str]: 按槽位顺序排列的科室名称列表。
        """
        final_layout = [None] * self.num_slots
        for slot_idx, dept_id in enumerate(self.layout):
            if dept_id > 0:
                final_layout[slot_idx] = self.placeable_depts[dept_id - 1]
        return final_layout
</file>

<file path="src/network/network.py">
"""
Orchestrates the construction of a single-floor network graph.
"""

import numpy as np
import networkx as nx
from src.rl_optimizer.utils.setup import setup_logger
from typing import Dict, Tuple, Any, List, Optional
from scipy.spatial import KDTree

from src.config import NetworkConfig
from .graph_manager import GraphManager
from .node import Node
from src.image_processing.processor import ImageProcessor
from .node_creators import (  # 节点创建器模块
    BaseNodeCreator,
    RoomNodeCreator,
    VerticalNodeCreator,
    PedestrianNodeCreator,
    OutsideNodeCreator,
    ConnectionNodeCreator
)

logger = setup_logger(__name__)

class Network:
    """
    Manages the creation of a network graph for a single floor from an image.

    The process involves:
    1. Loading and preprocessing the image.
    2. Creating different types of nodes (rooms, doors, corridors, etc.) using
       specialized NodeCreator strategies.
    3. Establishing connections between nodes, including specific logic for
       connecting doors to pedestrian/outside mesh areas.
    """

    def __init__(self,
                 config: NetworkConfig,
                 color_map_data: Dict[Tuple[int, int, int], Dict[str, Any]],
                 id_generator_start_value: int):
        """
        Initializes the Network orchestrator.

        Args:
            config: The main configuration object.
            color_map_data: The RGB color to type mapping.
            id_generator_start_value: The starting ID for nodes in this network.
                                      Crucial for `SuperNetwork` to ensure global ID uniqueness
                                      when processing multiple floors in parallel.
        """
        self.config = config
        self.color_map_data = color_map_data  # 传递给创建器的颜色映射数据

        self.image_processor = ImageProcessor(config, color_map_data)
        self.graph_manager = GraphManager(id_generator_start_value)

        # 初始化节点创建器
        self._node_creators: List[BaseNodeCreator] = [
            RoomNodeCreator(config, color_map_data,
                            self.image_processor, self.graph_manager),
            VerticalNodeCreator(config, color_map_data,
                                self.image_processor, self.graph_manager),
            # Pedestrian and Outside creators mark areas in id_map first, then create mesh.
            # Connection creator relies on these id_map markings.
            PedestrianNodeCreator(config, color_map_data,
                                  self.image_processor, self.graph_manager),
            # Outside creator might be conditional based on 'outside' flag in run()
            # 连接创建器应该在房间、垂直、行人、室外区域被标记/创建之后运行
            ConnectionNodeCreator(config, color_map_data,
                                  self.image_processor, self.graph_manager)
        ]

        # have an instance of OutsideNodeCreator for conditional use
        self._outside_node_creator = OutsideNodeCreator(
            config, color_map_data, self.image_processor, self.graph_manager)

        self._current_image_data: Optional[np.ndarray] = None
        self._id_map: Optional[np.ndarray] = None
        self._image_height: Optional[int] = None
        self._image_width: Optional[int] = None

    def _initialize_run(self, image_path: str) -> None:
        """Loads image, prepares internal data structures for a run."""
        # Load and preprocess image (quantize colors)
        raw_image_data = self.image_processor.load_and_prepare_image(
            image_path)
        self._current_image_data = self.image_processor.quantize_colors(
            raw_image_data)

        self._image_height, self._image_width = self.image_processor.get_image_dimensions()

        # id_map stores the ID of the node occupying each pixel, or special area IDs
        self._id_map = np.full((self._image_height, self._image_width),
                               self.config.BACKGROUND_ID_MAP_VALUE, dtype=np.int32)  # 使用int32类型存储ID

        # This ensures that doors of type 'out' are recognized correctly even if the OutsideNodeCreator is not run
        # to create detailed outside mesh nodes.
        if self.config.OUTSIDE_TYPES:  # 检查是否定义了室外类型
            for outside_type_name in self.config.OUTSIDE_TYPES:
                # 从量化图像中为此室外类型创建掩码
                # We don't need full morphology here, just the raw areas.
                # ConnectionNodeCreator will use its own dilation.
                outside_mask = self._outside_node_creator._create_mask_for_type(
                    self._current_image_data,
                    outside_type_name,
                    apply_morphology=True  # 应用基础形态学操作清理掩码
                )
                if outside_mask is not None:
                    self._id_map[outside_mask !=
                                 0] = self.config.OUTSIDE_ID_MAP_VALUE

    def _create_all_node_types(self, z_level: int, process_outside_nodes: bool) -> None:
        """Iterates through node creators to populate the graph."""
        if self._current_image_data is None or self._id_map is None:
            raise RuntimeError(
                "Network run not initialized properly. Call _initialize_run first.")

        # Specific order of creation can be important
        # 1. Rooms and Vertical transport (solid areas with own IDs)
        # 2. Pedestrian areas (mesh + special ID in id_map)
        # 3. Outside areas (mesh + special ID in id_map) - if requested
        # 4. Connections (doors - rely on previously set IDs in id_map)

        # Execute creators in the predefined order
        for creator in self._node_creators:
            logger.info(f"Running creator: {creator.__class__.__name__}")
            creator.create_nodes(self._current_image_data,
                                 self._id_map, z_level)

        # 注意：室外节点创建已被禁用，因为在医院室内布局优化中不需要室外节点
        # 如果将来需要处理室外区域，可以通过配置参数process_outside_nodes来启用
        # if process_outside_nodes:
        #     logger.info(f"运行节点创建器: {self._outside_node_creator.__class__.__name__} (mesh节点)")
        #     self._outside_node_creator.create_nodes(
        #         self._current_image_data, self._id_map, z_level)

    def _connect_doors_to_mesh_areas(self, z_level: int) -> None:
        """
        Connects door nodes (type 'in' or 'out') to the nearest pedestrian/outside
        mesh nodes respectively.
        """
        if not self.graph_manager.get_all_nodes():
            return  # Optimization

        connection_nodes = [
            node for node in self.graph_manager.get_all_nodes().values()
            if node.node_type in self.config.CONNECTION_TYPES and node.z == z_level
            # Only connect these
            and hasattr(node, 'door_type') and (node.door_type == 'in' or node.door_type == 'out')
        ]
        if not connection_nodes:
            return

        pedestrian_mesh_nodes = [
            node for node in self.graph_manager.get_all_nodes().values()
            if node.node_type in self.config.PEDESTRIAN_TYPES and node.z == z_level
        ]
        outside_mesh_nodes = [
            node for node in self.graph_manager.get_all_nodes().values()
            if node.node_type in self.config.OUTSIDE_TYPES and node.z == z_level
        ]

        ped_tree = None
        if pedestrian_mesh_nodes:
            ped_positions = np.array([(p_node.x, p_node.y)
                                     for p_node in pedestrian_mesh_nodes])
            if ped_positions.size > 0:  # Ensure not empty before creating KDTree
                ped_tree = KDTree(ped_positions)

        out_tree = None
        if outside_mesh_nodes:
            out_positions = np.array([(o_node.x, o_node.y)
                                     for o_node in outside_mesh_nodes])
            if out_positions.size > 0:
                out_tree = KDTree(out_positions)

        max_door_to_mesh_distance = self.config.GRID_SIZE * 3

        for conn_node in connection_nodes:
            door_pos_2d = (conn_node.x, conn_node.y)

            if conn_node.door_type == 'in' and ped_tree:
                dist, idx = ped_tree.query(door_pos_2d, k=1)
                # Check if idx is a valid index and not out of bounds (e.g. if ped_tree was empty for some reason)
                if idx < len(pedestrian_mesh_nodes) and dist <= max_door_to_mesh_distance:
                    nearest_ped_node = pedestrian_mesh_nodes[idx]
                    self.graph_manager.connect_nodes_by_ids(
                        conn_node.id, nearest_ped_node.id)

            elif conn_node.door_type == 'out':  # 'out' doors connect to outside AND potentially nearby pedestrian areas
                connected_to_main_outside = False
                if out_tree:
                    dist, idx = out_tree.query(door_pos_2d, k=1)
                    if idx < len(outside_mesh_nodes) and dist <= max_door_to_mesh_distance:
                        nearest_out_node = outside_mesh_nodes[idx]
                        self.graph_manager.connect_nodes_by_ids(
                            conn_node.id, nearest_out_node.id)
                        connected_to_main_outside = True

                # Also check for nearby pedestrian nodes if this 'out' door is on a path
                if ped_tree:
                    # Query for potentially multiple pedestrian nodes within a smaller radius
                    # This is for cases like an exit onto a patio (pedestrian) then to lawn (outside)
                    indices_in_ball = ped_tree.query_ball_point(
                        door_pos_2d, r=np.sqrt(2 * self.config.GRID_SIZE ** 2))
                    for ped_idx in indices_in_ball:
                        if ped_idx < len(pedestrian_mesh_nodes):
                            self.graph_manager.connect_nodes_by_ids(
                                conn_node.id, pedestrian_mesh_nodes[ped_idx].id)
                            # If it connects to outside mesh AND pedestrian mesh, that's fine.
                            # The pathfinding will choose the best route.

    def run(self, image_path: str, z_level: int = 0, process_outside_nodes: bool = False) \
            -> Tuple[nx.Graph, int, int, int]:
        """
        Executes the full network generation pipeline.
        Args:
            process_outside_nodes: If True, detailed mesh nodes for outside areas are created.
                                   If False, outside areas are only marked in id_map (for door typing)
                                   but no actual outside mesh nodes are generated by OutsideNodeCreator.
        """
        logger.info(f"--- Processing floor: {image_path} at z={z_level}, process_outside_nodes={process_outside_nodes} ---")
        
        # self.graph_manager.clear(...) # Only if reusing Network instance, typically not.

        self._initialize_run(image_path) # This now pre-marks OUTSIDE_ID_MAP_VALUE
        
        self._create_all_node_types(z_level, process_outside_nodes) # process_outside_nodes controls mesh creation
        
        self._connect_doors_to_mesh_areas(z_level)
        
        logger.info(f"--- Finished floor. Nodes: {len(self.graph_manager.get_all_nodes())} ---")
        
        if self._image_width is None or self._image_height is None:
            raise RuntimeError("Image dimensions not set.")

        return (
            self.graph_manager.get_graph_copy(),
            self._image_width,
            self._image_height,
            self.graph_manager.get_next_available_node_id_estimate()
        )
</file>

<file path="main.py">
"""
医院布局优化系统主入口

统一的命令行接口，支持网络生成、算法优化和结果对比分析。
整合了所有功能模块，提供完整的医院布局优化解决方案。
"""

import argparse
import sys
import pathlib
from typing import List, Optional, Dict, Any
import pandas as pd

# 导入核心模块
from src.config import NetworkConfig, RLConfig, COLOR_MAP
from src.core.network_generator import NetworkGenerator
from src.core.algorithm_manager import AlgorithmManager
from src.comparison.results_comparator import ResultsComparator

# 导入优化组件
from src.rl_optimizer.data.cache_manager import CacheManager
from src.rl_optimizer.env.cost_calculator import CostCalculator
from src.algorithms.constraint_manager import ConstraintManager
from src.rl_optimizer.utils.setup import setup_logger

logger = setup_logger(__name__)



class HospitalLayoutOptimizer:
    """医院布局优化系统主类"""
    
    def __init__(self):
        """初始化系统"""
        self.network_config = NetworkConfig(color_map_data=COLOR_MAP)
        self.rl_config = RLConfig()
        self.network_generator = None
        self.algorithm_manager = None
        
        logger.info("医院布局优化系统初始化完成")
    
    def run_network_generation(self, 
                             image_dir: str = "./data/label/",
                             visualization_filename: str = "hospital_network_3d.html",
                             travel_times_filename: str = "hospital_travel_times.csv") -> bool:
        """
        运行网络生成
        
        Args:
            image_dir: 楼层标注图像目录
            visualization_filename: 可视化输出文件名
            travel_times_filename: 行程时间输出文件名
            
        Returns:
            bool: 是否成功
        """
        logger.info("=== 开始网络生成阶段 ===")
        
        self.network_generator = NetworkGenerator(self.network_config)
        
        success = self.network_generator.run_complete_generation(
            image_dir=image_dir,
            visualization_filename=visualization_filename,
            travel_times_filename=travel_times_filename
        )
        
        if success:
            network_info = self.network_generator.get_network_info()
            logger.info("网络生成完成，统计信息:")
            for key, value in network_info.items():
                logger.info(f"  {key}: {value}")
        
        return success
    
    def run_single_algorithm(self, 
                           algorithm_name: str,
                           travel_times_file: str = None,
                           **kwargs) -> bool:
        """
        运行单个优化算法
        
        Args:
            algorithm_name: 算法名称
            travel_times_file: 行程时间文件路径
            **kwargs: 算法特定参数
            
        Returns:
            bool: 是否成功
        """
        logger.info(f"=== 开始运行算法: {algorithm_name} ===")
        
        if travel_times_file is None:
            travel_times_file = self.rl_config.TRAVEL_TIMES_CSV
        
        # 初始化算法管理器
        if not self._initialize_algorithm_manager(travel_times_file):
            return False
        
        try:
            result = self.algorithm_manager.run_single_algorithm(
                algorithm_name=algorithm_name,
                custom_params=kwargs
            )
            
            logger.info(f"算法 {algorithm_name} 执行成功:")
            logger.info(f"  最优成本: {result.best_cost:.2f}")
            logger.info(f"  执行时间: {result.execution_time:.2f}秒")
            logger.info(f"  迭代次数: {result.iterations}")
            
            # 保存结果
            self.algorithm_manager.save_results()
            
            return True
            
        except Exception as e:
            logger.error(f"算法执行失败: {e}", exc_info=True)
            return False
    
    def run_algorithm_comparison(self, 
                               algorithm_names: List[str],
                               travel_times_file: str = None,
                               parallel: bool = False,
                               generate_plots: bool = True,
                               generate_report: bool = True) -> bool:
        """
        运行算法对比分析
        
        Args:
            algorithm_names: 算法名称列表
            travel_times_file: 行程时间文件路径
            parallel: 是否并行执行
            generate_plots: 是否生成图表
            generate_report: 是否生成报告
            
        Returns:
            bool: 是否成功
        """
        logger.info(f"=== 开始算法对比分析: {algorithm_names} ===")
        
        if travel_times_file is None:
            travel_times_file = self.rl_config.TRAVEL_TIMES_CSV
        
        # 初始化算法管理器
        if not self._initialize_algorithm_manager(travel_times_file):
            return False
        
        try:
            # 运行多个算法
            results = self.algorithm_manager.run_multiple_algorithms(
                algorithm_names=algorithm_names,
                parallel=parallel
            )
            
            if not results:
                logger.error("没有算法成功执行")
                return False
            
            logger.info(f"成功执行 {len(results)} 个算法")
            
            # 生成对比表格
            comparison_df = self.algorithm_manager.get_algorithm_comparison()
            logger.info("算法对比结果:")
            logger.info(f"\n{comparison_df.to_string(index=False)}")
            
            # 创建结果对比分析器
            comparator = ResultsComparator(results, cache_manager=self.cache_manager)
            
            # 生成详细对比表格
            detailed_df = comparator.generate_comparison_table()
            
            # 生成图表
            if generate_plots:
                comparator.create_comparison_plots()
                logger.info("对比图表已生成")
            
            # 生成报告
            if generate_report:
                report_path = comparator.generate_detailed_report()
                logger.info(f"详细报告已生成: {report_path}")
            
            # 导出布局对比
            layouts_path = comparator.export_layouts_comparison()
            logger.info(f"最优布局对比已导出: {layouts_path}")
            
            # 保存结果
            self.algorithm_manager.save_results()
            
            # 输出最佳结果
            best_result = self.algorithm_manager.get_best_result()
            if best_result:
                logger.info(f"整体最佳结果来自: {best_result.algorithm_name}")
                logger.info(f"最优成本: {best_result.best_cost:.2f}")
            
            return True
            
        except Exception as e:
            logger.error(f"算法对比分析失败: {e}", exc_info=True)
            return False
    
    def _initialize_algorithm_manager(self, travel_times_file: str) -> bool:
        """初始化算法管理器"""
        try:
            # 检查行程时间文件
            travel_times_path = pathlib.Path(travel_times_file)
            if not travel_times_path.exists():
                logger.error(f"行程时间文件不存在: {travel_times_file}")
                logger.error("请先运行网络生成阶段：python main.py --mode network")
                return False
            
            logger.info("正在初始化优化组件...")
            
            # 初始化缓存管理器
            cache_manager = CacheManager(self.rl_config)
            logger.info("缓存管理器初始化完成")
            
            # 初始化成本计算器
            cost_calculator = CostCalculator(
                config=self.rl_config,
                resolved_pathways=cache_manager.resolved_pathways,
                travel_times=cache_manager.travel_times_matrix,
                placeable_slots=cache_manager.placeable_slots,
                placeable_departments=cache_manager.placeable_departments
            )
            logger.info("成本计算器初始化完成")
            
            # 初始化约束管理器
            constraint_manager = ConstraintManager(
                config=self.rl_config,
                cache_manager=cache_manager
            )
            logger.info("约束管理器初始化完成")
            
            # 初始化算法管理器
            self.algorithm_manager = AlgorithmManager(
                cost_calculator=cost_calculator,
                constraint_manager=constraint_manager,
                config=self.rl_config,
                cache_manager=cache_manager
            )
            logger.info("算法管理器初始化完成")
            
            # 保存cache_manager供后续使用
            self.cache_manager = cache_manager
            
            return True
            
        except Exception as e:
            logger.error(f"算法管理器初始化失败: {e}", exc_info=True)
            return False
    
    def visualize_results(self, results_file: str):
        """可视化算法结果"""
        logger.info(f"=== 开始结果可视化: {results_file} ===")
        # 这里可以添加结果可视化逻辑
        logger.info("结果可视化功能待实现")


def create_argument_parser() -> argparse.ArgumentParser:
    """创建命令行参数解析器"""
    parser = argparse.ArgumentParser(
        description="医院布局优化系统 - 整合网络生成、算法优化和结果对比分析",
        formatter_class=argparse.RawTextHelpFormatter
    )
    
    parser.add_argument(
        '--mode',
        type=str,
        choices=['network', 'optimize', 'compare', 'visualize'],
        required=True,
        help="运行模式:\n"
             "  network    - 生成医院网络和行程时间矩阵\n"
             "  optimize   - 运行单个优化算法\n"
             "  compare    - 运行多个算法进行对比分析\n"
             "  visualize  - 可视化算法结果"
    )
    
    parser.add_argument(
        '--algorithm',
        type=str,
        choices=['ppo', 'simulated_annealing', 'genetic_algorithm'],
        help="优化算法名称 (用于 optimize 模式)"
    )
    
    parser.add_argument(
        '--algorithms',
        type=str,
        help="算法列表，用逗号分隔 (用于 compare 模式)\n"
             "例如: ppo,simulated_annealing,genetic_algorithm"
    )
    
    parser.add_argument(
        '--image-dir',
        type=str,
        default="./data/label/",
        help="楼层标注图像目录 (默认: ./data/label/)"
    )
    
    parser.add_argument(
        '--travel-times-file',
        type=str,
        default=None,
        help="行程时间文件路径 (默认: 由config.py中的RLConfig.TRAVEL_TIMES_CSV指定)"
    )
    
    parser.add_argument(
        '--parallel',
        action='store_true',
        help="并行执行多个算法 (用于 compare 模式)"
    )
    
    parser.add_argument(
        '--no-plots',
        action='store_true',
        help="不生成对比图表"
    )
    
    parser.add_argument(
        '--no-report',
        action='store_true',
        help="不生成详细报告"
    )
    
    parser.add_argument(
        '--results-file',
        type=str,
        help="结果文件路径 (用于 visualize 模式)"
    )
    
    # 算法特定参数
    parser.add_argument(
        '--max-iterations',
        type=int,
        help="最大迭代次数"
    )
    
    parser.add_argument(
        '--population-size',
        type=int,
        help="遗传算法种群大小"
    )
    
    parser.add_argument(
        '--initial-temperature',
        type=float,
        help="模拟退火初始温度"
    )
    
    parser.add_argument(
        '--total-timesteps',
        type=int,
        help="PPO总训练步数"
    )
    
    parser.add_argument(
        '--pretrained-model-path',
        type=str,
        help="预训练模型路径（用于从已有模型继续训练）"
    )
    
    parser.add_argument(
        '--verbose',
        action='store_true',
        help="详细输出"
    )
    
    return parser


def main():
    """主函数"""
    # 创建参数解析器
    parser = create_argument_parser()
    args = parser.parse_args()
    
    # 设置日志
    log_level = 10 if args.verbose else 20  # DEBUG=10, INFO=20
    log_dir = pathlib.Path("./logs")
    log_dir.mkdir(parents=True, exist_ok=True)
    # loguru logger会在setup_logger中自动配置文件输出
    
    logger.info("=== 医院布局优化系统启动 ===")
    logger.info(f"运行模式: {args.mode}")
    
    # 创建系统实例
    optimizer = HospitalLayoutOptimizer()
    
    success = False
    
    try:
        if args.mode == 'network':
            # 网络生成模式
            success = optimizer.run_network_generation(
                image_dir=args.image_dir
            )
            
        elif args.mode == 'optimize':
            # 单算法优化模式
            if not args.algorithm:
                logger.error("optimize 模式需要指定 --algorithm 参数")
                sys.exit(1)
            
            # 构建算法参数
            algorithm_params = {}
            if args.max_iterations:
                algorithm_params['max_iterations'] = args.max_iterations
            if args.population_size:
                algorithm_params['population_size'] = args.population_size
            if args.initial_temperature:
                algorithm_params['initial_temperature'] = args.initial_temperature
            if args.total_timesteps:
                algorithm_params['total_timesteps'] = args.total_timesteps
            if args.pretrained_model_path:
                algorithm_params['pretrained_model_path'] = args.pretrained_model_path
            
            success = optimizer.run_single_algorithm(
                algorithm_name=args.algorithm,
                travel_times_file=args.travel_times_file,
                **algorithm_params
            )
            
        elif args.mode == 'compare':
            # 算法对比模式
            if not args.algorithms:
                logger.error("compare 模式需要指定 --algorithms 参数")
                sys.exit(1)
            
            algorithm_names = [name.strip() for name in args.algorithms.split(',')]
            
            success = optimizer.run_algorithm_comparison(
                algorithm_names=algorithm_names,
                travel_times_file=args.travel_times_file,
                parallel=args.parallel,
                generate_plots=not args.no_plots,
                generate_report=not args.no_report
            )
            
        elif args.mode == 'visualize':
            # 结果可视化模式
            if not args.results_file:
                logger.error("visualize 模式需要指定 --results-file 参数")
                sys.exit(1)
            
            optimizer.visualize_results(args.results_file)
            success = True
        
        if success:
            logger.info("=== 系统执行成功完成 ===")
        else:
            logger.error("=== 系统执行失败 ===")
            sys.exit(1)
            
    except KeyboardInterrupt:
        logger.warning("用户中断执行")
        sys.exit(1)
    except Exception as e:
        logger.error(f"系统执行异常: {e}", exc_info=True)
        sys.exit(1)


if __name__ == "__main__":
    main()
</file>

<file path="src/config.py">
"""Configuration module for the network generation project."""

import pathlib
from typing import Dict, Tuple, List, Any, Optional

# 全局颜色映射表 - 医院区域类型的RGB颜色编码
COLOR_MAP: Dict[Tuple[int, int, int], Dict[str, Any]] = {
    (244, 67, 54): {'name': '内诊药房', 'time': 781, 'e_name': 'Pharmacy', 'code': 'T16'},
    (0, 150, 136): {'name': '挂号收费', 'time': 678, 'e_name': 'Registration & fee', 'code': 'R1-R4'},
    (103, 58, 183): {'name': '急诊科', 'time': 9000, 'e_name': 'Emergency', 'code': 'D26'},
    (145, 102, 86): {'name': '中心供应室', 'time': 1, 'e_name': 'CSSD', 'code': 'T3'},
    (33, 150, 243): {'name': '全科', 'time': 1965, 'e_name': 'General practice', 'code': 'D1'},
    (3, 169, 244): {'name': '放射科', 'time': 1484, 'e_name': 'Radiology', 'code': 'T2'},
    (0, 188, 212): {'name': '儿科', 'time': 5632, 'e_name': 'Pediatrics', 'code': 'D2'},
    (207, 216, 220): {'name': '走廊', 'time': 1, 'e_name': "Corridor", 'code': "Z1"},
    (117, 117, 117): {'name': '楼梯', 'time': 1, 'e_name': "Stairs", 'code': "Z2"},
    (189, 189, 189): {'name': '电梯', 'time': 1, 'e_name': "Elevator", 'code': "Z3"},
    (158, 158, 158): {'name': '扶梯', 'time': 1, 'e_name': "Escalator", 'code': "Z4"},
    (76, 175, 80): {'name': '绿化', 'time': 1, 'e_name': "Greening", 'code': "Z5"},
    (255, 235, 59): {'name': '墙', 'time': 1, 'e_name': "Wall", 'code': "Z6"},
    (121, 85, 72): {'name': '门', 'time': 1, 'e_name': "Door", 'code': "Z7"},
    (156, 39, 176): {'name': '室外', 'time': 1, 'e_name': "Outdoor", 'code': "Z8"},
    (139, 195, 74): {'name': '内镜中心', 'time': 5333, 'e_name': 'Endoscopy', 'code': 'T7'},
    (205, 220, 57): {'name': '检验中心', 'time': 180, 'e_name': 'Clinical Laboratory', 'code': 'T4'},
    (255, 193, 7): {'name': '消化内科', 'time': 916, 'e_name': 'Gastroenterology', 'code': 'D10'},
    (255, 152, 0): {'name': '甲状腺外科', 'time': 5637, 'e_name': 'Thyroid surgery', 'code': 'D18'},
    (254, 87, 34): {'name': '肾内科', 'time': 1493, 'e_name': 'Nephrology', 'code': 'D6'},
    (169, 238, 90): {'name': '心血管内科', 'time': 8089, 'e_name': 'Cardiovascular Medicine', 'code': 'D8'},
    (88, 67, 60): {'name': '采血处', 'time': 1104, 'e_name': 'Blood collection', 'code': 'T15'},
    (239, 199, 78): {'name': '眼科', 'time': 3228, 'e_name': 'Ophthalmology', 'code': 'D22'},
    (253, 186, 87): {'name': '中医科', 'time': 3110, 'e_name': 'Chinese medicine', 'code': 'D3'},
    (250, 133, 96): {'name': '耳鼻喉科', 'time': 9550, 'e_name': 'Otolaryngology', 'code': 'D23'},
    (197, 254, 130): {'name': '口腔一区', 'time': 10542, 'e_name': 'Dental clinic 1', 'code': 'D24'},
    (173, 133, 11): {'name': '超声科', 'time': 3023, 'e_name': 'Ultrasound', 'code': 'T1'},
    (119, 90, 10): {'name': '病理科', 'time': 1, 'e_name': 'Pathology', 'code': 'T5'},
    (250, 146, 138): {'name': '骨科', 'time': 1090, 'e_name': 'Orthopedics', 'code': 'D7'},
    (255, 128, 171): {'name': '泌尿外科', 'time': 1019, 'e_name': 'Urology', 'code': 'D12'},
    (33, 250, 230): {'name': '肝胆胰外科', 'time': 2953, 'e_name': 'Hepatobiliary & Pancreatic', 'code': 'D19'},
    (82, 108, 255): {'name': '皮肤科', 'time': 2802, 'e_name': 'Dermatology', 'code': 'D21'},
    (226, 58, 255): {'name': '妇科', 'time': 5000, 'e_name': 'Gynaecology', 'code': 'D15'},
    (100, 139, 55): {'name': '产科', 'time': 2208, 'e_name': 'Obstetrics', 'code': 'D14'},
    (170, 190, 150): {'name': '产房', 'time': 1, 'e_name': 'Delivery room', 'code': 'T9'},
    (113, 134, 91): {'name': '手术室', 'time': 1, 'e_name': 'Theater', 'code': 'T13'},
    (175, 207, 142): {'name': '门诊手术室', 'time': 1, 'e_name': 'Ambulatory Surgery', 'code': 'T12'},
    (179, 116, 190): {'name': '中庭', 'time': 1, 'e_name': "Courtyard", 'code': "Z9"},
    (232, 137, 248): {'name': '口腔科二区', 'time': 4964, 'e_name': 'Dental clinic 2', 'code': 'D25'},
    (63, 100, 23): {'name': '神经内科', 'time': 2396, 'e_name': 'Neurology', 'code': 'D9'},
    (240, 222, 165): {'name': '呼吸内科', 'time': 1457, 'e_name': 'Respiratory', 'code': 'D5'},
    (187, 24, 80): {'name': '综合激光科', 'time': 1848, 'e_name': 'Laser clinic', 'code': 'D20'},
    (150, 133, 179): {'name': '透析中心', 'time': 44267, 'e_name': 'Haemodialysis unit', 'code': 'T8'},
    (112, 40, 236): {'name': '肿瘤科', 'time': 11729, 'e_name': 'Oncology', 'code': 'D11'},
    (241, 190, 186): {'name': '产前诊断门诊', 'time': 1252, 'e_name': 'Prenatal Diagnosis', 'code': 'D13'},
    (186, 146, 160): {'name': '体检科', 'time': 7336, 'e_name': 'Physical Examination', 'code': 'D4'},
    (71, 195, 180): {'name': '生殖医学科', 'time': 1086, 'e_name': 'Reproductive Medicine', 'code': 'D16'},
    (187, 152, 247): {'name': '烧伤整形科', 'time': 854, 'e_name': 'Plastic surgery', 'code': 'D17'},
    (254, 210, 145): {'name': '介入科', 'time': 14688, 'e_name': 'Interventional Therapy', 'code': 'T6'},
    (251, 242, 159): {'name': '栏杆', 'time': 1, 'e_name': "Handrail", 'code': "Z10"},
    (240, 61, 123): {'name': 'NICU', 'time': 1, 'e_name': 'NICU', 'code': 'T10'},
    (250, 162, 193): {'name': 'ICU', 'time': 1, 'e_name': 'ICU', 'code': 'T11'},
    (252, 201, 126): {'name': '静配中心', 'time': 1, 'e_name': 'PIVAS', 'code': 'T14'},
    (255, 255, 255): {'name': '空房间', 'time': 1, 'e_name': "Vacant Room", 'code': "Z11"}
}

class NetworkConfig:
    """Stores configuration parameters for network generation and plotting."""

    def __init__(self, color_map_data: Dict[Tuple[int, int, int], Dict[str, Any]] = COLOR_MAP):
        self.RESULT_PATH: pathlib.Path = pathlib.Path(__file__).parent.parent / 'results' / 'network'
        self.DEBUG_PATH: pathlib.Path = pathlib.Path(__file__).parent.parent / 'debug'
        self.IMAGE_ROTATE: int = 180
        self.AREA_THRESHOLD: int = 60  # 被视为节点的连通区域最小面积阈值

        # Node Type Definitions (derived from COLOR_MAP)
        self.ALL_TYPES: List[str] = [v['name'] for v in color_map_data.values()]

        self.CONNECTION_TYPES: List[str] = ['门']
        _ban_type_base: List[str] = ['墙', '栏杆', '室外', '走廊', '电梯', '扶梯', '楼梯', '空房间', '绿化', '中庭']
        self.BAN_TYPES: List[str] = [name for name in _ban_type_base if name in self.ALL_TYPES]

        self.ROOM_TYPES: List[str] = [
            v['name'] for v in color_map_data.values()
            if v['name'] not in self.BAN_TYPES and v['name'] not in self.CONNECTION_TYPES
        ]
        self.VERTICAL_TYPES: List[str] = [name for name in ['电梯', '扶梯', '楼梯'] if name in self.ALL_TYPES]
        self.PEDESTRIAN_TYPES: List[str] = [name for name in ['走廊'] if name in self.ALL_TYPES]
        self.OUTSIDE_TYPES: List[str] = [name for name in ['室外'] if name in self.ALL_TYPES]

        # 网格和特殊ID用于在id_map中进行像素级别的识别
        self.GRID_SIZE: int = 40  # mesh节点生成的基础网格大小
        self.OUTSIDE_ID_MAP_VALUE: int = -1  # id_map中'室外'区域的特殊ID
        self.BACKGROUND_ID_MAP_VALUE: int = -2  # id_map中'背景'区域的特殊ID
        self.PEDESTRIAN_ID_MAP_VALUE: int = -3  # id_map中'行人通道'区域的特殊ID

        # 节点属性时间配置
        self.OUTSIDE_MESH_TIMES_FACTOR: int = 2  # 室外节点的网格大小和时间乘数因子
        self.PEDESTRIAN_TIME: float = 1.75  # 行人节点的默认通行时间
        self.CONNECTION_TIME: float = 3.0  # Default time for connection nodes (e.g., doors)

        # 绘图和可视化配置
        self.IMAGE_MIRROR: bool = True  # 是否在绘图中水平镜像图像
        self.NODE_COLOR_FROM_MAP: bool = True  # 在绘图中是否使用COLOR_MAP的颜色显示节点

        self.NODE_SIZE_DEFAULT: int = 2
        self.NODE_SIZE_PEDESTRIAN: int = 2
        self.NODE_SIZE_CONNECTION: int = 2
        self.NODE_SIZE_VERTICAL: int = 2
        self.NODE_SIZE_ROOM: int = 7
        self.NODE_SIZE_OUTSIDE: int = 2
        self.NODE_OPACITY: float = 0.8
        self.SHOW_PEDESTRIAN_LABELS: bool = False

        self.HORIZONTAL_EDGE_COLOR: str = "#1f77b4"
        self.VERTICAL_EDGE_COLOR: str = "rgba(151,152,155,0.7)"  #"#ff7e0e75"
        self.DOOR_EDGE_COLOR: str = "#2ca02c"
        self.SPECIAL_EDGE_COLOR: str = "#d62728"
        self.EDGE_WIDTH: float = 1.5

        self.X_AXIS_RATIO: float = 1
        self.Y_AXIS_RATIO: float = 1
        self.Z_AXIS_RATIO: float = 2

        #Plotly Configuration
        self.PLOTLY_CONFIG = {
            'toImageButtonOptions': {
                'format': 'png',
                'filename': 'Network',
                'scale': 3
            }
        }

        # SuperNetwork多层网络专用配置
        self.DEFAULT_FLOOR_HEIGHT: float = 10
        self.DEFAULT_VERTICAL_CONNECTION_TOLERANCE: int = 0  # 跨楼层连接垂直节点的默认像素距离容差
        # Estimated max nodes per floor. Used for pre-allocating ID ranges in multi-processing.
        # Should be an overestimate to avoid ID collisions.
        self.ESTIMATED_MAX_NODES_PER_FLOOR: int = 10000
        self.DEFAULT_OUTSIDE_PROCESSING_IN_SUPERNETWORK: bool = False # Default for processing outside nodes per floor in SuperNetwork
        self.GROUND_FLOOR_NUMBER_FOR_OUTSIDE: Optional[int] = None # Or 0, or None to rely on auto-detection

        # Morphology Kernel
        self.MORPHOLOGY_KERNEL_SIZE: Tuple[int, int] = (5, 5)
        self.CONNECTION_DILATION_KERNEL_SIZE: Tuple[int, int] = (3,3)

        # KDTree query parameters
        self.MESH_NODE_CONNECTIVITY_K: int = 9 # k-nearest neighbors for mesh node connection

        # Ensure paths exist
        self.RESULT_PATH.mkdir(parents=True, exist_ok=True)
        self.DEBUG_PATH.mkdir(parents=True, exist_ok=True)

class RLConfig:
    """存储用于基于强化学习的布局优化的所有配置参数。

    Attributes:
        ROOT_PATH (pathlib.Path): 项目的根目录路径。
        RL_OPTIMIZER_PATH (pathlib.Path): RL优化器模块的根目录。
        DATA_PATH (pathlib.Path): RL优化器的数据目录。
        CACHE_PATH (pathlib.Path): 用于存放所有自动生成的中间文件的缓存目录。
        LOG_PATH (pathlib.Path): 用于存放训练日志和模型的目录。
        TRAVEL_TIMES_CSV (pathlib.Path): 原始通行时间矩阵的CSV文件路径。
        PROCESS_TEMPLATES_JSON (pathlib.Path): 用户定义的就医流程模板文件路径。
        NODE_VARIANTS_JSON (pathlib.Path): 自动生成的节点变体缓存文件路径。
        TRAFFIC_DISTRIBUTION_JSON (pathlib.Path): 自动生成的流量分布缓存文件路径。
        RESOLVED_PATHWAYS_PKL (pathlib.Path): 最终解析出的流线数据缓存文件路径。
        COST_MATRIX_CACHE (pathlib.Path): 预计算成本矩阵的缓存文件路径。
        AREA_SCALING_FACTOR (float): 科室面积允许的缩放容差。
        MANDATORY_ADJACENCY (List[List[str]]): 强制相邻的科室对列表。
        FIXED_NODE_TYPES (List[str]): 在布局中位置固定、不参与优化的节点类型列表。
        EMBEDDING_DIM (int): 节点嵌入向量的维度。
        TRANSFORMER_HEADS (int): Transformer编码器中的多头注意力头数。
        TRANSFORMER_LAYERS (int): Transformer编码器的层数。
        FEATURES_DIM (int): 特征提取器输出的特征维度。
        LEARNING_RATE (float): 优化器的学习率。
        NUM_ENVS (int): 用于训练的并行环境数量。
        NUM_STEPS (int): 每个环境在每次更新前收集的数据步数。
        TOTAL_TIMESTEPS (int): 训练的总时间步数。
        GAMMA (float): 奖励的折扣因子。
        GAE_LAMBDA (float): 通用优势估计(GAE)的lambda参数。
        CLIP_EPS (float): PPO中的裁剪范围。
        ENT_COEF (float): 熵损失的系数，用于鼓励探索。
        BATCH_SIZE (int): 每个优化轮次中使用的批大小。
        NUM_EPOCHS (int): 每次收集数据后，对数据进行优化的轮次。
        REWARD_TIME_WEIGHT (float): 奖励函数中通行时间成本的权重。
        REWARD_ADJACENCY_WEIGHT (float): 奖励函数中相邻性偏好的权重。
        RESUME_TRAINING (bool): 是否启用断点续训功能。
        PRETRAINED_MODEL_PATH (str): 预训练模型路径，用于断点续训。
        CHECKPOINT_FREQUENCY (int): checkpoint保存频率（按训练步数计算）。
        SAVE_TRAINING_STATE (bool): 是否保存完整训练状态（包括优化器、学习率调度器状态）。
    """

    def __init__(self):
        # --- 路径配置 (使用Pathlib) ---
        """
        初始化 RLConfig 类，设置强化学习布局优化所需的所有路径、输入文件、缓存文件、约束参数及模型训练超参数。
        
        该方法会自动创建缓存目录和日志目录（如不存在）。
        """
        self.ROOT_PATH: pathlib.Path = pathlib.Path(__file__).parent.parent
        self.RL_OPTIMIZER_PATH: pathlib.Path = self.ROOT_PATH / 'src' / 'rl_optimizer'
        self.DATA_PATH: pathlib.Path = self.RL_OPTIMIZER_PATH / 'data'
        self.CACHE_PATH: pathlib.Path = self.DATA_PATH / 'cache'
        self.LOG_PATH: pathlib.Path = self.ROOT_PATH / 'logs'
        self.RESULT_PATH: pathlib.Path = self.ROOT_PATH / 'results'

        # --- 输入文件 ---
        self.TRAVEL_TIMES_CSV: pathlib.Path = self.RESULT_PATH / 'network' / 'hospital_travel_times.csv'
        self.PROCESS_TEMPLATES_JSON: pathlib.Path = self.DATA_PATH / 'process_templates_traditional.json'

        # --- 自动生成/缓存的中间文件 ---
        self.NODE_VARIANTS_JSON: pathlib.Path = self.CACHE_PATH / 'node_variants.json'
        self.TRAFFIC_DISTRIBUTION_JSON: pathlib.Path = self.CACHE_PATH / 'traffic_distribution.json'
        self.RESOLVED_PATHWAYS_PKL: pathlib.Path = self.CACHE_PATH / 'resolved_pathways.pkl'
        self.COST_MATRIX_CACHE: pathlib.Path = self.CACHE_PATH / 'cost_precomputation.npz'

        # --- 约束配置 ---
        self.AREA_SCALING_FACTOR: float = 0.1
        self.EMPTY_SLOT_PENALTY_FACTOR: float = 10000.0  # 每个空槽位的惩罚系数
        self.ALLOW_PARTIAL_LAYOUT: bool = True  # 是否允许部分布局（跳过槽位）
        self.MANDATORY_ADJACENCY: List[List[str]] = []  # 例如: [['手术室_30007', '中心供应室_10003']]
        self.FIXED_NODE_TYPES: List[str] = [
            '门', '楼梯', '电梯', '扶梯', '走廊', '墙', '栏杆', 
            '室外', '绿化', '中庭', '空房间', '急诊科','挂号收费'
        ]

        # --- Transformer模型配置 ---
        self.EMBEDDING_DIM: int = 128  # 嵌入维度
        self.FEATURES_DIM: int = 128  # 特征提取器输出的特征维度
        self.TRANSFORMER_HEADS: int = 4  # 多头注意力头数
        self.TRANSFORMER_LAYERS: int = 4  # Transformer层数
        self.TRANSFORMER_DROPOUT: float = 0.1  # Dropout比例
        
        # --- 策略网络配置 ---
        self.POLICY_NET_ARCH: int = 128
        self.POLICY_NET_LAYERS: int = 2
        self.VALUE_NET_ARCH: int = 128
        self.VALUE_NET_LAYERS: int = 2
        
        # --- 学习率调度器配置 ---
        self.LEARNING_RATE_SCHEDULE_TYPE: str = "linear"  # "linear", "constant"
        self.LEARNING_RATE_INITIAL: float = 1e-4  # 初始学习率
        self.LEARNING_RATE_FINAL: float = 1e-8   # 最终学习率（线性衰减的目标值）
        self.LEARNING_RATE: float = 1e-4  # 保持向后兼容性

        # --- PPO 训练超参数 ---
        self.NUM_ENVS: int = 8
        self.N_STEPS: int = 512  # 修正为N_STEPS以匹配PPO参数
        self.TOTAL_TIMESTEPS: int = 5_000_000
        self.GAMMA: float = 0.99
        self.GAE_LAMBDA: float = 0.95
        self.CLIP_RANGE: float = 0.2  # 修正为CLIP_RANGE以匹配PPO参数
        self.ENT_COEF: float = 0.05
        self.VF_COEF: float = 0.5  # 添加值函数损失系数
        self.MAX_GRAD_NORM: float = 0.5  # 添加梯度裁剪参数
        self.BATCH_SIZE: int = 64
        self.N_EPOCHS: int = 10  # 修正为N_EPOCHS以匹配PPO参数
        
        # --- 性能优化参数 ---
        self.COST_CACHE_SIZE: int = 1000  # LRU缓存大小
        self.MAX_PATHWAY_COMBINATIONS: int = 10000  # 最大流线组合数
        
        # --- 算法默认参数 ---
        self.DEFAULT_PENALTY: float = max([i['time'] for i in COLOR_MAP.values()])  # 默认惩罚值使用最大通行时间
        self.INVALID_ACTION_PENALTY: float = -100.0  # 无效动作惩罚
        
        # --- 模拟退火默认参数 ---
        self.SA_DEFAULT_INITIAL_TEMP: float = 1000.0 # 初始温度
        self.SA_DEFAULT_FINAL_TEMP: float = 0.1 # 最终温度
        self.SA_DEFAULT_COOLING_RATE: float = 0.95 # 冷却速率
        self.SA_DEFAULT_TEMPERATURE_LENGTH: int = 100 # 温度长度
        self.SA_DEFAULT_MAX_ITERATIONS: int = 10000 # 最大迭代次数
        self.SA_MAX_REPAIR_ATTEMPTS: int = 10 # 约束修复最大尝试次数

        # --- 遗传算法默认参数 ---
        self.GA_DEFAULT_POPULATION_SIZE: int = 300 # 种群大小
        self.GA_DEFAULT_ELITE_SIZE: int = 20 # 精英大小
        self.GA_DEFAULT_MUTATION_RATE: float = 0.15  # 初始变异率
        self.GA_DEFAULT_CROSSOVER_RATE: float = 0.85  # 初始交叉率
        self.GA_DEFAULT_TOURNAMENT_SIZE: int = 5 # 锦标赛大小
        self.GA_DEFAULT_MAX_AGE: int = 100 # 最大代数
        self.GA_DEFAULT_CONVERGENCE_THRESHOLD: int = 300 # 停滞代数阈值
        self.GA_DEFAULT_MAX_ITERATIONS: int = 10000 # 最大迭代次数

        # --- 约束感知遗传算法增强参数 ---
        self.GA_CONSTRAINT_REPAIR_STRATEGY: str = 'greedy_area_matching'  # 默认约束修复策略
        self.GA_ADAPTIVE_PARAMETERS: bool = True  # 启用自适应参数调整
        self.GA_MAX_REPAIR_ATTEMPTS: int = 5  # 约束修复最大尝试次数
        self.GA_DIVERSITY_THRESHOLD_LOW: float = 0.05  # TODO:多样性极低阈值
        self.GA_DIVERSITY_THRESHOLD_MEDIUM: float = 0.15  # TODO:多样性较低阈值
        
        # --- 并发控制 ---
        self.MAX_PARALLEL_ALGORITHMS: int = 3
        
        # --- 评估和检查点配置 ---
        self.EVAL_FREQUENCY: int = 10000  # 添加评估频率

        # --- 软约束奖励权重 ---
        self.REWARD_TIME_WEIGHT: float = 1.0
        self.REWARD_ADJACENCY_WEIGHT: float = 0.1
        self.REWARD_PLACEMENT_BONUS: float = 0.1  # 成功放置一个科室的即时奖励
        self.REWARD_EMPTY_SLOT_PENALTY: float = 0.5  # 每个空槽位的最终惩罚
        self.REWARD_SCALE_FACTOR: float = 40000.0  # 奖励缩放因子, 仅对加权总时间成本有效
        self.REWARD_SKIP_PENALTY: float = -0.2  # 跳过惩罚
        self.REWARD_COMPLETION_BONUS: float = 1.0  # 完成奖励

        # --- 势函数奖励配置 ---
        self.ENABLE_POTENTIAL_REWARD: bool = False  # 是否启用势函数奖励
        self.POTENTIAL_REWARD_WEIGHT: float = 1.0  # 势函数奖励权重
        
        # --- 面积匹配奖励配置 ---
        self.AREA_MATCH_REWARD_WEIGHT: float = 0.2  # 面积匹配在势函数中的权重
        self.AREA_MATCH_BONUS_BASE: float = 10.0  # 基础面积匹配奖励值
        
        # --- 相邻性奖励配置 ---
        # 相邻性奖励总开关
        self.ENABLE_ADJACENCY_REWARD: bool = True
        
        # 相邻性奖励优化开关
        self.ENABLE_ADJACENCY_OPTIMIZATION: bool = True  # 启用优化相邻性计算器
        
        # 相邻性奖励权重(在势函数中的权重)
        self.ADJACENCY_REWARD_WEIGHT: float = 0.15
        
        # 多维度相邻性权重分配
        self.SPATIAL_ADJACENCY_WEIGHT: float = 0.4      # 空间相邻性权重
        self.FUNCTIONAL_ADJACENCY_WEIGHT: float = 0.5   # 功能相邻性权重  
        self.CONNECTIVITY_ADJACENCY_WEIGHT: float = 0.1 # 连通性相邻性权重
        
        # 相邻性判定参数
        self.ADJACENCY_PERCENTILE_THRESHOLD: float = 0.1  # 相邻性分位数阈值
        self.ADJACENCY_K_NEAREST: int | None = None              # 最近邻数量(None=自动)
        self.ADJACENCY_CLUSTER_EPS_PERCENTILE: float = 0.1 # 聚类邻域分位数
        self.ADJACENCY_MIN_CLUSTER_SIZE: int = 2          # 最小聚类大小
        
        # 连通性相邻性参数
        self.CONNECTIVITY_DISTANCE_PERCENTILE: float = 0.3  # 连通距离分位数阈值
        self.CONNECTIVITY_MAX_PATH_LENGTH: int = 3          # 最大路径长度
        self.CONNECTIVITY_WEIGHT_DECAY: float = 0.8         # 路径长度权重衰减因子
        
        # 奖励缩放参数
        self.ADJACENCY_REWARD_BASE: float = 5.0           # 基础奖励值
        self.ADJACENCY_PENALTY_MULTIPLIER: float = 1.5    # 负向惩罚倍数
        
        # 缓存和性能参数
        self.ADJACENCY_CACHE_SIZE: int = 500              # 相邻性缓存大小
        self.ADJACENCY_PRECOMPUTE: bool = True            # 是否预计算相邻性矩阵
        
        # 优化相邻性计算器配置
        self.ADJACENCY_OPTIMIZATION_SPARSE_THRESHOLD: float = 0.1  # 稀疏矩阵阈值
        self.ADJACENCY_OPTIMIZATION_VECTORIZE_BATCH_SIZE: int = 1000  # 向量化批处理大小
        self.ADJACENCY_OPTIMIZATION_MEMORY_EFFICIENT: bool = True  # 启用内存优化
        
        # 医疗功能相邻性数据
        self.MEDICAL_ADJACENCY_PREFERENCES: Dict[str, Dict[str, float]] = {
            # 正向偏好 (值为正数)
            "静配中心": {
                "ICU": 0.8,      # 静配中心与ICU物流相关
            },
            "中心供应室": {
                "内镜中心": 0.8,        # 中心供应室与内镜中心物流相关
            },
            "采血处": {
                "检验中心": 0.8,    # 采血处与检验中心物流相关
            },
            "手术室": {
                "ICU": 0.8,          # 手术室与ICU强相关
                "门诊手术室": 0.6
            },
            "ICU": {
                "NICU": 0.8          # ICU与NICU强相关
            },
            "产房": {
                "NICU": 0.8          # 产房与NICU强相关
            },
            "检验中心": {
                "病理科": 0.8,    # 检验中心与病理科物流相关
            }
        }

        # --- 断点续训配置 ---
        self.RESUME_TRAINING: bool = False  # 是否启用断点续训
        self.PRETRAINED_MODEL_PATH: str = "data/model"  # 预训练模型路径（用于断点续训）
        self.CHECKPOINT_FREQUENCY: int = 50000  # checkpoint保存频率（训练步数）
        self.SAVE_TRAINING_STATE: bool = True  # 是否保存完整训练状态（优化器、调度器等）

        # 确保关键路径存在
        self.CACHE_PATH.mkdir(parents=True, exist_ok=True)
        self.LOG_PATH.mkdir(parents=True, exist_ok=True)
        
        # --- 动态基线奖励归一化配置 ---
        self.ENABLE_DYNAMIC_BASELINE: bool = True  # 启用动态基线奖励归一化
        self.EMA_ALPHA: float = 0.1  # 指数移动平均平滑因子 (0 < alpha <= 1)
        self.BASELINE_WARMUP_EPISODES: int = 10  # 基线预热期episode数量
        self.BASELINE_UPDATE_FREQUENCY: int = 10  # 基线更新频率（每N个episode更新一次）
        
        # 动态基线归一化权重（归一化后各组件的权重）
        self.NORMALIZED_TIME_WEIGHT: float = 1.0  # 时间成本归一化权重
        self.NORMALIZED_ADJACENCY_WEIGHT: float = 0.5  # 相邻性奖励归一化权重
        self.NORMALIZED_AREA_WEIGHT: float = 0.3  # 面积匹配归一化权重
        self.NORMALIZED_SKIP_PENALTY_WEIGHT: float = 0.3  # 跳过惩罚归一化权重
        self.NORMALIZED_PLACEMENT_BONUS_WEIGHT: float = 0.1  # 放置奖励归一化权重
        self.NORMALIZED_COMPLETION_BONUS_WEIGHT: float = 0.1  # 完成奖励归一化权重
        
        # 奖励归一化参数
        self.REWARD_NORMALIZATION_CLIP_RANGE: float = 1.0  # 归一化时的裁剪范围（几个标准差）
        self.REWARD_NORMALIZATION_MIN_STD: float = 1e-8  # 最小标准差，防止除零错误
        self.ENABLE_REWARD_CLIPPING: bool = True  # 启用奖励裁剪
        self.REWARD_CLIP_RANGE: Tuple[float, float] = (-10.0, 10.0)  # 最终奖励裁剪范围
        
        # 基线预测相关配置
        self.ENABLE_RELATIVE_IMPROVEMENT_REWARD: bool = True  # 启用相对改进奖励
        self.RELATIVE_IMPROVEMENT_SCALE: float = 5.0  # 相对改进奖励缩放因子
        self.BASELINE_SMOOTHING_WINDOW: int = 50  # 基线平滑窗口大小
        
        # 验证相邻性奖励配置参数
        if self.ENABLE_ADJACENCY_REWARD:
            self._validate_adjacency_parameters()
        
        # 验证动态基线配置参数
        if self.ENABLE_DYNAMIC_BASELINE:
            self._validate_dynamic_baseline_parameters()
    
    def _validate_dynamic_baseline_parameters(self):
        """
        验证动态基线配置参数的有效性。
        在配置错误时抛出异常或自动修正参数值。
        """
        import logging
        logger = logging.getLogger(__name__)
        
        # 验证EMA平滑因子
        if not (0 < self.EMA_ALPHA <= 1):
            raise ValueError(f"EMA平滑因子必须在(0,1]范围内，当前值：{self.EMA_ALPHA}")
        
        # 验证预热期参数
        if not isinstance(self.BASELINE_WARMUP_EPISODES, int) or self.BASELINE_WARMUP_EPISODES < 0:
            logger.warning(f"基线预热期必须为非负整数，当前值：{self.BASELINE_WARMUP_EPISODES}，自动修正为200")
            self.BASELINE_WARMUP_EPISODES = 200
        
        # 验证更新频率
        if not isinstance(self.BASELINE_UPDATE_FREQUENCY, int) or self.BASELINE_UPDATE_FREQUENCY <= 0:
            logger.warning(f"基线更新频率必须为正整数，当前值：{self.BASELINE_UPDATE_FREQUENCY}，自动修正为10")
            self.BASELINE_UPDATE_FREQUENCY = 10
        
        # 验证归一化权重
        weight_params = [
            ('NORMALIZED_TIME_WEIGHT', self.NORMALIZED_TIME_WEIGHT),
            ('NORMALIZED_ADJACENCY_WEIGHT', self.NORMALIZED_ADJACENCY_WEIGHT),
            ('NORMALIZED_AREA_WEIGHT', self.NORMALIZED_AREA_WEIGHT),
            ('NORMALIZED_SKIP_PENALTY_WEIGHT', self.NORMALIZED_SKIP_PENALTY_WEIGHT),
            ('NORMALIZED_COMPLETION_BONUS_WEIGHT', self.NORMALIZED_COMPLETION_BONUS_WEIGHT)
        ]
        
        for param_name, param_value in weight_params:
            if not isinstance(param_value, (int, float)) or param_value < 0:
                raise ValueError(f"归一化权重参数 {param_name} 必须为非负数值，当前值：{param_value}")
        
        # 验证裁剪范围
        if not isinstance(self.REWARD_NORMALIZATION_CLIP_RANGE, (int, float)) or self.REWARD_NORMALIZATION_CLIP_RANGE <= 0:
            raise ValueError(f"奖励归一化裁剪范围必须为正数，当前值：{self.REWARD_NORMALIZATION_CLIP_RANGE}")
        
        # 验证最小标准差
        if not isinstance(self.REWARD_NORMALIZATION_MIN_STD, (int, float)) or self.REWARD_NORMALIZATION_MIN_STD <= 0:
            raise ValueError(f"最小标准差必须为正数，当前值：{self.REWARD_NORMALIZATION_MIN_STD}")
        
        # 验证奖励裁剪范围
        if isinstance(self.REWARD_CLIP_RANGE, (list, tuple)) and len(self.REWARD_CLIP_RANGE) == 2:
            min_reward, max_reward = self.REWARD_CLIP_RANGE
            if min_reward >= max_reward:
                raise ValueError(f"奖励裁剪范围必须满足min < max，当前值：{self.REWARD_CLIP_RANGE}")
        else:
            raise ValueError(f"奖励裁剪范围必须为包含两个元素的tuple，当前值：{self.REWARD_CLIP_RANGE}")
        
        # 验证相对改进奖励配置
        if not isinstance(self.RELATIVE_IMPROVEMENT_SCALE, (int, float)) or self.RELATIVE_IMPROVEMENT_SCALE <= 0:
            logger.warning(f"相对改进奖励缩放因子必须为正数，当前值：{self.RELATIVE_IMPROVEMENT_SCALE}，自动修正为5.0")
            self.RELATIVE_IMPROVEMENT_SCALE = 5.0
        
        # 验证基线平滑窗口
        if not isinstance(self.BASELINE_SMOOTHING_WINDOW, int) or self.BASELINE_SMOOTHING_WINDOW <= 0:
            logger.warning(f"基线平滑窗口必须为正整数，当前值：{self.BASELINE_SMOOTHING_WINDOW}，自动修正为50")
            self.BASELINE_SMOOTHING_WINDOW = 50
        
        logger.info("动态基线配置参数验证通过")
    
    def _validate_adjacency_parameters(self):
        """
        验证相邻性奖励相关配置参数的有效性。
        在配置错误时抛出异常或自动修正参数值。
        """
        import logging
        logger = logging.getLogger(__name__)
        
        # 验证权重参数
        weight_params = [
            ('ADJACENCY_REWARD_WEIGHT', self.ADJACENCY_REWARD_WEIGHT),
            ('SPATIAL_ADJACENCY_WEIGHT', self.SPATIAL_ADJACENCY_WEIGHT), 
            ('FUNCTIONAL_ADJACENCY_WEIGHT', self.FUNCTIONAL_ADJACENCY_WEIGHT),
            ('CONNECTIVITY_ADJACENCY_WEIGHT', self.CONNECTIVITY_ADJACENCY_WEIGHT)
        ]
        
        for param_name, param_value in weight_params:
            if not isinstance(param_value, (int, float)) or param_value < 0:
                raise ValueError(f"相邻性权重参数 {param_name} 必须为非负数值，当前值：{param_value}")
                
        # 验证分位数阈值参数
        if not (0 < self.ADJACENCY_PERCENTILE_THRESHOLD < 1):
            raise ValueError(f"相邻性分位数阈值必须在(0,1)范围内，当前值：{self.ADJACENCY_PERCENTILE_THRESHOLD}")
        
        # 验证连通性参数
        if self.CONNECTIVITY_ADJACENCY_WEIGHT > 0:
            if not isinstance(self.CONNECTIVITY_MAX_PATH_LENGTH, int) or not (2 <= self.CONNECTIVITY_MAX_PATH_LENGTH <= 5):
                logger.warning(f"连通性最大路径长度应在2-5之间，当前值：{self.CONNECTIVITY_MAX_PATH_LENGTH}，自动修正为3")
                self.CONNECTIVITY_MAX_PATH_LENGTH = 3
                
            if not (0 < self.CONNECTIVITY_WEIGHT_DECAY < 1):
                logger.warning(f"连通性权重衰减因子应在(0,1)范围内，当前值：{self.CONNECTIVITY_WEIGHT_DECAY}，自动修正为0.8")
                self.CONNECTIVITY_WEIGHT_DECAY = 0.8
                
            if not (0 < self.CONNECTIVITY_DISTANCE_PERCENTILE < 1):
                logger.warning(f"连通性距离分位数应在(0,1)范围内，当前值：{self.CONNECTIVITY_DISTANCE_PERCENTILE}，自动修正为0.3")
                self.CONNECTIVITY_DISTANCE_PERCENTILE = 0.3
        
        # 验证奖励缩放参数
        if not isinstance(self.ADJACENCY_REWARD_BASE, (int, float)) or self.ADJACENCY_REWARD_BASE <= 0:
            raise ValueError(f"相邻性奖励基础值必须为正数，当前值：{self.ADJACENCY_REWARD_BASE}")
            
        if not isinstance(self.ADJACENCY_PENALTY_MULTIPLIER, (int, float)) or self.ADJACENCY_PENALTY_MULTIPLIER <= 0:
            raise ValueError(f"相邻性惩罚倍数必须为正数，当前值：{self.ADJACENCY_PENALTY_MULTIPLIER}")
        
        # 验证优化配置参数
        if not isinstance(self.ENABLE_ADJACENCY_OPTIMIZATION, bool):
            logger.warning(f"相邻性优化开关应为布尔值，当前值：{self.ENABLE_ADJACENCY_OPTIMIZATION}，自动修正为True")
            self.ENABLE_ADJACENCY_OPTIMIZATION = True
            
        if not (0 < self.ADJACENCY_OPTIMIZATION_SPARSE_THRESHOLD <= 1):
            logger.warning(f"稀疏矩阵阈值应在(0,1]范围内，当前值：{self.ADJACENCY_OPTIMIZATION_SPARSE_THRESHOLD}，自动修正为0.1")
            self.ADJACENCY_OPTIMIZATION_SPARSE_THRESHOLD = 0.1
            
        if not isinstance(self.ADJACENCY_OPTIMIZATION_VECTORIZE_BATCH_SIZE, int) or self.ADJACENCY_OPTIMIZATION_VECTORIZE_BATCH_SIZE <= 0:
            logger.warning(f"向量化批处理大小应为正整数，当前值：{self.ADJACENCY_OPTIMIZATION_VECTORIZE_BATCH_SIZE}，自动修正为1000")
            self.ADJACENCY_OPTIMIZATION_VECTORIZE_BATCH_SIZE = 1000
        
        # 验证缓存参数
        if not isinstance(self.ADJACENCY_CACHE_SIZE, int) or self.ADJACENCY_CACHE_SIZE <= 0:
            logger.warning(f"相邻性缓存大小应为正整数，当前值：{self.ADJACENCY_CACHE_SIZE}，自动修正为500")
            self.ADJACENCY_CACHE_SIZE = 500
        
        # 验证医疗功能相邻性偏好数据
        if not isinstance(self.MEDICAL_ADJACENCY_PREFERENCES, dict):
            raise ValueError("医疗相邻性偏好配置必须为字典类型")
            
        # 检查偏好数据的格式正确性
        for dept, preferences in self.MEDICAL_ADJACENCY_PREFERENCES.items():
            if not isinstance(dept, str) or not isinstance(preferences, dict):
                raise ValueError(f"医疗相邻性偏好格式错误：{dept} -> {preferences}")
                
            for target_dept, score in preferences.items():
                if not isinstance(target_dept, str) or not isinstance(score, (int, float)):
                    raise ValueError(f"医疗相邻性偏好分数格式错误：{dept} -> {target_dept}: {score}")
                
                if not (-2.0 <= score <= 2.0):
                    logger.warning(f"医疗相邻性偏好分数建议在[-2.0, 2.0]范围内：{dept} -> {target_dept}: {score}")
        
        # 权重组合合理性检查
        total_adjacency_weight = (self.SPATIAL_ADJACENCY_WEIGHT + 
                                 self.FUNCTIONAL_ADJACENCY_WEIGHT + 
                                 self.CONNECTIVITY_ADJACENCY_WEIGHT)
        
        if total_adjacency_weight <= 0:
            raise ValueError("至少需要启用一种相邻性权重（空间、功能、连通性）")
            
        if total_adjacency_weight > 3.0:
            logger.warning(f"相邻性权重总和过高可能影响训练稳定性：{total_adjacency_weight:.2f}")
        
        logger.info("相邻性奖励配置参数验证通过")
</file>

</files>
